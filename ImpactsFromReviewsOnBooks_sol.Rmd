---
  output: 
  word: default
  html_document: default
---

#< ignore

```{r "setup", eval=FALSE}
library(RTutor)
library(modelsummary)
library(yaml)
library(xfun)
library(tidyverse)
library(kableExtra)
library(grid)
library(gridExtra)
 
 #Set Working Directory
 setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket") 

 # Set problem set's name
 ps.name = "ImpactsFromReviewsOnBooks"; sol.file = paste0(ps.name,"_sol.Rmd")
 
 # character vector of all required packages in the problem set
 libs = c("ggplot2", "tidyverse", "kableExtra", "modelsummary") 
 
 # Create Problem Set
 create.ps(sol.file=sol.file, user.name = NULL, ps.name=ps.name,libs=libs,stop.when.finished=FALSE, addons = "quiz")
 
 #open problem set in web browser
show.ps(ps.name,launch.browser=TRUE, auto.save.code=FALSE, sample.solution=FALSE, load.sav=FALSE,  is.solved=FALSE)

 stop.without.error()

```

#>

# The Impacts of Professional Reviews and Crowd Ratings on the Book Market

Author: Leonard Pöhls 

Hello readers and welcome to my Bachelor Thesis on the impact of professional reviews and crowd ratings on the book market. Do not wonder about the way this Thesis is constructed. The whole program is based on the statistical programming language R and the package RTutor to generate interactive problem sets with exercises for the readers. My created Problem is based on the Paper **Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings"** by Imke Reimers and Joel Waldfogel, that has been published in 2021 in the American Economic Journal. This paper examines two main aspects. First, how pre-purchase information in the form of crowd ratings from other individual purchaser and professional reviews from daily newspapers affect sales ranks and quantities. Second, to conclude inferences due to welfare effects with and without the presence of pre-purchase information. The base of those mentioned welfare effects got determined through the transformation from sales ranks into quantity elasticities. The Paper can be opened [here](https://www.aeaweb.org/articles?id=10.1257/aer.20200153). 

In 1995, one year after Amazon was incorporated, more than 61 percent of all book sales in the U.S. were generated by physical bookstores and book clubs, while only ten percent were made through other channels, including Amazon (Curcic, 2023).  

In the following 28 years, we have witnessed the growth of digitization and the development of crowd rating infrastructure on online sites. Users are thus able to obtain important non-professional pre-purchase information from other users that can influence purchasing behavior and economic welfare effects. As a result, online retailers have largely replaced the trade in physical books. From an economic point of view, pre-purchase information also has an impact on personal quality expectations and thus on demand.  

Today, the distribution of print book market share has changed significantly, and Amazon has taken the lead as the largest retailer of print books in the entire world. For instance, the Amazon share of the U.S. book market accounts more than 40 percent and around 50 percent of the Uk market share (McLoughlin, 2022).

To examine these effects, this problem set aims to reorganize and replicate part of the study from Reimers and Waldfogel by retyping and extending their research on the effects of professional reviews and crowd ratings on sales ranks and quantities. 

## Exercise Content

1. Motivation

 1.1 Book Market, Professional Reviews and Crowd Ratings
 
 1.2 Introduction to Welfare, Demand and Price Elasticity

2. Data and Descriptive Insights

 2.1 Introduction to the Data Set
 
 2.2 Analysis of Pre-Purchase Information
 
 2.3 Recognition of Potential Effects through Descriptive Approaches
 
3. Empirical Strategies on Sales Ranks 

 3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
 3.2 Estimation of the Effects on Sales Ranks and Prices
 
 3.3 Introduction and Implementation of Event Studies
 
4. Translating Sales Ranks into Quantities and Price Elasticity
 
5. Conclusion

6. References

## Overview

Initially, basic knowledge about the book market and related terminology related to Amazon will be explained. In order to be capable to classify the background of pre-purchase information in economic terms, the economic added value of this information and the price elasticity are presented. Then, I focus on the underlying data set to show how it is structured and which attributes are important for further examinations. On the basis of this data set, descriptive analyses of the occurrence of professional reviews, price and sales rank developments are carried out in order to obtain an overview and to be able to make initial assumptions about potential effects. Subsequently, the empirical part of this problem set begins. First, basic, and advanced knowledge of regressions, robust standard errors, and fixed effects is provided, which is necessary for the following replication of the regression table originally created by the authors. Afterwards, a so-called cross-sectional event study is replicated to graphically display these estimated effects. Finally, I explain how the authors convert relative sales ranks into quantities to estimate elasticities, which we then implement in R. The summary of the results of the problem set and similar results of other authors are presented in the final section.                     

## An Instruction how to work with Problem sets

As mentioned above, the purpose of this problem set is to create an interactive environment for the readers. Thus, sometimes different types of exercises appear within a so-called chunk (window) below. There are essentially three different types of exercises:   
* An empty code chunk without any Information. Consequently, you have to find the solution by yourself. 
* Code sections with gaps like ___ , which need to be replaced with the correct code. 
* Those where all the code is already given. This code is ready to be executed.  
If you need some advice while solving the task, just press `hint` to get help. Pressing `run` will execute the code. The `solution` button is a shortcut, which immediately provides the correct code. To verify the task, click on `check`. If your code was not correct, you will receive a corresponding report.  

Next to code chunks, you can also work on some multiple choice quizzes to test your prior knowledge or to check your own text comprehension. Guessing the correct answer can also lead to better understanding and maintaining attention.  

Press `Go to next exercise...` to continue and work through further tasks of this problem set.

<br/>


## Exercise 1 -- Motivation 

In this study, the book market was selected to determine the impact of crowd ratings and professional reviews. 

The following chapter will first explain what is meant by "pre-purchase information," what the situation is on the book market, and why this market in particular is predestined for measuring review effects. To find an answer, we focus on newspaper magazines and on the role of Amazon concerning crowd ratings and so-called sales ranks. 

In the second part of this chapter, I illustrate possible welfare effects from access to pre-purchase information. In addition, I explain the transition from sales ranks and sales prices to quantity elasticities.   

After editing this chapter, you gained an insight of the initial situation on the book market and you received deeper knowledge about basic economic issues. Thus you will be well prepared to continue with chapter two. 

### Structure 

1.1 Book Market, Professional Reviews and Crowd Ratings

1.2 Introduction to Welfare, Demand and Price Elasticity


## Exercise 1.1 -- Book Market, Professional Reviews and Crowd Ratings

In general, economists distinguish goods concerning their characteristics, their occurrence, or other conditions. With respect to the degree of uncertainty, we differentiate between three different goods, **Search Goods**, **Experience Goods** and **Credence Goods**. Buyer of **Search Goods** already have an accurate perception and a high degree of certainty about what they want to buy (Wieneke, 2019). For instance, sugar or computer are typical search goods. **Experience Goods** are associated with a lower degree of certainty. Without information advantage, buyers are unable to assess the quality of the good until they consume it, as in the case cinema visits or whine (Wieneke, 2019). By the consumption of **Credence Goods** the buyer never gets into the situation of evaluating the quality of the underlying good, as the degree of uncertainty is the highest here. Common credence goods are services such as lawyers or surgeons (Wieneke, 2019).  

#< quiz "Books_As_Goods"
question: What would you guess books belong to?
sc:
- Search Goods.
- Experience Goods.*
- Credence Goods.
success: Great, your answer is correct!
failure: Try again.

#>

The underlying paper focuses on the book market to examine the impacts from crowd ratings and professional reviews on sales ranks. So why is the book market particularly well suited for this study? In their paper, Reimers and Waldfogel (2021) enumerated three main reasons for this assertion. First, books are among experience goods. For the other two goods, pre-purchase information is less or not relevant. Second, the number of professional reviews (in high visible media) is relatively small and distributed among a few major newspapers. The third reason relates to the data set on which the entire study is based. The high frequently data on book demand at Amazon should contain about 45% of the US physical book market, roughly comparable to the figures of Mcloughlin in 2022.

As explained, professional reviews are periodic reviews in daily newspaper articles. The data set includes information on the appearance of professional reviews from the New York Times, the Chicago Tribune, the Boston Globe, the Wall Street Journal, the Los Angeles Times, and the Washington Post. However, quantitative information as well as star ratings are not available. Additionally, The New York Times recommends nine books every week (New York Times, 2023). Professional reviews enable access to pre-purchase information.  

#< quiz "Big_Newspapers"
question: What is your suggestion, which of these newspapers has the most impact on the sales quantity?
sc:
- The New York Times.*
- The Chicago Tribune.
- The Boston Globe.
- The Wall Street Journal.
- The Los Angeles Times.
- The Washington Post.
success: Great, your answer is correct!
failure: Try again.

#>

As well as professional reviews, the Amazon data set also contains information about star ratings from buyers. Identified as a buyer on Amazon, anyone may rate their purchased product on a five-point scale. In principle, people benefit from the ratings of other customers because they optimize their buying behaviour as a result. In contrast to professional ratings, however, crowd ratings tend to generate less trust. Crowd reviews are prone to fake content for defamatory or fraudulent purposes, while professional reviews are created by objective and professional raters. Therefore, the more ratings exist for the particular product, the more likely the calculated average star rating approximates the "actual" quality. Thus, the crowd ratings represent the second piece of information before the purchase.
 
Hence, Reimers and Waldfogel (2021) claim that consumers interact with these two types of pre-purchase information in different ways, potentially leading to different or overlapping effects. One possible reason for this could be the different accessibility of these types of information. While audience reviews are visible to every Amazon user, professional reviews are accessible but not automatically visible to everyone. Also, people who happen to find a book on Amazon are less likely to check for the presence of a professional review after the fact. Conversely, people who have found a book reviewed by experts automatically get access to the reviews of the masses during the purchase process. Incidentally, word-of-mouth can be also understood as pre-purchase information. Nevertheless, the consumer has access to at least one type of pre-purchase information.  


#< quiz "Fakereviews_Amazon"
question: How many reviews on Amazon are fake or unreliable?
sc:
- 61%.
- 23%.
- 9%.
- 42%.*
- 15%.
success: Great, your answer is correct! (Stieb, 2022)
failure: Try again.

#>


We mentioned earlier that we want to make an estimate based on so-called sales ranks. Sales ranks are the numerical representation of how your products sell compared to other products in the same category (Wisniach, 2022). In fact, we do not have information on the quantities sold, so the sales ranks substitute for these values. [Amazon](https://www.amazon.com/gp/help/customer/display.html?nodeId=GGGMZK378RQPATDJ) defined their sales rank as hourly updated calculation to "reflect recent and historical sales of every item sold on Amazon". Hence, sales ranks are relative numbers to compare sales activities. To make assumptions for a welfare analysis, the authors of the underlying paper collected additional data from the 2018 New York Times weekly top 100 bestsellers to convert ranks into quantities. This enables us to examine price elasticities. 

**Summary**

In summary, we have obtained an explanation of the goods in terms of their level of certainty and have assigned the books to the experience goods. We also identified three different types of pre-purchase information, two of which we use for estimation. We also recognized that crowd ratings and professional reviews have different effects on sales rankings. Finally, we got an introduction to Amazons sales ranks and their importance for the further course. 

The following chapter 1.2 will provide you with the basic economic knowledge you need to understand the analysis and the way of this audit. 


## Exercise 1.2 -- Introduction to Welfare, Demand and Price Elasticity

Colloquially, the term "welfare" is associated with many contexts, such as unemployment benefit or other social assistance. The actual origin of this term lies in the economy. Mathematically, welfare is sum of producer surplus and consumer surplus. The consumer surplus is the difference between the price for a good that consumers are willing to pay and the actual price of this good. Against this, Producer surplus is the difference between the price that suppliers would be willing to charge for their goods and the actual price of this good. Actually, the economic reality is much more complex. For simplicity, we focus on polypol markets (markets with many suppliers) under perfect conditions, which the model requires to work. Without even one of these assumptions, the model is invalid.   

#< quiz "perfectMarket_Conditions"
question: Which of these conditions is **not** relevant for a perfect polypol market? 
sc:
- Perfect information availability (Knowledge about every price for the underlying good).
- No personal preferences (Preferences, that prevent you from acting rationally).
- Homogeneous goods (Equal goods).
- Fast reaction velocity (Changing market conditions are quickly recognized from every market participant).
- Every good has the same quality.*
- A large number of demanders and suppliers.
success: Great, your answer is correct! Equality of quality and homogeneity of goods do not from an equivalent. Homogeneity only includes the physical condition and the substitutability. 
failure: Try again.

#>

Under these conditions, we illustrate the situation between demand and supply with a diagram to better understand the added value of pre-purchase information. 

#< info "How to create graphs with the package `ggplot2`."

The `ggplot` package allows you to create diagrams based on a data set you provide. The structure for the most important functions is as follows:

```{r}
#General function to initialize ggplot2 and aes() is used to define mappings between variables and visual properties.
ggplot(data = data_set, aes(y = y, x = x)) +
#Family function for geometric objects to be added (e.g. geom_bar, geom_line, geom_density, geom_histogramm, geom_text,...)
geom_line(aes(y=y, x=x), color = "color", size = 2, ...) +
#To create multiple panels within one plot
facet_grid(~variable_to_split) +
#Function to set the labels of the plot as x-axis, y-axis, title
labs() +
#Define the scales for the axis
xlim()/ylim() +
#Function to modify the overall appearance of the plot
theme() +
#Customizing legends and guides for color, size, ...
guides() +
#Change the limits or designations for x-axis or y-axis
scale_x_continuous() +
scale_y_continuous() 
```

**Note:** Add a `+` after these functions to connect them (except of the last function).
#>

**Task:** Run the following chunk to create this model. For performance purposes, I saved the essential part `fdemandsupplyfinal` in `fdemandsupply`. Press `check` to collect your points. 

```{r "1.2.1"}
#< task_notest
#setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket/replication/data")
#load the package "ggplot2"
library(ggplot2)
demand <- c(2, 1.5, 1, 0.5, 0)
xAxis <- c(0, 1,2,3,4)
supply <- c(0, 0.5, 1, 1.5, 2)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Price = demand, Quantity = xAxis, Supply = supply, Group = xGroup)
#Read in fdemandsupply
fdemandsupply <- readRDS("fdemandsupply.RDS")
fdemandsupplyfinal <- fdemandsupply  +
geom_text(aes(x=0.5, y=1.83, label = "Demand"), color = "Black", angle = 329, size= 4, alpha=0.1) +
geom_text(aes(x=3.35, y=1.78, label = "Supply"), color = "Black", angle = 31, size= 4, alpha=0.1) +
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(labels = c("0", "", "Q*", "", "")) +
  scale_y_continuous(labels = c("0", "", "P*", "", "")) +
  theme(
  panel.grid.minor=element_blank(),plot.background=element_blank())
fdemandsupplyfinal
#>

```

The linear demand curve displays how the consumers behave on a perfect market. Obviously, a maximum price exists at which no more is consumed. This model also unrealistically assumes that free goods are consumed infinitely often. On the other side, the linear supply curve illustrates the suppliers point of view. The model concludes that every supplier can offer his good for the maximum price. Vice versa, the more the price falls, the fewer suppliers can still offer their good. In natural competition, suppliers are forced to align their prices. This is due to the fact that each supplier wants to maximize its turnover, so it is not worthwhile for the suppliers who can offer the lowest price to actually offer the lowest price. In long term, the actual market price will converge to P* and every supplier that can not bid for P* will disappear. Finally, P* defines the equilibrium price and Q* the equilibrium quantity.   

#< quiz "Mark_Surplus"
question: Which of these marked triangles represents the consumer surplus? 
sc:
- A.*
- B.
- C.
success: Great, your answer is correct!
failure: Try again.
#>

**Price Elasticity**

As mentioned earlier, the previous model represented a severe simplification of a complex market. For instance, the fiscus also impacts market activity by implementing price caps, by subsidizing various branches or in ensuring that no so called price cartels are created. Price cartels are an association of organizations closing price agreements for particular goods to bypass the competition. Usually, the course of the two curves is not linear and the conditions for a perfect market are not satisfied. The slope of the demand or supply curve depends on the so called **Price Elasticity**. For the demand side, **Price elasticity** indicates how demand reacts on changes in prices relatively. The formula looks as follows:   

$$
 \epsilon = \frac{\Delta Q/Q}{\Delta P/P} = \frac{\text{% Change in the quantity of goods demanded}}{\text{% Change in price}} 
$$

Hence, price elasticity delivers a value $ε$ $:=$ $[0, ∞]$ that can also show whether price increases would be worthwhile. An elasticity of $ε =$ 1 tells us that a price increase of one percents implies a demand decrease of one percent and represents the maximum turnover point for suppliers. As a result, elasticities of $ε <$ 1 are inelastic and otherwise elastic. 

#< quiz "Elastic_orInelastic"
question: You are bar owner and want to maximize your turnover. In a particular period of time, you found out that the demand on beer is linear and a price increase from three Units to four Units implies a decreasing sales quantity of 1000 quantity units to 875 quantity units. Which of the following responses is correct? 
sc:
- The price is inelastic - You should not increase the price.
- The price is elastic - You should increase the price by 47 percent.
- The price is inelastic - You should increase the price by 34 percent.
- The price is inelastic - You can easily more than double the price.*

success: Great, your answer is correct!
failure: Try again.

#>

Price elasticity can be represented graphically. The following graph illustrates the difference between elasticities in demand. 

**Task:** Execute the chunk below to see how the demand curve slope changes with different price elasticities

```{r "1.2.2", warning=FALSE}
#< task
#Create a fictive data set 
DatasetPE <- data.frame(
  Price = 1:10,
  Demand_05 = 10 - 0.5*1:10,
  Demand_1 = 10 - 1*1:10,
  Demand_2 = 10 - 2*1:10
)
colnames(DatasetPE) <- c("Price", "ε > 1", "ε = 1", "ε < 1")

DatasetPEN <- DatasetPE %>% 
  pivot_longer(cols = starts_with("ε"), names_to = "ε", values_to = "Quantity_Value") 

# Umwandeln der Daten in "long format"
ggplot(data = DatasetPEN, aes(x = Price, y = Quantity_Value, group = ε)) +
  geom_line() +
  theme_bw() +
  geom_segment(aes(x = 4, y = 8, xend = 4, yend = 9), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 9, xend = 2, yend = 9), color = "red", linetype = "dashed") +
  geom_text(aes(x=2.75, y=9.25, label = "ΔQU = 2"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=9.1, label = "ΔPU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=7.85, label = "ε = 2"), color = "black", angle = 340.5, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 6, xend = 4, yend = 7), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 7, xend = 3, yend = 7), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=7.2, label = "ΔQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=6.9, label = "PU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=5.45, label = "ε = 1"), color = "black", angle = 323.5, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 2, xend = 4, yend = 4), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 4, xend = 3, yend = 4), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=4.2, label = "ΔQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=3.45, label = "ΔPU = 2"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.45, y=1.75, label = "ε = 0,5"), color = "black", angle = 307.5, hjust=0, size=4, alpha = 0.1) +
  
  labs(title = "Demand Curves and their Price Elasticities",
       x = "Quantity in QU (Quantity Units)", 
       y = "Price in PU (Price Units)") +
  xlim(0, 10) + 
  ylim(0, 10) +
  scale_x_continuous(breaks = 0:10) +
  theme(panel.grid.minor=element_blank(),plot.background=element_blank()) 
#>
```

Returning to the subject of pre-purchase information, a clear classification of these is needed. Alan T. Sorensen (2007) and Kenneth Train (2015) contend that pre-purchase information alters perception of the goods quality and that a distinction between anticipated ex ante utility and experienced ex post utility is necessary to measure the effect of this information on welfare. In simpler terms, we assume that a consumer has a different utility depending on the availability of pre-purchase information. In summary, consumers may face three different situations depending on the expected quality of the books. The consumer can expect lower quality than the actual quality ( $\bar R_{j}$ < $R_{j}$ ), the same quality ( $\bar R_{j}$ = $R_{j}$ ) or a higher expected quality than the actual quality ( $\bar R_{j}$ > $R_{j}$ ). 

```{r "1.2.3", warning=FALSE}
#< task

#Create a fictive data set 
yPredQualityPrice <- c(1.6,1.0,0.50,0.15,-0.10)
xQuan <- c(1,2,3,4,5)
yRealQualityPrice <- c(2.1,1.5,1,0.65,0.4)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Pred_Price = yPredQualityPrice, Quan = xQuan, Real_Price = yRealQualityPrice, Group = xGroup)

#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quan, y=Pred_Price, group = Group), linetype = 2, size = 0.9) +
  geom_line(aes(x=Quan, y=Real_Price, group = Group), linetype = 1, size = 0.9) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 3, y = 0, xend = 3, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 0, xend = 4, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 5, yend = 1),size=1.1, color = "red") +

#Mark Zone A
  geom_text(aes(x=1.25, y=1.15, label = "A"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1, xend = 1, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone B
  geom_text(aes(x=1.5, y=1.5, label = "B"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1.5, xend = 1, yend = 2.1), color = "black", linetype = "dashed") +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone C
  geom_text(aes(x=2.27, y=1.17, label = "C"), color = "black", hjust=0, size=5, alpha = 0.1) +
#Mark Zone D
  geom_text(aes(x=3.6, y=0.9, label = "D"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 4, y = 0.65, xend = 4, yend = 1), color = "black", linetype = "dashed") +
#Mark intersection points
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
  geom_point(aes(x = 4, y=1), color = "red", size=2.5) +
  geom_text(aes(x=1.8, y=0.93, label = "P1"), color = "black", hjust=0, size=3, alpha = 0.1) +
  geom_text(aes(x=3.8, y=0.93, label = "P2"), color = "black", hjust=0, size=3, alpha = 0.1) +

#Add line description
  geom_text(aes(x=0, y=1.55, label = "Expected Quality without \nPre-Purchase Information\n _  _  _  _  _  _  _  _  _  _  "), color = "Black", hjust=0, size= 2.2, alpha=0.1) + 
  geom_text(aes(x=0, y=2.05, label = "Actual Quality with \nPre-Purchase Information\n ___________________"), color = "Black", hjust=0, size= 2.2, alpha=0.1) +
  
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(breaks = 1:5, labels = c("", "Q1", "Q*", "Q2", "")) +
  theme(
  axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#>
  
```

As visualized in the diagram above, the consumer would choose quantity Q1 for ( $\bar R_{j} < R_{j}$ ), quantity Q* for ( $\bar R_{j} = R_{j}$ ) and quantity Q2 for ( $\bar R_{j} > R_{j}$ ). For choosing Q1, the consumer would expect a surplus of triangle A while getting an actual surplus of A + B (ex post recognition of actual quality). A consumer with access to pre-purchase information would always choose Q* with a surplus of A + B + C. Overestimating the books quality, the consumer would buy quantity Q2 and obtains a surplus of A + B + C - D. It is obvious, that quality expectations affect the overall utility of the consumer and thus the demand curve appears to shift without impacting the price elasticity. In fact, the curve merely adjusts to the actual demand. 

#< quiz "ValueAdded_Triangle"
question: Which triangle represents the value added of pre-purchase information?
sc:
    - A
    - B
    - C*
    - D
success: Great, your answer is correct!
failure: Try again.
#>

**Summary**

After looking at a simple market diagram, we learnt to distinguish between demand and supply, to categorize the economic meaning of welfare and the composition of consumer / supplier surpluses and the formation of equilibrium values. Focusing the demand side, we got an insight into the topic of price elasticity to better understand the following examination of price elasticities on books. Hence, after collecting the necessary knowledge, we returned to pre-purchase information and their affects on welfare. We found out that pre-purchase information impacts the expectation for quality and leads to an adjustment of the demand curve. 

In the following chapter 2, we get an introduction to the Amazon data set followed by descriptive analyses.

## Exercise 2 -- Data and Descriptive Insights (3814)

The underlying examination by Reimers and Waldfogel is based on a data set provided by Amazon. 

In the first part, we deepen our understanding and the origin of the underlying data set and define important attributes that we later use for descriptive and empirical measurements.

Second, we focus on professional reviews and crowd ratings to investigate superficial contexts. To illustrate them, we use descriptive tables. 

Finally, we create an entire overview about descriptive analyses to detect potential effects between professional and non-professional reviews on sales ranks and prices to create a transition to the following empirical part of my Thesis.

After working through this chapter, you are surefooted in dealing with the underlying data set and thus well prepared to continue with the empirical part in chapter 3. 

### Structure

2.1 Introduction to the data set

2.2 Analysis of the pre-purchase information

2.3 Recognition of Potential Effects through overall Descriptive Approaches


## Exercise 2.1 -- Introduction to the Data Set

As already mentioned, the entire quantitative calculations are based on a data set provided by Amazon. The data set includes slightly less than 8.8 million observations of non-professional crowd ratings. One observation represents one sales rank for one day between 02-01-2018 and 31-12-2018 for each book in every country. The sales ranks are incomplete for some books, which means that sales ranks are not available for every day for these books. Observations were made in the USA, in UK and in Canada. In addition, the data set was merged with newspaper data from every newspaper magazine listed in 1.1 with information on whether the newspaper reviewed the particular book and whether the New York Times recommended the book. In fact, only 3.22 million observations were taken into consideration due to missing values, which reduces the actual market share what we examined. Reimers and Waldfogel claim that Amazon covered about 44.5 percent of the physical book market share in 2017, which we cannot accurately confirm. After removing about 63% of the data set, 44.5 percent cited by Reimers and Waldfogel means that about 16 percent of the book market is still covered by the Amazon data set. To improve the problem sets performance, we have already removed observations that are not relevant to our examination. 

To better understand our data set, we categorize some important variables and take a look at an excerpt of it. 

#< info "How to read in data sets with readRDS()."

`readRDS()` enables you to read in data sets from defined working directories. The advantage of RDS files is that they are R files, which can be read much faster and are more compact than, for example, csv or dta files. Furthermore, RDS files can also store alternative data, such as regressions or three-dimensional data. To read in, the structure is as follows:

```{r eval=FALSE}

data_set <- readRDS("find/your/path/data_set.RDS")
 
```

The data set is stored as variable `data_set`. If you want to save RDS files, the structure is as follows:

```{r eval=FALSE}

data_set <- saveRDS(data_set, file = "find/your/path/data_set.RDS")
 
```

**Note:** Originally, the Amazon data set was five gigabytes in size as a dta file. After converting this record, I saved this file as RDS and now this record is less than 100 megabytes.  

#>

**Task:** Use the function `readRDS` to load the data set called `dataEst.RDS`. Save this data set under the name `data`.

```{r "2.1.1", warning=FALSE}
#< task
#use readRDS("file_name.RDS") to load your data
#>
data <- readRDS("dataEst.RDS")
#< hint
cat("d__a <- re_dR__('da__Es_.RD_'.")
#>
```

You successfully loaded the data set. Now, we want to see how the data set is organized and which values the single attributes can take. 

**Task:** Use the function `head` to show the first rows from `data`. In addition, use the function `colnames` to list every column name from `data`.

```{r "2.1.2"}
#< task
#use head(data set name) to show your data set
#>
head(data[,1:17])
colnames(data)
#< hint
cat("h_e_(da__)")
cat("col_a__s(__ta)")
#>
```

As can be seen, the main data set consists of 230 variables. Not every one of them is essential for the following course and will not be considered further. For those to whom this does not apply, there is an explanation below.  

**Identification Variables**

The variable `asin` represents the corporate amazon ID to differentiate between different products. `Country` indicates whether the underlying valuation belongs to the US market (US), the UK market (UK) or the Canadian market (CA). `canum` represents the main ID and combines these variables to create a powerful country-dependent identifier. A title-author identifier is stored in the variable `titleno`. Identification variables are indispensable for each data set to clearly distinguish each observation. In this data set, the combination of `ddate` (see below) and `canum` defines each unique observation. 

**Chronological variables**

`ddate `indicates the main date on which the crowd ratings were created, to which all other chronological variables refer. `NYT_elapse` (New York Times), `BG_elapse` (Boston Globe), `CHI_elapse` (Chicago Tribune), `LAT_elapse` (Los Angeles Times), `WAPO_elapse` (Washington Post), `WSJ_elapse` (Wall Street Journal) and `OTH_elapse` (All magazines except NYT) indicate how many days have passed since/until the publication of the individual professional review. As a reference date serves here `ddate`. `epos` (if `pubno` $>$ 0) and `eneg` (if `pubno` $<$ 0) display counting days since publication. 

**Value Variables** 

The variable `rank` represents the Amazon sales rank, based on which we will do most of the calculations. `pamzn` represents the price of the particular book while `R` provides the given star rating on a five-point scale. `review` delivers the number of reviews in total, so this variable delivers the same number for each specification of `canum`. Each of these attributes occur twice, once each with a preceding "l". This means that the values are logarithmized. 

**Dummy Variables** 

Dummy variables are binary variables that can only take two values (here: 1 and 0). Any variable starting with `dnytpost` indicates "1" if the New York Times has published reviews within the period defined by the following numbers after `dnyt`. For instance, `dnytpost1_5` takes the value 1 if [0 < `NYT_elapse` < 6]. For `dnytpost` and `dnytpost10`, [0 <`NYT_elapse` $<$ 10] and [11 <`NYT_elapse` < 20] holds. `dnytpostpre` takes the value 1 if [-10 < `NYT_elapse` < 20]. The same principle applies to all variables starting with `dothpost` where the variable `OTH_elapse` defines the base instead of `NYT_elapse`. 

In order to convert the sales figures from our main data set into quantities, the authors provided confidential data for determining the quantities. These data were provided by the market research company Nielsen, and Reimers and Waldfogel have published them only in already edited form. The aforementioned confidential data sets provide information on the weekly 100 best-selling books between 2015 and 2018, while the accessible confidential data only provide intermediate calculations to determine price elasticities. 

**Note:** Many of the transformations from the raw data to the data on which the studies are based have already been added to the dataset to improve the performance of this problem set.

**Summary**

Obviously, with the underlying data set, we have not only a very large set of observations in a given time period, but also a large amount of information in the form of variables. Particularly in this case, where an already merged and edited dataset has been published, this large amount of information may also have a detrimental effect on the correct replication of this study. Nevertheless, Nevertheless, we continue with the dataset provided by Reimers and Waldfogel. In this chapter, we have gained insight into the dataset by running initial code chunks in R and the key variables that will guide us throughout the study. Finally, we learned how to define dummy variables and the importance of uniquely identifying variables. 

In chapter 2.2, we perform initial calculations on the data set by constructing descriptive statistics that focus on journal reviews and Amazon ratings.   

## Exercise 2.2 -- Analysis of the Pre-Purchase Information

In the following part, we will analyse data to gain a deeper understanding of the occurrence of professional ratings. In addition, we want to check whether the collected data from journals appropriates for following estimations.  

**Task:** Check the following chunk to read in the main data set.

```{r "2.2.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS") %>%
  arrange(canum, ddate)
#>

```

#< info "How to use `ifelse()` to generate new columns."

The `ifelse` function is predestined to generate dummy-variables. This function allows you to create nested values that assume certain expressions for different conditions. The structure is as follows: 

```{r eval=FALSE}

condition1 <- x >= 1
data_set$new_column <- ifelse(condition1, Value_if_condition_is_met (1), Value_if_condition_not_met (0))
 
```

In this case, the value of `new_column` take the value 1 if $x \ge 1$. Otherwise, the value for `new_column` take the value 0.

**Note:** The symbol `|` between several conditions stands for the logical OR and `&` for the logical AND. Use `==` instead of `=` within the condition formulation.  

#>

**Task:** Create two dummy variables that indicate `1` when an observation belongs to a book that has been reviewed. Also, read in the data set `DesRat` and save it accordingly, and then `check` the chunk.

**Note:** In sum, the underlying data set includes data about **8770 books**. `Doth` should contain reviews from non-New York Times magazines and `DALL` from all magazines.

```{r "2.2.2", warning=FALSE}

#< fill_in

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$____ <- ifelse(data$DBG == _ | data$DCHI == _ | data$DLAT == _ | data$DWAPO == _ | data$DWSJ == 1 , _, _)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , __ 0)

DesRat <- r____DS(_________DS_)

#>

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$DOTH <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 , 1, 0)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , 1, 0)

DesRat <- readRDS("DesRat.RDS")
```

**Task:** Press `check` to visualize the percentage of books reviewed by magazines.

```{r "2.2.3", warning=FALSE}

#< task

#Create Row Names
rownames(DesRat) <- c("Relative share", "Absolut share") 

#Use of kbl() funktion to create a visualable table
DesRat  %>%
kbl(col.names = c("Share_of_NYT_Ratings" = "New York Times", "Share_of_BG_Ratings" = "Boston Globe", "Share_of_CHI_Ratings" = "Chicago Tribune", "Share_of_LAT_Ratings" = "Los Angeles Times", "Share_of_WAPO_Ratings" = "Washington Post", "Share_of_DWSJ_Ratings" = "Wall Street Journal", "Share_of_OTH_Ratings" = "Non New York Times", "Share_of_ALL_Ratings" = "All"), caption = "Share of Professional Reviewed Books ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

#>

```

In total, 12.66 percent of all books (1521) included in the data set have been professionally reviewed. 11 percent (1315) of these books were reviewed by the New York Times, representing the largest percentage of books reviewed by professionals. The second largest percentage provides the Chicago Tribune with only 1.36 percent (139), which is approximately ten times less than the number of reviews published by the New York Times. Overall, non-New York Times magazines account for nearly 3.1 percent of the share. The following calculations distinguish between The New York Times and non-New York Times magazines, so we should note the size of the intersection between these distinctions. 

#< info "How to use `kbl()` to generate tables."

The `kbl` function belongs to the package `kableExtra` and creates tables based on data sets. Next to the basic structure that we use within this problem set, there are multiple more methods to style these tables. We adhere to a uniform structure within the problem set. More details about styling possibilities you find [here](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html). The structure is as follows: 

```{r eval = FALSE}
library(kableExtra)
```

```{r eval=FALSE}

# For multiple functions we use so-called pipes `%>%` to connect these functions 
data_set  %>%
kbl(col.names = c("old_column_name = new_column_name"), caption = "title") %>%
  kable_paper("striped", full_width = TRUE) %>%
#Creates seperate area with a caption within the table
  pack_rows("caption", starting_row, ending_row) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
 
```

**Note:** To apply `kbl()`, the underlying data set requires the so-called *long format*. Further explanations about this format you find in chapter 2.3.  

#>

**Task:** First, read in the data set `DesIntersection`. Then, create according to chunk `2.2.2` a table based on the function `kbl()` to visualize the intersections between the single magazines. `Check` your results.  

```{r "2.2.4", warning=FALSE}

#< fill_in

DesIntersection <- re___D_("DesIntersection.RDS")

row.names(_______________) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections", "Sum")

DesIntersection  %>%
kbl(__l_n____ = c("Intersection_NYT" = "∩ New York Times", "Intersection_LAT" = "∩ Los Angeles Times", "Intersection_BG" = "∩ Boston Globe",  "∩ Intersection_CHI" = "∩ Chicago Tribune", "∩ Intersection_WAPO" = "∩ Washington Post", "Intersection_DWSJ" = "∩ Wall Street Journal"), ______ = "Intersections between New York Times and other magazines") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("", 8, 8) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

#>

DesIntersection <- readRDS("DesIntersection.RDS")

row.names(DesIntersection) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections", "Sum")

DesIntersection  %>%
kbl(col.names = c("Intersection_NYT" = "∩ New York Times", "Intersection_LAT" = "∩ Los Angeles Times", "Intersection_BG" = "∩ Boston Globe",  "∩ Intersection_CHI" = "∩ Chicago Tribune", "∩ Intersection_WAPO" = "∩ Washington Post", "Intersection_DWSJ" = "∩ Wall Street Journal"), caption = "Intersections between New York Times and other magazines") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("", 8, 8) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

By focusing on New York Times reviews, we found several 201 books were reviewed by at least one other magazine. Conversely, a larger proportion of the reviews from other magazines were also rated by the New York Times. To determine the exact proportion, we calculated that at least 30 percent (Wall Street Journal) and at most 67 percent (Boston Globe) of non-New York Times magazines were also reviewed by the New York Times. As a result, estimates in this regard may be contaminated. In this context, contamination are intersections resulting from overlaps, whereby respective effects can no longer be clearly assigned. The row or column sum does not reflect the total number of reviews in the journals, as we only consider individual overlays. In reality, there are also multiple overlays where three or four journals review one book. 

Similarly, an uneven distribution of professional ratings in terms of author name recognition could lead to possible bias. For instance, if professional reviewers at the New York Times prefer certain well-known authors because of personal relations or preferences, those reviewers might tend to review or even recommend just those books rather than other books. The same applies for preferences to particular genres of the reviewers. Below, we use a density function to examine whether the professional reviewers at the New York Times and other magazines had any preference in selecting the books to be evaluated.   


#< info "How to work with `group_by`, `summarise` and other functions using the `dplyer` package."

The package `dplyer` is part of the package `tidyverse` and is used for data manipulation. Several functions from this package are used in the further course of this problem set. First, load the tidyverse package 

```{r eval = FALSE}
library(tidyverse)
```

Then, some important functions with individual explanation are listed below. 

```{r eval=FALSE}

# Starting sequence
data_set  <- data_set_manipulated %>%
 # Use filter() to filter out observations according to certain conditions  
  filter(x < 1 & x > 3)
 # Use group_by() to aggregate the data set to one or more variables
  group_by(variable1, variable2, ...)
 # Use summarise() associated with aggregation functions such as mean() or sum() to calculate new attributes
  summarise(new_column = mean(x), 
            New_column2 = sum(x), 
            ...) 
 # Use mutate() to add customized columns
  mutate(new_column_Ratio = x / y)
 # Use arrange() to arrange rows by specific variables (use a "-" to order them in ascending order)
  arrange(column_id, column_date)
```

**Note:** The function `rbind()` is not part of the `dplyer` package but also used for data manipulation. This function connects the rows of two or more data sets with each other. The condition is matching column names. 
  
#>



**Task:** Create a dataset `DesGenre` in which you calculate the proportion of professional reviews per book title. Differentiate according to the genre of the books (`genre`). Do not forget to check your solution. 

```{r "2.2.5", warning=FALSE}

#< fill_in

#Create Dataset "DesCrowd" to show the distribution among crowd ratings
DesGenre  <- data %>%
  group_by(_____) %>%
  summarise(Number_Ratings = n_distinct(_______),
            Number_NYT = n_distinct(titleno[which(data$DNYT == 1)]) / n_distinct(titleno),
            Number_OTH = n_distinct(titleno[which(data$____ == 1)]) / n_distinct(titleno))

# The formulation [,c(1, 3)] 
y1 <- DesGenre [,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2 <- DesGenre [,c(1, 4)] %>%
  mutate(Group = "Others")     

#Change the column names of y1 and y2
colnames(y1) <- c("Genre", "Number", "Group")
colnames(y2) <- c("Genre", "Number", "Group")

DesGenrefinal  <- rbind(__, __)
#>

DesGenre  <- data %>%
  group_by(genre) %>%
  summarise(Number_Ratings = n_distinct(titleno),
            Number_NYT = n_distinct(titleno[which(data$DNYT == 1)]) / n_distinct(titleno),
            Number_OTH = n_distinct(titleno[which(data$DOTH == 1)]) / n_distinct(titleno))

y1 <- DesGenre [,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2 <- DesGenre [,c(1, 4)] %>%
  mutate(Group = "Others")     

colnames(y1) <- c("Genre", "Number", "Group")
colnames(y2) <- c("Genre", "Number", "Group")

DesGenrefinal  <- rbind(y1, y2)

```

After we have created the data set, we can proceed with the creation of the chart.

**Task:** Create the graph representing the density of professional reviews among the number of published books. Save this graph as `f1` and use `facet_grid()` to split this graph by the variable `Group`.

```{r "2.2.6", warning=FALSE}

#< fill_in

#Create the graph based on book genres
 __ <- ggplot(__ta = ________) +
  facet_grid(~_____) +
  geom_bar(aes(x = Genre, y = Number/sum(Number), fill = _____), stat = "identity") +
  theme_bw() +
  labs(title = "Distribution of Professional Reviews on Book Genres",
       x = " Categories of Book Genres", 
       y = "Occurrence Density of professional reviews") +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = "bottom",
  axis.text.x=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())
f1
#> 

 f1 <- ggplot(data = DesGenrefinal) +
  facet_grid(~Group) +
  geom_bar(aes(x = Genre, y = Number/sum(Number), fill = Group), stat = "identity") +
  theme_bw() +
  labs(title = "Density of Professional Reviews among Book Genres",
       x = "Book Genres", 
       y = "Occurrence Density of professional reviews") +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = "bottom",
  axis.text.x=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())
f1

```

Every underlying bar represents the density of professional reviews on the individual book genre. In total, the Amazon data set provides information about 121 different book genres which are not listed in the chart for clarity. The aim is to identify possible distributional skews that might reject the assumption of randomized book selection by professional reviewers. Apparently, the New York Times and other magazines prefer to rate some of the genre categories more frequently than others. However, with the exception of about one-tenth of the book genres, the probability density for other books appears uniform. 

#< quiz "Genre_Guess"
question: What could this probably be due to?
sc:
- This could be due to the fact that book genres vary in their demand on reviews. For instance, the need for professional reviews is likely larger for novels than for travel guides.*
- The assumption of randomization can generally be rejected.
- Novels and other few categories generate higher sales than many other genres, making them stand out.
success: Great, your answer is correct!
failure: Try again.
#>

#< quiz "Distribution_Guess"
question: Focusing the **New York Times reviews**, What is your suggestion? 
sc:
- The more books an author publishes, the more likely these books will be professionally reviewed. 
- The more books an author publishes, the less likely these books will be professionally reviewed.
- The overall distribution is even.*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** `Check` the following chunk to create a density graph for the distribution on published books. 

```{r "2.2.7", warning=FALSE}

#< task
DesAuth <- readRDS("DesAuth.RDS")

f2 <- readRDS("f2.RDS")
f2
#>

```

The variable `numbooks` appropriates as an indication of prior author awareness. We focus on the distribution of professional reviews among the number of published books to review the assumption of randomization according to the top graph. For the New York Times, we overall note an even distribution across all popularity levels. In contrast, other magazines visibly preferred to specifically review books by already well-known authors. As a result, the assumption of randomization for other magazines can be rejected which calls into question all estimation results regarding the effects of other magazines in the further course of this examination.    

**Summary**

In summary, we have gained deeper insights into the relative and absolut numbers of professional reviews. It was illustrated to us that a relatively high proportion, between 30 and 67 percent, of all non-New York Times magazines were also reviewed by the New York Times, which may bias future estimates. To disprove the assumption that the selection of professionally reviewed books is not random, we used two variables (regarding the book genre and the Number of previous published books) to review whether the relative distribution on said variables disproves this assumption. For book genres, we found that the distribution across New York Times and other magazine reviews is uniform, so we can assume random selection of books based on their genre. Across the number of published books indicating the popularity of an author, we detected an even distribution for New York Times reviews and a left skew for reviews from other magazines. After all, the professional reviewers at the New York Times have discretion over about which books they actually review. Accordingly, we can never assume a complete random distribution.

## Exercise 2.3 -- Recognition of Potential Effects through Descriptive Approaches

Before we start identifying potential effect, we need a good overview of the relevant descriptive values. Generally, descriptive values are based on simple statistics to describe and to visualize contexts, that have already happened. These values are easily understood and can provide sound summaries of high data volumes within seconds. The most powerful function of descriptive analyses is the ability to identify relationships in order to make predictions using more complex statistics. This procedure is exactly the same as what we have already done in chapter 2.2. 

**Task:** Check the following chunk to read in the main data set.

```{r "2.3.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS") %>%
  arrange(canum, ddate)
#>
```

Starting with a good overview, we want to create a table including simple descriptive values distinguished by `country`. 

#< info "How to calculate with descriptice formulas in R"

R provides multiple formulas to calculate simple descriptive statistics. The R implementations is as follows:

* mean(x) - Calculates the mean of vector or column $x$.
* sum(x) - Calculates the sum of vector or column $x$.
* quantile(x, probs = 0.25) - Calculates the 25 percent quantile of vector or column $x$.
* NROW(data_set) - Calculates the number of rows of a data set.
* length(x) - Calculates the length of vector $x$ (or the number of columns of data_set).
* n_distinct(x) - Calculates the distinct number of observations of vector or column $x$.

**Note:** The `rownames()` function adds or edits the row names of a data set.

#>

**Task:** Create `dataDesKript` to create a descriptive chart of specific variables. When creating this chart, distinguish between data from the USA, Canada and the UK. Also read in `dataDesKript2` to add information about all countries. `Check` your result. 

```{r "2.3.2", warning=FALSE}

#< fill_in

# Descriptive Values differentiated by country
dataDesKript <- data %>%
   group_by(_______) %>%
   summarize(Price = round(mean(_____),2), 
            Star_Rating = round(____(R),2), 
            Sales_Rank = round(mean(rank),2), 
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quan____(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quan____(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quan____(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quan____(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = NROW(country), 
            Editions = n_distinct(asin))
#Read in dataDesKript2.RDS
dataDesKript2 <- ________("dataDesKript2.RDS")
# Merge Dataframes
DataDes <- rbind(dataDesKript, dataDesKript2)

# Transform Dataframe into a clearer schema
DataDesTest <- t(DataDes)
colnames(DataDesTest) <- rownames(DataDes)
DataDescriptive <- as.data.frame(DataDesTest)
colnames(DataDescriptive) <- unlist(DataDescriptive[1,])
DataDescriptive <- DataDescriptive[-1,]

# Change Row Names 
row.names(Data__________) = c("Price", "Star rating", "Sales rank", "Number of ratings", "10th", "25th", "50th", "75th", "90th", "Titles", "Observations", "Editions")

# Kable-Function to create an attractive overview
DataDescriptive %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)
#>

dataDesKript <- data %>%
   group_by(country) %>%
   summarize(Price = round(mean(pamzn),2), 
            Star_Rating = round(mean(R),2), 
            Sales_Rank = round(mean(rank),2), 
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quantile(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quantile(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quantile(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quantile(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = NROW(country), 
            Editions = n_distinct(asin))
#Read in dataDesKript2.RDS
dataDesKript2 <-readRDS("dataDesKript2.RDS")
# Merge Dataframes
DataDes <- rbind(dataDesKript, dataDesKript2)

# Transform Dataframe into a clearer schema
DataDesTest <- t(DataDes)
colnames(DataDesTest) <- rownames(DataDes)
DataDescriptive <- as.data.frame(DataDesTest)
colnames(DataDescriptive) <- unlist(DataDescriptive[1,])
DataDescriptivefinal <- DataDescriptive[-1,]

# Change Row Names 
row.names(DataDescriptivefinal) = c("Price", "Star rating", "Sales rank", "Number of ratings", "10th", "25th", "50th", "75th", "90th", "Titles", "Observations", "Editions")

# Kable-Function to create an attractive overview
DataDescriptivefinal %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)
```


The first four rows return the mean values of the book price, the Amazon Star Rating, the sales rank and the number of ratings per book. The book markets in all three considered countries do not have fixed book prices. Fixed book prices (FBP) means that the publisher has the exclusive right to set the price of his book. The retailer is not permitted to discount more than five percent from this set price (Nakayama, 2015).

#< quiz "Fixed_Prices"
question: Comparing UK (without FBP) and Germany (with FBP), which country accounted a higher price increase between 1996 (end of FBP in UK) and 2018? 
sc:
- United Kingdom (UK).* 
- Germany.
success: Great, your answer is correct! The UK accounted a price increase of 80 percent after this period, while Germany accounted an increase of 29 percent (Fuchs, Sprang, Beurich, Götz, 2019).
failure: Try again.
#>

Unfortunately, our data set only includes countries data from countries without FBP. Comparing the prices of the three available countries, we recognize a large price difference between Canada (21.07) and Great Britain (13.12) while the US account an average price of 15.86. Possible reasons for those expensive book prices could be that Canada imports many of these books where fees and other costs are incurred (Kwan, 2013), transportation costs over large land masses and a loss of economies of scale due to a smaller book market. The star ratings and their percentiles are quite even while large differences occur in the sales ranks and the number of ratings. Considering the fact that sales ranks are generated on their individual market place and less transparent, we cannot list any specific reasons for this. The high differences in the number of ratings may be due to the level of awareness of Amazon in the individual countries. Obviously, the US market accounts for more than twice as many observations as Canada or Great Britain.      

Hence, we gained insight into general descriptive values over the entire data set. To elaborate this, let us review the same values for observations where the availability of professional reviews is guaranteed. 

**Task:** Use the fuction `readRDS` to read the RDS-file `DataDescriptiveJournals`. Save this data set under the file name `DataDescriptiveJournals`. In Addition, create a table with `kbl()` according to the table above.

```{r "2.3.3", warning=FALSE}

#< task

#read out 
DataDescriptiveJournals <- readRDS("DataDescriptiveJournals.RDS")

# Kable-Function to create a table
DataDescriptiveJournals %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)
#>

```

Without differentiating between magazines or knowing whether the review turned out "positive" or "negative", we find an increase of price of about 15 percent, a decrease in average sales rank of about 39 percent, and the average number of ratings of about 86 percent, as well as significantly higher variance in percentiles of star ratings. The Amazon star rating also drops by a smaller percentage of about 2.3 percent. This leads to the assumption that the occurrence of professional reviews extensively affects the price, the sales rank and the number of ratings. 

The variance in percentiles of star ratings could be due to the fact that we have no information about how it has been reviewed, with which a polarization of the reviews may have taken place. For the New York Times reviews, the main data set contains information about whether the New York Times recommended a book using the variable `drecommended`. In the following, we generate two chronological graphs to show the descriptive affects from professional reviews on sales ranks and prices. 

**Note:** For further examinations we distinguish between the US data and the entire data, as the US market is the largest and has the most average crowd ratings (`cno` == 3 -> U.S.). 

**Task:** Execute the following chunk to create three different filtered data sets. Press `check` to collect your points.

```{r "2.3.4", warning=FALSE}

#< fill_in

#U.S. data recommended by New York Times 
dataReviewRec <- data %>%
  filter(drecommended == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(_, na.rm = TRUE))

#U.S. data reviewed by the New York Times but not recommended
dataReviewNRec <- data %>%
  filter(drecommended == _ & DNYT == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE))

#U.S. data not recommended or reviewed by The New York Times.
dataReviewNNYT <- data %>%
  filter(drecommended == _ & DNYT == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE)) 
#>

#U.S. data recommended by New York Times 
dataReviewRec <- data %>%
  filter(drecommended == 1 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE))

#U.S. data reviewed by the New York Times but not recommended
dataReviewNRec <- data %>%
  filter(drecommended == 0 & DNYT == 1 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE))

#U.S. data not recommended or reviewed by The New York Times.
dataReviewNNYT <- data %>%
  filter(drecommended == 0 & DNYT == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE)) 

```
 
After creating these three data sets, proceed to transform the data by creating the final dataset to visualize the results. 
**Task:** Use `data.frame` to create the data set for the following graph. Press `check` to execute your solution. 
 
```{r "2.3.5", warning=FALSE}

#< fill_in
dataNYTPrice <- data.frame(Price = c(dataReviewRec$AVGPrice, dataReviewNRec$AVGPrice, dataReviewNNYT$AVGPrice), SalesRank = c(dataReviewRec$AVGSalesRank, dataReviewNRec$AVGSalesRank, dataReviewNNYT$AVGSalesRank), Category = c("Recommended", "Not Recommended", "Not NYT"))
#>

dataNYTPrice <- data.frame(Price = c(dataReviewRec$AVGPrice, dataReviewNRec$AVGPrice, dataReviewNNYT$AVGPrice), SalesRank = c(dataReviewRec$AVGSalesRank, dataReviewNRec$AVGSalesRank, dataReviewNNYT$AVGSalesRank), Category = c("Recommended", "Not Recommended", "Not NYT"))

```
 
Next, this dataset is presented in two charts to distinguish between different expressions of New York Times reviews.  
 
**Task:** Try to fill in the gaps to create the following plot `f3`. Use `geom_text` to label the bars. Then, read in and the save the plot `f4` correspondingly. Finally, use `grid.arrange` to plot these graphs side by side. Press `check` to confirm.  
 
```{r "2.3.6", warning = FALSE}

#< fill_in

#Plot 1: Mean Prices
f3 <- ____________ %>%
   ggplot() +
   geom_histogram(___(y = _____, x = ________,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = _____ - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ylim(0, 21) +
  # geom_bar(aes(y = SR, x = Category,  fill = Category), position = "dodge", stat = "identity")
 scale_fill_manual("", values = c("Not NYT" = "deepskyblue", "Not Recommended" = "darkorchid2", "Recommended" = "blue3")) +
  labs(title = "Differences in mean prices (US Data only)",
       x = "", 
       y = "Price in USD") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f4 <- readRDS("f4.RDS")
#Side by side plot
grid.arrange(__, __, ncol = 2)

#>

#Plot 1: Mean Prices
f3 <- dataNYTPrice %>%
   ggplot() +
   geom_histogram(aes(y = Price, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = Price - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ylim(0, 21) +
  # geom_bar(aes(y = SR, x = Category,  fill = Category), position = "dodge", stat = "identity")
 scale_fill_manual("", values = c("Not NYT" = "deepskyblue", "Not Recommended" = "darkorchid2", "Recommended" = "blue3")) +
  labs(title = "Differences in mean prices (US Data only)",
       x = "", 
       y = "Price in USD") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f4 <- readRDS("f4.RDS")
#Side by side plot
grid.arrange(f3, f4, ncol = 2)

```

The Price differences between New York Times recommended and non-recommended books is not significant while there is a big price gap between New York Times data and non new York Times data. The fact that the price difference between recommended and non-recommended books is quite small could be attributable to the fact that a non-recommendation does not always equate to a poor rating. On the other hand, books not mentioned in the New York Times are on average about $2.8 US cheaper than books mentioned in the New York Times. Observing the average sales ranks, we determine clearer differences between these three categories. Since the sales ranks are not direct quantity data, they can still serve as a quite useful comparative value. We see a decline of approximately 35% in sales rank from category to category. 

#< quiz "Prices_and_SalesRanks"
question: Which of the following statements may be made? 
sc:
- With the information of these graphs we can assume a price elasticity ε < 1 for the US book market. 
- None of those listed.
- With the information of these graphs we can assume a price elasticity ε > 1 for the US book market.
- The effect of professional reviews on sales ranks (quantities) could be higher than the effect on prices.*
success: Great, your answer is correct! 
failure: Try again.
#>

We want to conduct a similar examination for crowd ratings, with the difference that we have information not only on the occurrence of these ratings, but also on their level on the five-point scale. Above, we already noted a negative association between the Amazon star rating and the occurrence of professional reviews. 

Before we start, let us divide Amazon star ratings into three categories: less than three stars, three and four stars, and more than four stars. 

**Task:** Create three dummy variables that show `1` when the star rating is between one and three, three and four and more than five. Orientate on exercise `2.2.1` to create a dummy variable. Then, create three filtered data sets according to `2.3.3`. Name these data sets `dataCR1_3`, `dataCR3_4` and `dataCR4_5`.

**Note:** Professionally reviewed books gets filtered out to focus on star ratings that could not affected by professional reviews.    

```{r "2.3.7", warning = FALSE}

#< fill_in 

data$starRating1_3 <- ifelse(data$R >= _ & data$R < _, 1, 0)
data$starRating3_4 <- ifelse(data$R >= _ & data$R < _, 1, 0)
data$starRating4_5 <- ifelse(data$R >= _ & data$R < _, 1, 0)

dataCR1_3 <- data %>%
  filter(data$starRating1_3 == _ & DALL == 0 & cno == 3) %>%
  _________(AVGPrice = ____(pamzn), 
            AVGSalesRank = ____(rank, na.rm = TRUE))

dataCR3_4 <- data %>%
  filter(data$starRating3_4 == _ & DALL == 0 & cno == 3) %>%
  _________(AVGPrice = ____(pamzn), 
            AVGSalesRank = ____(rank, na.rm = TRUE))

dataCR4_5 <- data %>%
  filter(data$starRating4_5 == _ & DALL == 0 & cno == 3) %>%
  _________(AVGPrice = ____(pamzn), 
            AVGSalesRank = ____(rank, na.rm = TRUE))

#> 

data$starRating1_3 <- ifelse(data$R >= 1 & data$R < 3, 1, 0)
data$starRating3_4 <- ifelse(data$R >= 3 & data$R < 4, 1, 0)
data$starRating4_5 <- ifelse(data$R >= 4 & data$R < 5, 1, 0)

dataCR1_3 <- data %>%
  filter(data$starRating1_3 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))

dataCR3_4 <- data %>%
  filter(data$starRating3_4 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))

dataCR4_5 <- data %>%
  filter(data$starRating4_5 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))

```

**Task:** We continue with the same procedure as for `2.3.4` and `2.3.5`. Now, execute the following chunk to create the needed data set. Press `check` to confirm. 

```{r "2.3.8", warning=FALSE}

#< task
dataCRPrice <- data.frame(Price = c(dataCR1_3$AVGPrice, dataCR3_4$AVGPrice, dataCR4_5$AVGPrice), SalesRank = c(dataCR1_3$AVGSalesRank, dataCR3_4$AVGSalesRank, dataCR4_5$AVGSalesRank), Category = c("1-3 stars", "3-4 Stars", "more than 4 stars"))
#>

```
 
**Task:** Create `f5` to create a chart that shows the average prices according to their star ratings. Use `ylim` to define the y-axis scale between 0 and 21 (note that the diagram has been flipped). Do not forget to press `Check`.

```{r "2.3.9", warning= FALSE}

#< fill_in

#Plot 1: Mean Prices
f5 <- dataCRPrice %>%
   ggplot() +
   geom_histogram(aes(y = Price, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = Price - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_____() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ___________ +
 scale_fill_manual("", values = c("1-3 stars" = "deepskyblue", "3-4 Stars" = "darkorchid2", "more than 4 stars" = "blue3")) +
  labs(title = "Differences in Mean Prices (U.S. Data Only)",
       x = "", 
       y = "Price in USD") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f6 <- readRDS("f6.RDS")

____.arrange(f5, f6, ____ = 2)

#>

f5 <- dataCRPrice %>%
   ggplot() +
   geom_histogram(aes(y = Price, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = Price - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ylim(0, 21) +
 scale_fill_manual("", values = c("1-3 stars" = "deepskyblue", "3-4 Stars" = "darkorchid2", "more than 4 stars" = "blue3")) +
  labs(title = "Differences in Mean Prices (U.S. Data Only)",
       x = "", 
       y = "Price in USD") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f6 <- readRDS("f6.RDS")

grid.arrange(f5, f6, ncol = 2)

```

In contrast to the affects of professional reviews, the mean price exhibits an inverse behavior. The Price seems to decrease the more stars a rating has while sales ranks generally behave according to the occurrence of professional reviews. Obviously, crowd ratings affect prices differently than professional reviews in magazines. This could be due to professional reviews triggering a (short-term) increase in demand that may not be served in the short term, leading to a price increase. On the other hand, higher crowd ratings on Amazon could rather resulting in long-term increase in demand, which leads to more contested competition, where everyone has to reduce their prices. In fact, book pricing depends on more factors than are included in our data set. 

The mean sales rank decreases the more stars were given. As we noticed, more than 75 percent of all observations were rated with four stars or higher, so we can assume a similar percentage of observations in the upper bar (565063). Nevertheless, the mean sales rank for books recommended by the New York Times is significantly lower than the mean sales rank of books rated more than four stars by the crowd. Nevertheless, the mean sales rank for New York Times-recommended books is significantly lower than the mean sales rank for crowd ratings rated higher than four stars. 

To verify whether absolute book prices remain constant over time, we create a timeline for the average book price. We only observe for top 10000 and top 1000 sales ranks because we claim higher time-based dependencies. 

**task:** Press `check` to create and merge the data sets `dataTop10000` and `dataTop1000` by `ddate`, to visualize the price development for observations for which applies $SalesRank < 10.001$ and $SalesRank < 1.001$. 

```{r "2.3.10", warning=FALSE}
#< task
dataTop10000 <- data %>%
  filter(cno == 3 & rank <= 10000) %>%
  group_by(ddate) %>%
  summarise(price = mean(pamzn), 
            Group = "Top 10.000")

dataTop1000 <- data %>%
  filter(cno == 3 & rank <= 1000) %>%
  group_by(ddate) %>%
  summarise(price = mean(pamzn), 
            Group = "Top 1.000")

dataTop <- rbind(dataTop10000, dataTop1000)

dataTop %>%
   ggplot() +
   theme_bw() +
   geom_line(aes(x = ddate, color = Group,  y = price), stat = "identity", width = 0.3,  position = position_dodge()) +
   scale_color_manual("Category", values = c("Top 10.000" = "deepskyblue", "Top 1.000" = "red"))  +
   labs(title = "Differences in mean Sales Ranks (US Data only)",
        x = "Date", 
        y = "Mean Price in USD")  
#>  
```

Overall, prices rise within one year by approximately $1.5 US. Price collapses can be observed in the spring, and there is a massive drop in prices in the summer months from June to September, with average prices (Top 1.000) falling by around 14 percent. The slump at the end of November is more like an outlier value. Obviously, better sold books tend to vary in price due to specific seasons. Lower prices in summer could be due to lower demand, as people prefer to read during the winter months. For the following event study in chapter 3.3 we have to take into account that there are fluctuations in book prices within a year and that these can be attributed to various factors. 

**Summary**

Using descriptive analysis, we gained deeper insight into the data and were able to establish initial associations between variables related to pre-purchase information and price or quantity (sales ranks). First, we created an overview over general values of the underlying data set. By selectively filtering out certain observations that were effected by pre-purchase information from professional reviewers, we found differences primarily in prices and sales ranks. We have deepened our research and found out that both professional reviews and Amazon crowd ratings negatively affect the sales rank (a lower rank implies higher volumes sold). Conversely, crowd ratings lower average prices while professional reviews raise average prices. Finally, we reviewed whether book prices fluctuating within one year and found out that the more "successful" books are, the more the price fluctuates within one year. For long-term studies, we need data that go beyond a one-year horizon. 

In the following chapter, we start with the empirical part of this problem set. 

## Exercise 3 -- Empirical Strategies on Sales Ranks and Prices (ca. 4650)

In following chapter *Empirical Strategies on Sales Ranks and Prices* we focus on empirical methods in general and their application to real data from Amazon. 

First, we give an introduction to empirical basics about the subject of regressions and the specific methods that come along with it. In general, it deals with control variables, fixed effects, robust standard errors, and the use of logarithmic estimates. 

In chapter 3.2, we focus on the replication of the main regression originally created by Reimers and Waldfogel. We discuss different of those effects and deepen the examination with further regressions. Based on these regressions on real data, we also focus on the validity of regressions and the explanation of values that inform them. 

Chapter 3.3 explains the subject of event studies. We also conduct two event studies based on the underlying data set to identify long- and short-term reactions of sales ranks and prices to professional reviews. 

Working through this chapter will provide fundamentals to advanced empirical strategies that are used to make predictive statements in economic science. 

### Structure

3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
3.2 Estimation of the Effects on Sales Ranks and Prices
 
3.3 Introduction and Implementation of Event Studies

## Exercise 3.1 -- Regressions, Robust Standard Errors and Fixed Effects

Before we get into the subject matter of regressions, we learn how to classify this topic. In general, regressions describe a quantitative statistical approach to explaining associations between a dependent variable and one (linear) or more (multiple) independent (explanatory) variables in order to gain predictive information. The formula for a linear regression is as follows:

$$ \hat{y} = \hat{β_0} + \hat{β_1}x_1 + \varepsilon $$

where $\hat{y}$ indicates the dependent variable and $x_1$ represents the explanation variable. $x_0$ denotes the so called intercept on the y-axis, id est the average value of y if the explanatory variable x1 has no influence. ε is the error term and shows the sum of the residuals from the regression. Finally, $\hat{β_1}$ indicates the average increase or decrease of y for every unit increase of x1. Here, the value of β1 is determined by the Ordinary least squares method (OLS). Further information about this method you find [here](https://www.mathsisfun.com/data/least-squares-regression.html).                                      

#< quiz "Simple_Regression"
question: We want to estimate the effect from years of education (x1) on the income (y). We determined the following formula for 1.000 people with ŷ = 500 + 300*x1. What is the correct answer? 
sc:
- With ten years of education you exactly earn 3500 monetary units. 
- Within the 1.000 individuals, each additional year in education results in 300 more monetary units.
- We only have 1.000 observations - we cannot make a statement.
- Within the 1.000 individuals, each additional year in education results in an average of 300 more monetary units -> to make statements beyond our sample, we rather need more data and more variables.*  
success: Great, your answer is correct!
failure: Try again.
#>

Obviously, the amount of income depends not only on the years of education (yoe). In reality, many more measurable variables affect the amount of income. For instance, the older people get the more they tend to earn. Hence, the age also effects the amount of income. In following we add the variable age as a so called *control variable* to the regression from the quiz:  

$$ y = \hat{β_0} + \hat{β_1}yoe + \hat{β_2}age + \varepsilon $$

Now we directly control for the effect of age on income, so that age no longer affects the coefficient on price. However, adding to many control variables could lead to over fitting, which means that the model is overfitted to the specific sample, so that the model reacts to random variation instead of actual contexts. 

#< quiz "Multiple_Regression"
question: What is the correct answer? 
sc:
- Adding the age to the regression increases the value of β1. 
- Adding the age to the regression decreases the value of β1.*
- Adding the age to the regression does not influence the coefficient of yoe.  
success: Great, your answer is correct!
failure: Try again.
#>

The ideal number of control variables generally depends on the research question and the number of observations. But even with a high frequently data set like the Amazon data set it would be recommended not to control for too many variables (Kranz, 2022). 

**Task:** Check the following chunk to read in the main data set.

```{r "3.1.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS") %>%
  arrange(canum, ddate)
#>
```

We then perform a multiple regression in R. Regression results are usually presented in the form of tables, listing the values of the coefficients. For illustration, we regress the sales rank on the price from the main Amazon data set:  

$$ y = x_0 + β_1Price + β_2StarRating + \varepsilon $$


#< info "How to draw samples in R"

To randomize examinations in R, random samples can be drawn. Likewise, it can be useful not to count on all data all the time for performance reasons. The following chunks shows how to draw samples in R. 

```{r eval = FALSE}
      # set.seed() is used to fix a sample (without this command there would be another sample every time after execution)
set.seed(123)
      # create sample data using the function sample()
sample_data <- data_set[sample(nrow(data_set), size = number_of_targeted_observations, replace = TRUE),]

```

As a result, we draw a sample in amount of `number_of_targeted_observations`. 

#>


**Task:** Replace the ___ gaps with the correct code. Use the function `lm` to estimate the regression above. Do not forget to press `check`.

```{r "3.1.2", warning = FALSE}
#< fill_in

set.seed(123) 
# We draw a sample of 100.000 observations for performance purposes from `data`
dataLite <- data[sample(nrow(____), size = _____, replace = TRUE),]
# We use the function lm() to regress `pamzn` and `R` on `rank`
regFirstStep <- __(rank ~ _____ + _, data = dataLite)
# The function summary() creates a regression table
summary(regFirstStep)
#>

set.seed(123)
dataLite <- data[sample(nrow(data), size = 100000, replace = TRUE),]
regFirstStep <- lm(rank ~ pamzn + R, data = dataLite)
summary(regFirstStep)

```

The first column, starting from row two, contains the estimated coefficients for `pamzn` and `R`. The estimate value of `pamzn` indicates that an increase of price by one unit on average results in an increase of the sales rank by 13009.4 positions. In the second column, the standard error is given. This is a calculation to determine the accuracy of the individual estimator. Basically, the lower the standard error is, the less the estimator varies. The formula is as follows and can be found [here](https://bookdown.org/mike/data_analysis/linear-regression.html),

$$\widehat{SE}(\hat\beta_k)=s\sqrt{[(\mathbf{X}^T\mathbf{X})^{-1}]_{kk}}$$

Where $s$ is the standard error of the entire regression, $X$ the the covariance matrix, and $\hat\beta_k$ the regression coefficient. The t-value is the estimation coefficient divided by the individual standard error. The last column shows the significance level, which indicates the probability of the estimator being exactly as high as it is merely by chance.

#< quiz "t_value"
question: What is the correct answer? 
sc:
- A high level on significance indicates an expressive model. 
- The significance level gives no information about the expressiveness of the model.
- To determine the expressiveness, we need a combination of the significance level and other values.*  
success: Great, your answer is correct! Standard errors, the number of observations and the R-squared value also affect the expressiveness of a model. 
failure: Try again.
#>

The R-squared value in the regression table below indicates a coefficient of determination within $[0;1]$, which provides information on how the explanatory variables fit the respective model. In this context, a perfect R-squared value of 1 means a perfect coefficient of determination. In practice, the R-squared value increases with a higher number of observations and more or better chosen explanatory variables. Adding too many variables to the regression might give the illusion of a higher R-squared value, but it also leads to overfitting. 

#### **Robust Standard Errors**

To apply regressions correctly, certain assumptions must be made. One of these assumptions is **homoskedasticity**. To fulfill this assumption, the residuals of the regression must be uniformly distributed. **Robust Statistics** addresses making estimates that are insensitive to small changes in the basic assumptions like outlier values falsifying the regression residuals (Prof.Dr. Rachev, 2007). In R, there are different methods to implement Robust Standard Errors, all of which follow a similar approach. Basically, this algorithm determines the coefficients by disregarding outlier values and other disruptive factors. 

**Task:** Solve the following chunk to run a regression with robust standard errors. Do not forget to press `check`.

```{r "3.1.3", warning=FALSE}
#< fill_in
# Load the package `fixest` to use the following functions feols()
________(fixest)
# We use the function feols() to add Fixed Effects and Robust Standard Errors. Use `dataLite` for your estimate.
regRSE <- _____(____ ~ pamzn + R, vcov = "hetero", data = ________)
# The function summary() creates a regression table
_______(regRSE)
#>

library(fixest)
regRSE <- feols(rank ~ pamzn + R, vcov = "hetero", data = dataLite)
summary(regRSE)
```

There are almost no differences in the coefficients for `pamzn` and `R`, while there are large differences in the standard errors. If the assumption of homoskedasticity is fulfilled, estimators without robust standard errors tend to have lower standard errors. This could be due to the fact, that Robust Standard Errors take uncertainties into consideration which makes the estimate somewhat less imprecise. However, Robust Standard Errors should still be used to guarantee homoscedasticity. 

#< quiz "Robust_standard_errors"
question: We assume that (for the same regression) the standard errors for the regular regression are lower than for the regression where robust standard errors were used. Whats is the correct answer? 
sc:
- You can omit the robust standard errors, as they reduce the accuracy of the coefficients. 
- Robust standard errors should generally be added to any regression, as regressions without them are generally inaccurate.
- It is advisable to add robust standard errors but not necessary. Even with higher standard errors, it may be important to avoid heteroscedasticity.*  
success: Great, your answer is correct!  
failure: Try again.
#>

**Note:** Heteroscedasticity is the opposite of homoskedasticity.

#### **Fixed Effects**

To make sense of so-called fixed effects, we start with an explanation of fixed effects followed by their implementation in R. Before we start explaining fixed effects, we focus on the **endogeneity problem** of explanatory variables. The endogeneity problem occurs when the explanatory variable $x_k$ depends on the error term $\varepsilon $. For instance, we examine the effect of price (as the only explanatory variable) on the sales rank. Since the book price is determined by many other factors, the price appears to be an endogenous variable. As a result, biases arise and the respective coefficient could become inaccurate. When using fixed effects, we select certain variables to control for. These fixed effects are kept constant and do not affect the estimation. Using this method, we attempt to reduce the variation within explanatory variables by minimizing the potential for bias from omitted variables (Hill, Davis, Roos and French, 2020).

#< quiz "Fixed_Effects"
question: What is your suggestion, does the implementation of fixed effects significantly affect the value of the coefficients of the explanatory variable(s)? 
sc:
- Yes.* 
- No.
success: Great, your answer is correct!  
failure: Try again.
#>

Let us look at an example for this. We want to create a regression where the sales rank indicates the dependent variables $y$ and $x_1$  as explanatory variable for the Amazon book price. The formula looks as follows:  

$$ y = β_0 + β_1 x_1 + \varepsilon $$

In the following, the variable `canum` (chapter 2.1) serves us as fixed effects. The regression considers the variable canum as fixed, controlling for each expression of the variable `canum`. Thus, book- and country-specific differences should eliminate effects on the price.  

**Task:** Replace the ___ gaps to create a regressions with fixed effects. Compare this regression `regFirstStep` with a regression without differences occur. Finally, we display the results with  `modelsummary`. Press `check` after solving this exercise. 

```{r "3.1.4", warning =FALSE}

#< fill_in

# Load the package `modelsummary` to gain a more handsome regression table
library(____________) 
# Create a regression using `canum` as fixed effects. Add robust standard errors. 
regFE <- feols(____ ~ pamzn | _____, ____ = "he____", data = dataLite)
# Create a regular regression without robust standard errors and fixed effects.
regWFE <- lm(rank ~ pamzn, data = dataLite)
modelsummary(list(regWFE, regFE), statistic = "({std.error})", coef_rename = c("pamzn" = "Amazon Price"))
#>

library(modelsummary) 
regFE <- feols(rank ~ pamzn | canum, vcov = "hetero", data = dataLite)
regWFE <- lm(rank ~ pamzn, data = dataLite)
modelsummary(list(regWFE, regFE), statistic = "({std.error})", coef_rename = c("pamzn" = "Amazon Price"))

```

Column two shows the fixed effects estimate with robust standard errors, while column one shows the regular regression. It is noticeable that the regressions estimate completely different values for the coefficients. Chapter 2.3 showed price differences that can be attributed to the respective country in which the book was rated. The regular regression seems to show exactly this endogeneity problem, that the Amazon price also depends on the country where the book was rated, as well as many other factors. Using fixed effects, we eliminated the country-specific effect and obtained the information that the Amazon price and the sales rank are not as strong related as we originally assumed.  

The fixed-effects model requires a so-called panel data structure that has data available for each individual (here: books) at different points in time. However, the addition of fixed effects can also lead to inaccuracies and biases. The disadvantage is that the variables used as fixed effects can no longer be included in the regression as explanatory variables. This model also assumes that the explanatory variable is not collinear (perfectly correlated) to the fixed effects, otherwise this explanatory variable would be determined purely by the fixed effects. Finally, the endogeneity problem can not get completely eliminated by a fixed-effects model. Better suited for this purpose is the Difference-in-Difference approach or Instrumental Variable Estimation, which are not further discussed in the course of this problem set.                                                      

**Summary**

To sum up, this chapter has given us a handsome overview about some basics and advanced methods and tools for working with regressions. First, we learned how regressions basically work, how to interpret them using particular values, and how they are implemented in R. We distinguished between linear and multiple regressions and how control variables potentially affect multiple regressions. We found multiple differences in the regression coefficients and the coefficients of determination when we added control variables. Second, the effect of Robust Standard Errors was discussed and implemented in R. We extracted the information that inserting Robust Standard Errors can be quite useful to ensure homoskedasticity. Finally, we learned how to explain and to implement fixed effects. Application on the Amazon data set underscored the utility and potential of fixed effects for subsequent examinations. 

In the following chapter, we apply the methods and tools from this chapter to examine the overall effects on sales ranks and prices.

## Exercise 3.2 -- Estimation of the Effects on Sales Ranks and Prices

After the introduction for regressions, we focus on the research question to estimate the effect from professional reviews and crowd ratings on the book market. Replicating the underlying regressions by Reimers and Waldfogel, the data set must first be transformed.  

**Task:** Check the following chunk to read in the main data set.

```{r "3.2.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS")
#>

```

**Task:** Create the data set `dataUS` that only includes observations from the US market. To enable this, use the condition `cno == 3`. Finally, use `arrange()` to sort the data by `canum` and then by `ddate`. Press `check` to confirm your solution. 

```{r "3.2.2", warning=FALSE}
#< task
#>
dataUS <- data %>%
    filter(cno == 3) %>%
    arrange(canum, ddate)
```

In their paper, Reimers and Waldfogel used logarithmic values for their estimation. The logarithmization of dependent or explanatory variables can be associated with many advantages. First, logarithmization can be used to stabilize the variance of the residuals to avoid heteroskedasticity. Further, outlier values are mitigated in their effect so that they have less influence on the magnitude of the regression coefficient. The third advantage is due to interpretation of the regression coefficients. Logarithmized coefficients allow us to interpret these coefficients as percentages for simplicity. The possible interpretation are as follows (Kranz, 2022):

* **log - log**: log $\hat{y}$ = $\hat\beta_0$ + log $β_1 x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately $β_1$ percent. 

* **log - level**: log $\hat{y}$ = $\hat{β_0}$ + $\hat{β_1} x_1$, for a one unit increase in $x_1$, the predicted value of $y$ increases by approximately 100 * $β_1$ percent.

* **level - log**: $\hat{y}$ = $\hat{β_0}$ + log $\hat{β_1} x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately 0.01 * $β_1$ units. 

The regressions created by Reimers and Waldfogel are based on the **log - log** method. In their main regression table, they created five different regressions using `lrank` as dependent variable. The variable `L1.lrank` specified the lagged sales rank from the last date documented in the data set. The addition of `L1.lrank` has the advantage that seasonal fluctuations are included in the regression. Similarly, this variable is predestined for estimates such as this one because the variable can capture short-term (triggered by professional reviews) increases in sales rank.    

**Task:** Run the following chunk to create `reg2` with `ano` as fixed effects and robust standard errors. 

```{r "3.2.3", warning=FALSE}

#< task

#read the package `fixest`
library(fixest)
#Create the regression `reg2`
reg2 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR  + dnytpost1_3 + dnytpost6_3 +  dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | ano, vcov = "hetero", data = dataUS)
#>
```

Computing regressions in this high-frequency data set requires high computational power. For this reason, we compute only two of these regressions within the problem set. Now, we want to generate the second regression using more explanatory variables. 

**Task:** Create `reg3` according to `reg2` above. Add the explanatory variables `lrR`, `dnytpost10_1`, `dnytpost10_2`, `dothpost_1`, `dothpost_2`, `dothpost_3`, `dothpost10_1` and `dothpost10_2`. Remove the explanatory variables `dothpostpre_1`,`dothpostpre_2` and `dothpostpre_3`.

```{r "3.2.4", warning=FALSE}
#< task
# Create reg3 here
#>
reg3 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR + lrR + dnytpost1_3 + dnytpost6_3 + dnytpost10_1 + dnytpost10_2 +  dnytpost10_3 + dothpost_1 + dothpost_2 + dothpost_3 + dothpost10_1 + dothpost10_2 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | ano, vcov = "hetero", data = dataUS)

```

The variable `ano` gives an Amazon identifier (asin) and the numbers `1`, `2` and `3` after the dummy variables `dnytpost` and `dothpost` provide information about which country the observation belongs to. U.S. Data were used for both regressions.  

Subsequently, the three remaining regressions are then read in. To the first regression `reg1` some dummy variables were added as control variables. These dummy variables  For performance reasons we have omitted these variables in this task, but they are stored in `reg1` which we will read in. In the interval $[-20;40]$ for `NYT_elapse` and `OTH_elapse` all expressions were stored individually in different dummy variables, in sum 120 control variables.

**Task:** check the following chunk to run `reg1`, `reg4` and `reg5`. 

```{r "3.2.5"}
#< task_notest
reg1 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR + DNYT + DNYT0 + DNYT1 + DNYT2 + DNYT3 +DNYT4 + DNYT5 + DNYT6 + DNYT7 + DNYT8 + DNYT9 + DNYT10 + DNYT11 + DNYT12 + DNYT13 +DNYT14 + DNYT15 +DNYT16 + DNYT17 +DNYT18 +DNYT19 + DNYT20 + DNYT21 + DNYT22 + DNYT23 + DNYT24 + DNYT25 + DNYT26 + DNYT27 + DNYT28 + DNYT29 + DNYT30 + DNYT31 + DNYT32 + DNYT33 + DNYT34 + DNYT35 + DNYT36 + DNYT37 + DNYT38 + DNYT39 +DNYT40 + DNYTm1 + DNYTm2 + DNYTm3 + DNYTm4 + DNYTm5 + DNYTm6 + DNYTm7 + DNYTm8 + DNYTm9 + DNYTm10 + DNYTm11 + DNYTm12 + DNYTm13 + DNYTm14 + DNYTm15 + DNYTm16 + DNYTm17 + DNYTm18 + DNYTm19 + DNYTm20 + DOTH0 + DOTH1 + DOTH2 + DOTH3 +DOTH4 + DOTH5 + DOTH6 + DOTH7 + DOTH8 + DOTH9 + DOTH10 + DOTH11 + DOTH12 + DOTH13 + DOTH14 + DOTH15 +DOTH16 + DOTH17 + DOTH18 + DOTH19 + DOTH20 + DOTH21 + DOTH22 + DOTH23 + DOTH24 + DNYT25 + DNYT26 + DNYT27 + DNYT28 + DNYT29 + DNYT30 + DNYT31 + DNYT32 + DNYT33 + DNYT34 + DNYT35 + DNYT36 + DNYT37 + DNYT38 + DOTH39 +DOTH40 + DOTHm1 + DOTHm2 + DOTHm3 + DOTHm4 + DOTHm5 + DOTHm6 + DOTHm7 + DOTHm8 + DOTHm9 + DOTHm10 + DOTHm11 + DOTHm12 + DOTHm13 + DOTHm14 + DOTHm15 + DOTHm16 + DOTHm17 + DOTHm18 + DOTHm19 + DOTHm20 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

reg4 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR  + dnytpost1_1 + dnytpost1_2 + dnytpost1_3 + dnytpost6_1 + dnytpost6_2 + dnytpost6_3 + dnytpost10_1 + dnytpost10_2 +  dnytpost10_3 + dothpost_1 + dothpost_2 + dothpost_3 + dothpost10_1 + dothpost10_2 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_1 + dnytpost1r_2 + dnytpost1r_3 + dnytpost6r_1 + dnytpost6r_2 + dnytpost6r_3 + dnytpost10r_1 + dnytpost10r_2 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = data)

reg5 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR + lrR + dnytpost1_1 + dnytpost1_2 + dnytpost1_3 + dnytpost6_1 + dnytpost6_2 + dnytpost6_3 + dnytpost10_1 + dnytpost10_2 +  dnytpost10_3 + dothpost_1 + dothpost_2 + dothpost_3 + dothpost10_1 + dothpost10_2 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_1 + dnytpost1r_2 + dnytpost1r_3 + dnytpost6r_1 + dnytpost6r_2 + dnytpost6r_3 + dnytpost10r_1 + dnytpost10r_2 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = data)
#>

```

After all regressions are available, proceed with the generation of a regression table for comparison purposes. For this purpose, we use the function `modelsummary`, which can display multiple regressions more clearly and with more possibilities than `summary`.

**Task:** Use the function `modelsummary` from the R package `modelsummary` to display the five regressions `reg1`, `reg2`, `reg3`, `reg4` and `reg5`. Do not forget to `check` this chunk. 

```{r "3.2.6"}

#< fill_in

RegTable <- ____________(list(____, reg2, ____, ____, ____), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("L1.lrank" = "Lagged log sales rank", "lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable 

#>

RegTable <- modelsummary(list(reg1, reg2, reg3, reg4, reg5), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("L1.lrank" = "Lagged log sales rank", "lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable 

```

The results of the upper estimation deviate visibly  from the underlying results of Reimers and Waldfogel. This deviation is likely due to the variable `L1.lrank`, which is originally calculated with an specific Stata (the programming language used by the authors for the original data processing) function called `L1`. For my calculation of this variable I created a manual function, which can be found [here](t.b.d.). Visibly, the coefficient for `L1.lrank` turns out to be lower than calculated by the authors. Consequently, the actual effect of seasonal effects emanating from the variable `L1.lrank` could potentially be smaller than calculated by Reimers and Waldfogel. 

#< quiz "Reg_Assumptions"
question: Which of these assumptions can be made? 
sc:
- The impact of the New York Times is only short-term, as the coefficient becomes smaller over time.
- The R-squared error of over 0.95 is due to overfitting after adding these amount of explanatory variables. 
- Log Amazon price (the coefficient) > 0, so an increasing price appears to increase sales rank and implies a decrease in sales volume.*
success: Great, your answer is correct!  
failure: Try again.
#>

The coefficient for `NYT: 0-5 days` represents the occurrence of a New York Times review in the US within the first five days of its publication.

#< quiz "Coeff_Interpretation"
question: How should the coefficient for column 2 `NYT 0-5 days` be interpreted? 
sc:
- A professional review in the New York Times lowers the sales rank by an average of 0.28 percent within the first five days of publication.
- A professional review from the New York Times lowers the sales rank by an average of 0.280*100 units within the first five days of publication.
- A professional review from the New York Times lowers the sales rank by an average of 28 percent within the first five days of publication.*
success: Great, your answer is correct!  
failure: Try again.
#>

The coefficient for `Log Amazon Price` is 0.083. This value indicates that a price increase on average increases the sales rank.

#< quiz "Coeff_Interpretation2"
question: How should the coefficient for column 2 `Log Amazon Price` be interpreted? 
sc:
- By increasing the price for one unit, the sales rank increases by an average of 0.084*100 units.
- By increasing the price for one percent, the sales rank increases by an average of 0.084 percent. As a result, the price is definitely elastic.
- By increasing the price for one percent, the sales rank increases by an average of 0.084 percent. There is no information about price elasticity.
success: Great, your answer is correct! Concrete quantities are needed to measure price elasticity.   
failure: Try again.
#>

Expectedly, the New York Times recommendations are associated with the greatest effect on sales ranks. The coefficient accounts approximately -0.34 for U.S. data and about -0.36 for all data. In column three, an *interaction term* between `lR` and `lreview` was used. Generally, interaction terms are used to control for bias when an explanatory variable also depends on another independent variable. In this case, the addition of these interaction term implies a shrinking effect of the star ratings. This suggests that star ratings have more influence depending on multiple underlying ratings (Reimers, Waldfogel, 2021).  

In the following, price is examined as independent variable. In chapter 2.3, the assumption was made that New York Times reviews could increases the price in the short-term. To investigate this further, we regress price on similar explanatory variables as in the regression above.        

**Task:** Create an according regression by filling in the gaps. Use the variable `canum` as fixed effects.

```{r "3.2.7"}
#< fill_in
reg6 <- fe___(______ ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 +  dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | _____, vcov = ________, data = dataUS)

reg7 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

#>

reg6 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + lrR + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

reg7 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

```

**Task:** Run the following chunk to visualize the regression with `modelsummary`.

```{r "3.2.8"}

#< task_notest
RegTable2 <- modelsummary(list(reg6, reg7), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable2 
#>

```

The occurrence of New York Times reviews within the first five days of publication is associated with a 0.01 percent price increase. This price increase remains relatively even over time looking at review occurrence within five days to 20 days of publication. Chapter 2.3 also assumed that Amazon star ratings tend to increase demand in long-term, which may lead to lower prices due to more competition. As above, I added an interaction term for "reg7" to illustrate the dependence of Amazon star ratings on the number of reviews. When estimating the effect on the Amazon price, the addition of this interaction term results in a positive coefficient on the `log star rating` of 0.045 instead of a negative coefficient of -0.007. 

#< quiz "Des_vs_Pred"
question: What does this mean for the assumption of a long-term price increase? 
sc:
- The assumption is incorrect because descriptive analyses cannot account for dependencies between different effects.*
- The assumption is correct and the interaction term is inappropriate for this situation.
success: Great, your answer is correct! As a rule, descriptive analyses do not take into account the dependencies of the variables. In this case, the descriptive analysis leads to a false assumption.    
failure: Try again.
#>

**Summary**

After expanding our methodical knowledge in empirical economics in Section 3.1, we replicated the main regression table (with log sales rank as the dependent variable) from the underlying paper, and found that the coefficients differed from the original estimation results. These differences are due to the lagged sales rank, which was not replicatable. However, the regression from this problem shew similar coefficients but with higher standard errors. We found large short-term effects of New York Times reviews, while the overall effect of other professional reviews was less present. In addition, we learned how interaction terms work and how they are implemented in R. Second, we examined the effects on the Amazon price. We found overall small but positive effects from New York Times reviews and negative effects from other journals. By using an interaction term, we discarded the assumption that an increase in Amazon star ratings is associated with a decrease in Amazon prices.

In the following chapter event studies are explained and implemented in R.

## Exercise 3.3 -- Introduction and Implementation of Event Studies

In their paper, Reimers and Waldfogel used a so-called **Event Study** to illustrate and estimate the impact of events (here: publication of professional reviews) that took place. Originally, Event Studies come from the financial sector from James Dolly in 1933. He examined the price effects of stock splits, studying the occurrence of price changes at the time of the split (MacKinley, 1997, ppt. 13). MacKinley listed several methods for applying Event Studies, including *Cross-Sectional Models* that use regressions (MacKinley, 1997, ppt. 33). Reimers and Waldfogel also used such an approach to visualize short-term effects on the Amazon sales rank.

**Task:** Check the following chunk to read in the main data set.

```{r "3.3.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS") %>%
  arrange(canum, ddate)
#>
```

The first step of the authors procedure is to place all books chronologically on top of each other. For this purpose, the variables `NYT_elapse` and `OTH_elapse` are suitable. For both scenarios, the variable is filtered 20 days before and 40 days after the review is published. Before implementing this Event Study, the data structure need to get reviewed. 

**Task:** Create the data set `dataESNYT` and `dataESOTH` to count the number of books with information about 20 days before and 40 days after the review was published.

```{r "3.3.2", warning=FALSE}
#< task
dataESNYT <- data %>%
  filter(cno == 3 & NYT_elapse >= -20 & NYT_elapse <= 40) %>%
  group_by(titleno) %>%
  summarize(Sum_observations = n())

nrow(dataESNYT)
sum(dataESNYT$Sum_observations)

dataESOTH <- data %>%
  filter(cno == 3 & OTH_elapse >= -20 & OTH_elapse <= 40) %>%
  group_by(titleno) %>%
  summarize(Sum_observations = n())

nrow(dataESOTH)
sum(dataESOTH$Sum_observations)
#>

```

A total of 46641 observations are available for the New York Times and 11917 observations are available for other magazines in the U.S. during this period. This corresponds to a number of 1193 and 322 books. The next step is to create a regression as follows:    

$$ y = β_0 + β_1LaggedSalesRank + β_2Price + β_3NumberOfReviews + β_4StarRating + \varepsilon $$


**Task:** Implement the regression above in R and save this regression under `reg7`. Use Robust Standard Errors and `canum` as fixed effects.

```{r "3.3.3", warning = FALSE}
#< task
# Create `reg7` here
#>
reg7 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR | canum, vcov = "hetero", data = dataUS)

```

Now, this approach consists in aggregation the regression residuals on the filtered variable `NYT_elapse`. Before that, however, the residuals must be adjusted so that they have a value of zero for `NYT_elapse = -1`. This ensures that we can ideally estimate the effects after event start. 

**Task:** `Check` the following Code to adjust the residuals from `reg7`. 

```{r "3.3.4", warning=FALSE}

#< task
reg7$residuals <- reg7$residuals - mean(reg7$residuals[which(dataUS$NYT_elapse == -1)])
#>

```

Further, we aggregate the residuals on `NYT_elapse`. As a result, for each expression of `NYT_elapse`, an average residual is generated.

**Task:** `Check` the following chunk to aggregate the residuals on `NYT_elapse`.

```{r "3.3.5", warning=FALSE}

#< task
reg7_agg <- aggregate(reg7$residuals, by = list(dataUS$NYT_elapse), FUN = mean)
  colnames(reg7_agg) <- c("NYT_elapse", "Avg.Sales.rank")
 reg7_agg <- reg7_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg7))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg7))))
#>
#mean(reg7$residuals[which(dataUS$NYT_elapse == -1)])
```

The variables `Max95` and `Min95` are indicating the 95 percent *confidence interval*. A confidence interval indicates to x percent (here: 95 percent) how probable it is that the actual value is in this interval or graphical range. The formula is as follows: 

$$\bar{X} \pm Z \cdot \frac{s}{\sqrt{n}}$$

where s indicates the standard deviation, Z the z-value for the confidence level and n the sample size. A simplified z-table can be found [here](http://www.ltcconline.net/greenl/courses/201/estimation/smallConfLevelTable.htm).

**Task:** Create a `ggplot`- graph to visualize the average residuals on the y-axis and `NYT_elapse` on the x-axis. Add a vertical line to illustrate the day before the review was published. Information can be found [here](https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_vline).

```{r "3.3.6", warning=FALSE}

#< fill_in
ESNYT <- ggplot(reg7_agg, aes(x = __________, y = ______________)) + 
   geom_line() + 
   ggtitle("Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
   geom_line(aes(y = _____), linetype = "dashed") + 
   geom_line(aes(y = _____), linetype = "dotted") +
   # Add the vertical line here (Use: size = 0.5, color = "red")
 
   scale_y_reverse() + 
   ylim(0.3, -0.6) +
   theme_minimal()
 ESNYT 
#>
ESNYT <- ggplot(reg7_agg, aes(x = NYT_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
  geom_line(aes(y = Max95), linetype = "dashed") + 
  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.3, -0.6) +
  theme_minimal()
ESNYT 
```

As the graph shows, a published review in the New York Times is associated with a huge increase in log sales rank of about 0.40 units. After almost two weeks, the sales ranks returns to its base. The proximity of the confidence interval line to the main line indicates a high level of significance. Analogous, we implement the same event study for all other journals. 

**Task:** According to above, adjust the residuals on `OTH_elapse = 1` and call it `reg7.1$residuals`. Furthermore, create `reg7.1_agg` with `aggregate()` on the variable `OTH_elapse`. Finally, add the minimum and maximum value of the 95 percent confidence interval. 

```{r "3.3.7", warning=FALSE}

#< task
# Add your code here
#>

reg7$residuals <- reg7$residuals - mean(reg7$residuals[which(dataUS$OTH_elapse == -1)])

reg7.1_agg <- aggregate(reg7$residuals, by = list(dataUS$OTH_elapse), FUN = mean)
  colnames(reg7.1_agg) <- c("OTH_elapse", "Avg.Sales.rank")
 reg7.1_agg <- reg7.1_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg7))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg7))))
```

**Task:** Press `check` to illustrate the differences between effects of New York Times and other magazines. 

```{r "3.3.8", warning=FALSE}

#< task
ESOTH <- ggplot(reg7.1_agg, aes(x = OTH_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Event Study - Other Magazines") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
#  geom_line(aes(y = Max95), linetype = "dashed") + 
#  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.5, -0.5) +
  theme_minimal()
ESOTH 
#>

```

While `ESNYT` delivers a similar course to the New York Times Event Study from Reimers and Waldfogel, there are major differences in the graph produced here. This could be due to `L1.lrank`, which is not replicatable and differs from the variable created by Reimers and Waldfogel. Since no added value is generated from the graph, the effects (based on Event Studies) of other magazines are not discussed further in the course of this problem set. 

In chapter 2.3, it was claimed that occurring intersections between magazines could contaminate our estimations. We found that there are a total of 201 books that have been reviewed by other magazines besides the New York Times. Contamination can potentially lead to higher standard errors and inconsistencies in our estimate and result in misinterpretations. Contamination was permitted in the main regression because a number of observations at this level were not available for the event study. Now, we implement another Event Study with adjusted data.  

**Task:** Create a data set `dataAdj` from which you filter out contaminated data where more than the New York Times has reviewed a book. 

```{r "3.3.9", warning=FALSE}

#< fill_in
dataAdj <- data %>%
  filter(((data$DBG == 0 & data$DCHI == _ & data$DLAT == _ & data$DWAPO == _ & data$DWSJ == _ & data$DNYT == _) | data$DNYT == _) & cno == _)
#>

dataAdj <- data %>%
  filter(((data$DBG == 0 & data$DCHI == 0 & data$DLAT == 0 & data$DWAPO == 0 & data$DWSJ == 0 & data$DNYT == 1) | data$DNYT == 0) & cno == 3)

```

We now count about 50.000 observations less and filtered out the contaminated data. Let us repeat creating an Event Study of New York Times reviews. 

**Task:** `Check` the following code to create an additional Event Study. 

```{r "3.3.10", warning = FALSE}

#< task
# 1. Create a regression based on the adjusted data
reg8 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR | canum, vcov = "hetero", data = dataAdj)
# 2. Mean-adjustment on NYT_elapse = -1.
reg8$residuals <- reg8$residuals - mean(reg8$residuals[which(dataAdj$NYT_elapse == -1)])
# 3. Aggregation 
reg8_agg <- aggregate(reg8$residuals, by = list(dataAdj$NYT_elapse), FUN = mean)
  colnames(reg8_agg) <- c("NYT_elapse", "Avg.Sales.rank")
 reg8_agg <- reg8_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg8))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg8))))
# 4. Visualizing
ESNYTNew <- ggplot(reg8_agg, aes(x = NYT_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Adjusted Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
  geom_line(aes(y = Max95), linetype = "dashed") + 
  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.3, -0.6) +
  theme_minimal()
ESNYTNew
#>

```

As the graph shows, there are almost no deviations from previous diagram. However, Event Studies also entail some disadvantages. First, the mean-adjusted method does not work that well. This is due to the fact that if many of these events occur simultaneously, possible seasonal fluctuation or other endogenous effects cannot be ruled out (V. Henderson Jr, 1990, pp. 288) As a result, the time period must be well chosen. Furthermore, Event Studies cannot provide causal evidence. Finally, other effects also influence the estimation, especially when the regression consists of only four explanatory variables.

**Summary** 

Event Studies are useful methods to illustrate the effect of events occurring at different times. We received information about the origin of Event Studies and how they work in general. We then focused on the implementation of event studies in R using so-called cross functional models, which can estimate the effect on a chronological sequence using aggregated residuals. We replicated these Event Studies from Reimers and Waldfogel and differentiated between New York Times reviews and reviews from other journals. While New York Times reviews show large effects on log sales rank, the estimate of the effect for other journals differs from the authors' estimate, making it unusable. Further, we adjusted the data to rule out contamination potential that was already detected in chapter 2.2. Finally, some disadvantages have been listed. 

In chapter 4., we link sales ranks to sales quantities to determine and examine price elasticities.

## Exercise 4 -- Translating Sales Ranks in Quantities and Price Elasticities (1380 Words)

The research results of the sales ranks on Amazon in regressions are limited to relative statements about the sales level of a book as opposed to other books. In order to make economic statements regarding to price elasticity and welfare, absolut sales volumes are needed. Reimers and Waldfogel collected data from the Top 100 Weekly Bestsellers, where Reimers and Waldfogel claim they matched 876 books (hits in U.S. asin). The collected data was originally produced by [the Nielsen Company](https://www.nielsen.com/), which is a market research company, and therefore the data cannot be replicated. Due to the fact that the Amazon data set is based on daily observations of sales ranks while the Nielsen data lists only weekly observations, the authors created a formula based on the general assumption that sales and ranks are exponentially related (Chevalier and Goolsbee, 2003) to reconcile daily sales ranks with weekly sales data: 

$$q_{jw} = \sum_{t \in w} A_{rjt} - B + \upsilon_{jw}$$

where $t$ and $j$ stand for the day and the book index, $w$ denotes the week, $\upsilon_{jw}$ represents an error term and A and B are to be estimated by using least squares (Reimers and Waldfogel, 2021). By applying a 500-fold bootstrapping procedure (re-sampling method in which the coefficients are estimated using 500 different samples), estimators A and B were associated with average values of 10167 and 0.45. 

#< quiz "Sampling_Methods"
question: Which of these two statements could potentially have an adverse effect at an bootstrapping procedure? 
sc:
- Re-sampling methods often require intensive computer power, especially if the data set is large.*
- To rely on the solution of this method, the underlying data must be normally distributed.
success: Great, your answer is correct!   
failure: Try again.
#>

In the following, not only price elasticities are calculated, but also elasticities from professional valuations and Amazon star ratings. Calculating these elasticities, Reimers and Waldfogel used the following formula:  

$$
 \epsilon = \frac{B*Coefficient \ of \ Elasticity}{1 - Coefficient \ of \  Lagged Log Sales Rank} 
$$

To implement this formula in R, we use the coefficients estimated in `reg5`.

**Task:** `Check` this chunk to read in `reg5`. 

```{r "4.2"}
#< task
reg5 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR + lrR + dnytpost1_1 + dnytpost1_2 + dnytpost1_3 + dnytpost6_1 + dnytpost6_2 + dnytpost6_3 + dnytpost10_1 + dnytpost10_2 +  dnytpost10_3 + dothpost_1 + dothpost_2 + dothpost_3 + dothpost10_1 + dothpost10_2 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_1 + dnytpost1r_2 + dnytpost1r_3 + dnytpost6r_1 + dnytpost6r_2 + dnytpost6r_3 + dnytpost10r_1 + dnytpost10r_2 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = data)
#>
```

Now, let us start to calculate the elasticities. 

**Task:** Check the following chunk to read in the main data set and to filter for U.S. observations. 

```{r "4.1", warning=FALSE}
#< task
data <- readRDS("dataEst.RDS") %>%
  arrange(canum, ddate)

data <- dataUS %>%
  filter(cno == 3)
#>

```

**Task:** `Check` the following Code to see how the 25th, the 50th and the 75th quantile of Amazon star rating elasticities get implemented in R.

```{r, "4.2"}

#< task_notest
star_elas_25 <- dataUS$B * (reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.25, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

star_elas_50 <- dataUS$B * ( reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.5, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

star_elas_75 <- dataUS$B * ( reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.75, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

dataUS$star_elas_25 <- star_elas_25
dataUS$star_elas_50 <- star_elas_50
dataUS$star_elas_75 <- star_elas_75
#>

```

After this instruction the further elasticities are calculated by the reader. 

**Task:** Determine the elasticities for `lpamzn`, `lreview`, `dothpost_3` and `dothpost10_3`. According to the chunk above, save your results as column in `dataUS` under the names `price_elas_mean`, `star_elas_mean`, `oth_1_10` and `oth_11_20`. Press `Check` to confirm.

**Note:** The coefficient for `lrR` was added to account for interaction effects. For the following chunks `lrR` is not needed.  

```{r "4.3"}
#< task

#>

price_elas_mean <- dataUS$B * reg5$coefficients["lpamzn"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$price_elas_mean <- price_elas_mean

star_elas_mean <- dataUS$B * (reg5$coefficients["lR"] + reg5$coefficients["lrR"] * mean(dataUS$lreview)) / (1 - reg5$coefficients["L1.lrank"])
dataUS$star_elas_mean <- star_elas_mean
	
oth_1_10 <- dataUS$B *  reg5$coefficients["dothpost_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$oth_1_10 <- oth_1_10

oth_11_20 <- dataUS$B *  reg5$coefficients["dothpost10_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$oth_11_20 <- oth_11_20


```

Elasticities for the New York Times elasticity effects are calculated below. 

**Task:** Press `check` to calculate the remaining elasticities. 

```{r "4.4"}

#< task_notest
nyt_1_5_not_rec <- dataUS$B * reg5$coefficients["dnytpost1_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_1_5_not_rec <- nyt_1_5_not_rec
nyt_6_10_not_rec <- dataUS$B * reg5$coefficients["dnytpost6_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_6_10_not_rec <- nyt_6_10_not_rec
nyt_11_20_not_rec <- dataUS$B * reg5$coefficients["dnytpost10_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_11_20_not_rec <- nyt_11_20_not_rec

nyt_1_5_rec <- dataUS$B * reg5$coefficients["dnytpost1r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_1_5_rec <- nyt_1_5_rec
nyt_6_10_rec <- dataUS$B * reg5$coefficients["dnytpost6r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_6_10_rec <- nyt_6_10_rec
nyt_11_20_rec <- dataUS$B * reg5$coefficients["dnytpost10r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_11_20_rec <- nyt_11_20_rec

#>

```

Finally, we read in the remaining elasticity values. 

**Task:** Read in the following RDS files: `other_only.RDS`, `nyt_not_rec_only.RDS`, `nyt_rec_only.RDS`, `both_not_rec.RDS`, `both_rec.RDS`, `nytoverall.RDS` and `overall.RDS`. 

```{r "4.5"}

#< task

#>
other_only <- readRDS("other_only.RDS")
nyt_not_rec_only <- readRDS("nyt_not_rec_only.RDS")
nyt_rec_only <- readRDS("nyt_rec_only.RDS")
both_not_rec <- readRDS("both_not_rec.RDS")
both_rec <- readRDS("both_rec.RDS")
nytoverall <- readRDS("nytoverall.RDS")
overall <- readRDS("overall.RDS")
```

After collecting all elasticity values, we create a table to visualize them. The authors also generated within a bootstrapping procedure standard errors for the estimated elasticity values. Basically, in addition to a variance-covariance matrix, they created normally distributed random variables to calculate within a bootstrapping procedure standard errors the elasticities that are present for us. For reasons of performance, these calculations will have no relevance in the further course. 

Let us illustrate these elasticities in a table. 

**Task:** First, create a data set with `data.frame` to list all elasticity values. Then, use `kbl()` to create a table containing the elasticity values. Fill in the ___ gaps and press `check` to confirm your input. 

```{r "4.6"}

#< fill_in
summary_data <- d_______me(Effects = c(mean(______elas_mean), mean(star_elas_25), mean(star_elas_50), mean(star_elas_75), mean(star_elas_mean), mean(nyt_1_5_not_rec), mean(nyt_6_10_not_rec), mean(nyt_11_20_not_rec), mean(nyt_1_5_rec), mean(nyt_6_10_rec), mean(nyt_11_20_rec), mean(oth_1_10), mean(oth_11_20), other_only, nyt_not_rec_only, nyt_rec_only, both_not_rec, both_rec, overall, nytoverall))

row.names(__m_________) = c("Price Elasticity", "Star Elasticity 25%", "Star Elasticity 50%", "Star Elasticity 75%", "Star Elasticity Overall", "NYT 1-5 Days", "NYT 6-11 Days", "NYT 11-20 Days", "NYT 1-5 Days rec", "NYT 6-11 Days rec", "NYT 11-20 Days rec", "OTH 1-10 Days", "OTH 11-20 Days", "Only other Magazines", "Only NYT not recommended", "Only NYT recommended", "Both not recommended", "Both recommended", "overall", "NYT reviewed")

______y_da__ %>%
kbl(caption = "Elasticities") %>%
  kable_paper("striped", full_width = TRUE) %>%
  p______ws("*Percent effect of review on annual q*", 14, 20) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
#>

summary_data <- data.frame(Effects = c(mean(price_elas_mean), mean(star_elas_25), mean(star_elas_50), mean(star_elas_75), mean(star_elas_mean), mean(nyt_1_5_not_rec), mean(nyt_6_10_not_rec), mean(nyt_11_20_not_rec), mean(nyt_1_5_rec), mean(nyt_6_10_rec), mean(nyt_11_20_rec), mean(oth_1_10), mean(oth_11_20), other_only, nyt_not_rec_only, nyt_rec_only, both_not_rec, both_rec, overall))

row.names(summary_data) = c("Price Elasticity", "Star Elasticity 25%", "Star Elasticity 50%", "Star Elasticity 75%", "Star Elasticity Overall", "NYT 1-5 Days", "NYT 6-11 Days", "NYT 11-20 Days", "NYT 1-5 Days rec", "NYT 6-11 Days rec", "NYT 11-20 Days rec", "OTH 1-10 Days", "OTH 11-20 Days", "Only other Magazines", "Only NYT not recommended", "Only NYT recommended", "Both not recommended", "Both recommended", "Average")

summary_data %>%
kbl(caption = "Elasticities") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Percent effect of review on annual q", 14, 19) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

```

As expected, these values differ slightly from the elasticity values determined by Reimers and Waldfogel. In all likelihood, this is due to the regression coefficient of `L1.lrank` differing from the value calculated in the underlying paper. Nevertheless, the top half of this table provides similar values to the original table.

#< quiz "General_Elasticity_Question"
question: Which of the following answers is **not** correct? 
sc:
- This table shows elasticities of demand.
- The addition of the interaction term `lrR` can be related to the differences in the quantile effects of the star ratings.
- Book prices on Amazon are generally elastic, so price increases are accompanied by disproportionately high changes in demand.*
success: Great, your answer is correct!   
failure: Try again.
#>

The coefficient for `Price Elasticity` accounts approximately -0.17. 

#< quiz "Coeff_Elasticity_Price"
question: Which of the following answers is correct? 
sc:
- Book prices on Amazon are elastic.
- A price increase of one percent is associated with a decrease in sales of 0.17 percent.*
- A price increase of one percent is associated with a increase in sales of 0.17 percent.
success: Great, your answer is correct!   
failure: Try again.
#>

The coefficient for `NYT 1-5 days` is about 0.53, while the coefficient for `NYT 1-5 days rec` accounts approximately 0.64.  

#< quiz "Coeff_Elasticity_NYT"
question: Which of the following answers is **not** correct? 
sc:
- The coefficient for `NYT 1-5 days` indicates that the occurrence of a New York Times review is associated with an increase in sales volume of 53 percent with the first five days of publication.
- The coefficient for `NYT 1-5 days` indicates that a one percent book price increase reviewed by The New York Times is associated with a $e^{0,53} - 1 = $ 70 percent increase in sales volume within the first five days after publication.*
- New York Times recommendations have larger impacts on the sales volume than New York Times reviews without recommendation.
success: Great, your answer is correct!   
failure: Try again.
#>

We now focus on the explanatory power of this model. The calculations are based on the estimated regression coefficients for determining the relative changes in the elasticity variables and the sales ranks (as a relative index for demand). 

#< quiz "Expl_Elasticity_Power"
question: Which of the following answers is **not** correct? 
sc:
- These elasticities are based on estimated coefficients, so significance levels and other coefficients of determination should be included for interpretation purposes
- Book prices on the U.S. Market are inelastic despite the fact that the prices are not statutory fixed. 
- Since the cost of a book is only a small part of the total consumption cost, the actual price elasticity tends to be larger than 1.*
success: Great, your answer is correct!   
failure: Try again.
#>

Focusing the elasticity effects below (from the 14th row), the annual percentage effects on the estimated sales volume `q` were calculated. In this part of the table, large differences in coefficients occurred. This could be due to the fact that these coefficients were calculated within a randomly drawn sample, and this sample was not defined in terms of size, nor is it possible to replicate which observations were used for the calculations. The highest effect in this table has the value for `Both not recommended`, which means that besides the New York Times at least one other magazine has also reviewed the book. The value of 2.57 means that the occurrence of the above condition is associated with an increase in sales of 2.57 percent. In contrast to this examination, Reimers and Waldfogel estimated for the same coefficient a value of 3.833 (Reimers and Waldfogel, 2021, Table 3). The average effect (that at least one magazine reviewed the book) estimated in this problem set accounts about 0.8 indicating an annual sales increasing effect of 0.8 percent. 

**Summary** 

In order to be able to make economic statements about price elasticity and actual demand with the help of the relative investigations of Amazon Sales Ranks, absolute sales figures are required. To generate these, the authors created an exponential formula based on sales numbers from the Nielsen Company and previous examinations from Chevalier and Goolsbee. As a result, it has become possible to estimate sales figures and thus determine price elasticities accordingly. Next to price elasticities, we examined elasticity effects of professional reviews and crowd ratings and estimated an annual effect of Amazon star ratings of approximately 0.6 percent and of professional reviews of about 0.8 percent. However, these elasticities are based on estimated sales volumes, estimated relative sales ranks, and common intercepts with respect to different magazines, making it difficult to make a statement without significance parameters.  

After working through this chapter, you will have completed the content portion of this assignment and will continue with the conclusion. 

## Exercise 5 -- Conclusion (~1000 Words)

**Recapitulation**

In the recapitulation part, all tasks covered in this problem set are summarized and evaluated once again. 

The motivation chapter provided important information for understanding the following tasks. First, it classified the book as an economic good, explained the current situation in the book market, and discussed why the book market is particularly suitable for this study. Second, basics were explained about how a market with many market participants behaves and how pre-purchase information can influence it.

The second exercise began with an explanation of the data set used for the research. Important variables were explained to understand the following exercises and to provide information on how Amazon classifies and identifies its products. Descriptive research was also conducted to analyse how frequently professional reviews occur and the extent to which they overlap, and whether they occur randomly or depend on certain factors (such as the popularity of the author). In addition, Amazon prices and sales ranks were analysed at different levels of expression of professional and non-professional reviews to make initial assumptions about their impact. Finally, prices were observed within one year to determine seasonal fluctuations.        

In exercise three, empirical research was conducted to estimate the specific effects on Amazon sales and prices. First, regressions and additional instruments were introduced to provide a basic knowledge of how to understand the following studies. These instruments include control variables, fixed effects, robust standard errors, and logarithmic estimators. To convey this, samples were drawn from the main data set to explain using real data. As a result, a simple regression was compared to a regression using these instruments to illustrate how they work. In the following, the main regression table of Reimers and Waldfogel has been replicated and explained in order to compare and interpret the estimated effects. In addition, an event study was replicated and explained to graphically illustrate the impact of professional reviews. 

The fourth exercise shows how to convert relative Amazon sales ranks into quantities to determine price elasticities and provide a transition to welfare analyses. First, it explained how the authors developed a specific function based on book data to which they had access and a similar formula used by other researchers. This formula estimates conversion factors based on existing data to estimate a specific sales volume per book. As a result, an elasticity table from Reimers and Waldfogel has been replicated to compare and to interpret serveral effects.  

**Results** 

The paper, which forms the basis of the underlying problem set, examines the influence of professional reviews from magazines such as the New York Times and non-professional crowd ratings on a five-point scale based on Amazon on Amazon sales ranks. In examining the impact of professional reviews, the authors distinguished between short- and long-term effects and found that a New York Times review without a recommendation increased a book's estimated sales by 55 percent within the first five days of publication. With recommendation, the effect should be about 80 percent. The long-term effect of New York Times reviews on the sales volume should account 2.8 percent. The effect from other magazines should only account 0.12 percent within the first ten days of publication.    

+The results of this problem sets state that New York Times reviews increase sales by an average of 70 percent, while the effect of New York Times reviews with recommendation within the first five days of publication averages 89 percent. The long-term effect for New York Times reviews on average accounts **$x$** percent. Similar to the authors calculation, the estimated price elasticity accounts -0.168 on average. The differences in estimators are likely due to the calculation of the lagged sales rank `L1.lrank`, which is also included in the elasticity estimations. Likewise, the authors claim that their formulations imply a causal relationship. In this problem set has been illustrated that multiple intersections between different magazines exist, so that about 15 percent of the entire New York Times reviews are contaminated, while other magazines are between 30 percent and 67 percent contaminated by the New York Times. As for the methodology, the use of fixed effects does not exclude but only reduces endogeneity effects. To ensure this, the instrumental variable approach or the difference-in-difference approach would have been more appropriate. Finally, New York Times reviewers are completely autonomous in the selection of their books, which is why, despite my research, it cannot be assumed that the study design is completely randomized.
Still, the New York Times has large short-term effects on sales, while Reimers and Waldfogel contend that crowd ratings tend to have large impacts on the economic welfare. To find further information about these implications, please continue reading the underlying paper.  

**Related Literature**

In addition to the paper of Reimers and Waldfogel, also similar studies have been published. For instance, Chevalier and Goolsbee published their study back in 2003. In their study, they examined the price elasticity from Amazon and BN.com and found out, that the price elasticity for books sold on Amazon was about -0.6. Regarding the discussion of the extent to which authors with higher name recognition benefit from "positive" or "negative" reviews, Berger, Sorensen, and Rasmussen (2010) found that authors with higher prior awareness (more than ten published books) are expected to benefit almost twice as much from positive journal reviews. In contrast, less prior awareness authors (less than two published books) should benefit particularly well from bad reviews, while well-known authors should receive a negative impact from bad reviews on the number of books they sell. Chevalier and Mayzlin (2006) explained the phenomenon by noting that books listed on Amazon are generally better known than books that are not listed, so these listed books are more likely to suffer (due to a bad review) than unlisted books. A similar study was published by Reinstein and Snyder (2005), where the authors estimated the impact of professional reviews on movies using a difference-in-difference approach. Using two different levels of ratings (from positive ratings), the authors found that a single thumbs-up from the rater should result in an 11 percent increase in sales, while two thumbs-ups should have a 25 percent effect on sales.  
