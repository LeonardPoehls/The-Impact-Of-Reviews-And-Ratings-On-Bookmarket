
#< ignore

```{r "setup", eval=FALSE}
library(RTutor)
library(yaml)
library(xfun)
library(tidyverse)
library(kableExtra)
library(grid)
library(gridExtra)
 
 #Set Working Directory
 setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket") 
 
 # Set problem set's name
 ps.name = "The Impacts of reviews and Ratings on Book Market"; sol.file = paste0(ps.name,"_sol.Rmd")
 
 # character vector of all required packages in the problem set
 libs = c("ggplot2", "tidyverse", "kableExtra") 
 
 # Create Problem Set
 create.ps(sol.file=sol.file, user.name = NULL, ps.name=ps.name,libs=libs,stop.when.finished=FALSE, addons="quiz")
 
 #open problem set in web browser
show.ps(ps.name,launch.browser=TRUE, auto.save.code=FALSE, sample.solution=FALSE)
 
 stop.without.error()

```

#>

# The Impacts of Professional Reviews and Crowd Ratings on the Book Market

Author: Leonard PÃ¶hls 

Hello readers and welcome to my Bachelor Thesis about how professional reviews and crowd ratings impacting the book market. Do not wonder about the manner of this Thesis. The whole Program is based on the statistical programming language R and its package RTutor to generate interactive problem sets with exercises for its readers. My created Problem is based on the Paper **Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings"** by Imke Reimers and Joel Waldfogel, that has been published in 2021 in the American Economic Journal. It investigates two main aspects. First, how pre-purchase information in the form of crowd ratings from other individual purchaser and professional reviews from daily newspapers do affect sales ranks and revenues. Second, to compare the welfare effect with and without the presence of pre-purchase information. The base of those mentioned welfare effects got determined through the transformation from sales prices and sales ranks into quantity elasticities. The Paper can be opened [here](https://www.aeaweb.org/articles?id=10.1257/aer.20200153). 

In 1995, one year after Amazon opened their gates, about more than 61% of all book sales in the USA has been generated by physical bookstores and bookclubs, while just 10 percent have been made from other channels including Amazon (Curcic, 2023). 

For the following 28 years we experienced the growth of digitization and development of crowd rating infrastructure on online pages. 
Hence, users got capable to receive important non-professional pre-information from other users that potentially influences purchasing behavior and economic welfare effects. As a result, online retailers substituted the trade of physical books in wide parts. From an economic perspective, pre-purchase information likewise effects personal expectation of quality and therefore also the demand.  

Today, the distribution of market share in print books have been changed significantly, while Amazon have taken over the leadership as the biggest retailer of print books in all over the world. For instance, the Amazon share for US book market accounts more than 40% percent and around 50% for the Uk market share (McLoughlin, 2022).

To examine these effects, this problem set aims to reorganize and replicate part of the study from Reimers and Waldfogel by retyping and extending their investigations about the effects of professional reviews and crowd ratings on sales ranks and revenues. 

## Exercise Content

1. Motivation

 1.1 Book Market, Professional Reviews and Crowd Ratings
 
 1.2 Introduction to Welfare, Demand and Price Elasticity

2. Data and Descriptive Insights

 2.1 Introduction to the Data Set
 
 2.2 Analysis of the pre-purchase information
 
 2.3 Recognition of Potential Effects through Descriptive Approaches
 
3. Empirical Strategies on Sales Ranks 

 3.1 Introduction and Implementation of Event Studies
 
 3.2 Robust Standard Errors, Fixed Effects and Logarithmized Estimates
 
 3.3 Estimation of the Effects on Sales Ranks
 
4. Translating Sales Ranks into Quantities and Price Elasticity
 
5. Conclusion

6. References

Appendix - tbd.

A1. tbd.

A2. tbd.

A3. tbd.

## An Instruction how to work with Problem sets

As already mentioned above, the manner of this problem set is to create an interactive environment for its readers. Thus, sometimes appear different Types of exercises with a so called chunk (window) below. Basically, there are three different type of exercises:   
* An empty code chunk without any Information. Consequently, you have to find the solution by yourself. 
* Code chunks with gaps like ___ to replace with the correct code. 
* Those where the whole code is already given. This Code is ready to run.  
If you need some advice by solving the exercise, just press `hint` to get some help. By pressing `run` the code gets executed. The `solution` button is a short-cut to deliver the correct code immediately. To verify the task, click on `check`. If your code was not correct, you would get a corresponding report. 

Next to code chunks, you can also work on some multiple choice quizzes to test your prior knowledge or to check your own text comprehension. Guessing the right answer can also lead to a higher understanding and a maintaining attention. 

Press `Go to next exercise...` to continue and to find further instructive competitions.

<br/>


## Exercise 1 -- Motivation

In this study, the book market was chosen to determine the impacts from crowd ratings and professional reviews. 

First of all, the following chapter intends to declare the meaning of "pre-purchase information", the book market situation and why especially this market is predestined for measuring review effects. To find an answer, we focus on newspaper magazines and on the role of Amazon concerning crowd ratings and so called sales ranks. 

In the second part of this chapter, we illustrate potential welfare effects by having access to pre-purchase information with and without the restriction of fixed book prices. Additionally, we explain the transition from sales ranks and sales prices into quantity elasticities.  

After editing this chapter, you gained an insight of the initial situation on the book market and you received deeper knowledge about basic economic issues. Thus you will be well prepared to continue with chapter 2. 

### Structure

1.1 Book Market, Professional Reviews and Crowd Ratings

1.2 Introduction to Welfare, Demand and Price Elasticity


## Exercise 1.1 -- Book Market, Professional Reviews and Crowd Ratings

Generally, economists differentiate goods concerning their characteristics, their occurrences or other conditions. Focusing the degree of uncertainty, we distinguish between three different goods, **Search Goods**, **Experience Goods** and **Credence Goods**. Buyer of **Search Goods** already have an accurate perception and a high degree of certainty of what they want to buy. For instance, sugar or computer are typical search goods. **Experience Goods** are associated with a lower degree of certainty. Without any information advantages, buyers are not capable to assess the goods quality until the buyer starts comsuming it, such as visits to cinema or whine. By the consumption of **Credence Goods** the buyer never gets into the situation to evaluate the quality of the underlying good as the degree of uncertainty is the highest here. Common credence goods are services like lawyers or surgeons (Wieneke, 2019). 

#< quiz "Books_As_Goods"
question: What would you guess books belong to?
sc:
- Search Goods.
- Experience Goods.*
- Credence Goods.
success: Great, your answer is correct!
failure: Try again.

#>

The underlying paper focuses the book market to examine the impacts from crowd ratings and professional reviews on sales ranks. So why is particulary the book market that appropriated for this study? In the underlying paper, Reimers and Waldfogel enumerated three main reasons for this claim. First, books belong to experience goods. For the other two goods, pre-purchase information is less or not relevant. Second, the number of professional reviews (in high visible media) is relatively small and distributed across a few big newspapers. The third reason refers to the data set on which the entire examination is based on. The high frequently data on book demand at Amazon should contain about 45% of the US physical book market, what is approximately comparable to Mcloughlins numbers from 2022. 

As explained, professional reviews are defined as periodically appearing reviews in daily newspaper articles. The data set includes information about the appearance of professional reviews from the New York Times, the Chicago Tribune, the Boston Globe, the Wall Street Journal, the Los Angeles Times and the Washington Post. However, quantitative information as well as star ratings are not available. Additionally, The New York Times recommends nine books every week (New York Times, 2023). Professional reviews enable access to pre-purchase information. 

#< quiz "Big_Newspapers"
question: What is your suggestion, which of these newspapers has the most impact on the sales quantity?
sc:
- The New York Times.*
- The Chicago Tribune.
- The Boston Globe.
- The Wall Street Journal.
- The Los Angeles Times.
- The Washington Post.
success: Great, your answer is correct!
failure: Try again.

#>

As well as professional reviews, the Amazon data set also includes information about star ratings from buyers. Identified as buyer on Amazon, everyone is permitted to evaluate the bought product on a five-point scale. Basically, people benefit from other customers reviews. However, in contrast to professional reviews, crowd rating are tending to generate less trust. Crowd ratings are susceptible to fake content for defaming or fraudulent purposes while professional reviews were created by objective and professional reviewers. Therefore, the more ratings exist for the particular product, the more likely the calculated average star rating approximates the "actual" quality. Thus crowd ratings are representing the second pre-purchase information.                                                                 
 
Hence, Reimers and Waldfogel claim that the consumer interacts with both of these types of pre-purchase information in a different way, perhaps resulting in different or superimposed effects. One possible reason could be the difference in accessibility of these information types. While crowd ratings are visible for every Amazon user, professional reviews are accessible but not automatically visible for everyone. Furthermore, it is less likely that people who randomly find a book on Amazon will verify the existence of a professional review afterwards. Vice versa, after finding a professional reviewed book, people automatically get access to crowd ratings during the purchasing process. Incidentally, word-of-mouth can be also understood as pre-purchase information. Nevertheless, the consumer has access to at least one type of pre-purchase information. 


#< quiz "Fakereviews_Amazon"
question: How many reviews on Amazon are fake or unreliable?
sc:
- 61%.
- 23%.
- 9%.
- 42%.*
- 15%.
success: Great, your answer is correct! (Stieb, 2022)
failure: Try again.

#>


Previously, we mentioned to aim an estimation on so called sales ranks. Sales ranks are the numerical representation of how your products sells in contrast to other products in the same categorie (Wisniach, 2022). Actually, we do not have information about sold quantities so that sales ranks are substituting these values. [Amazon](https://www.amazon.com/gp/help/customer/display.html?nodeId=GGGMZK378RQPATDJ) defined their sales rank as hourly updated calculation to "reflect recent and historical sales of every item sold on Amazon". Hence, sales ranks are relative numbers to compare sales activities. To make assumptions regarding to a welfare analysis, the authors of the underlying paper collected more data from New York Times top-100 weekly bestsellers from 2018 to transform ranks into quantities. As a result, we are capable to examine price elasticities. 

**Summary**

To sum up, we obtained an explanation of goods concerning their level of certainty and assigned books to the experience goods. Furthermore, we enumerated three different types of pre-purchase information from which we use two for the estimation. We recognized that crowd ratings and professional reviews exhibit different effects and potentially superimpose each other what possibly leads to different results in estimation. Finally, we got an introduction to Amazons sales ranks and their importance for the further course. 

In the following chapter 1.2, you will pick up the economic basic knowledge you need to understand the analysis and the manner of this examination. 

## Exercise 1.2 -- Introduction to Welfare, Demand and Price Elasticity

Colloquially, the term "welfare" is associated with many contexts, such as unemployment benefit or other social assistance. The actual origin of this term lies in the economy. Mathematically, welfare is sum of producer surplus and consumer surplus. Basically, consumer surplus is the difference between the price for a good that consumers are willing to pay and the actual price of this good. Against this, Producer surplus is the difference between the price that suppliers would be willing to charge for their goods and the actual price of this good. Actually, the economic reality is much more complex. For simplifying purposes, we focus on polypol markets (markets with many suppliers) under perfect conditions, that the model requires in order to work. Without even one of these assumptions, the model is invalid.   

#< quiz "perfectMarket_Conditions"
question: Which of these conditions is **not** relevant for a perfect market? 
sc:
- Perfect information availability (Knowledge about every price for the underlying good).
- No personal preferences (Preferences, that prevent you from acting rationally).
- Homogenous goods (Equal goods).
- Fast reaction velocity (Changing market conditions are quickly recognized from every market participant).
- Every good has the same quality.*
- A large number of demanders and suppliers.
success: Great, your answer is correct! Equality of quality and homogeneity of goods do not from an equivalent. Homogeneity only includes the physical condition and the substitutability. 
failure: Try again.

#>

Under these conditions, we apply a graph to illustrate the situation between demand and supply to better understand the added value of pre-purchase information. 

**Task:** Run the following chunk to create this model. Press `check` to collect your points. 

```{r "1.2.1", warning=FALSE}
#< task

#load the package "ggplot2"
library(ggplot2)
#Create a fictive data set 
demand <- c(2, 1.5, 1, 0.5, 0)
xAxis <- c(0, 1,2,3,4)
supply <- c(0, 0.5, 1, 1.5, 2)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Price = demand, Quantity = xAxis, Supply = supply, Group = xGroup)
#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quantity, y=Price, group = Group), linetype = 1, size = 0.8) +
  geom_line(aes(x=Quantity, y=supply, group = Group), linetype = 1, size = 0.8) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 2, yend = 1), color = "red", linetype = "dashed") +
#Mark Zone A
  geom_text(aes(x=0.5, y=1.3, label = "A"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone B
  geom_text(aes(x=0.5, y=0.7, label = "B"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone C
  geom_text(aes(x=1.5, y=0.3, label = "C"), color = "black", hjust=0, size=8, alpha = 0.1) +  
#Mark intersection point
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
#Add line description
geom_text(aes(x=0.5, y=1.83, label = "Demand"), color = "Black", angle = 315, size= 4, alpha=0.1) +
geom_text(aes(x=3.35, y=1.78, label = "Supply"), color = "Black", angle = 45, size= 4, alpha=0.1) +
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(labels = c("0", "", "Q*", "", "")) +
  scale_y_continuous(labels = c("0", "", "P*", "", "")) +
  theme(
 # axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#>

```

The linear demand curve shows how the consumers act on a perfect market. Obviously, a maximum price exists at which no more is consumed. This model likewise assumes unrealistically that free goods are consumed infinitely. On the other side, the linear supply curve illustrates the suppliers point of view. The model concludes that every supplier can offer his good for the maximum price. Vice versa, the more the price falls, the fewer suppliers can still offer their good. In natural competition, suppliers are forced to align their prices. This is due to the fact that each supplier wants to maximize its turnover, so it is not worthwhile for the suppliers who can offer the lowest price to actually offer the lowest price. In long term, the actual market price will converge to P* and every supplier that can not offer for P* disappears. Finally, P* defines the equilibrium price and Q* the equilibrium quantity.  

#< quiz "Mark_Surplus"
question: Which of these marked triangles represents the consumer surplus? 
sc:
- A.*
- B.
- C.
success: Great, your answer is correct!
failure: Try again.
#>

**Price Elasticity**

As already mentioned, the previous model was a severe simplification of a complex market. For instance, the fiscus also impacts market activity by implementing price caps, by subsidizing various branches or in ensuring that no so called price cartels are created. Price cartels are an association of organizations closing price agreements for particular goods to bypass the competition. Usually, the course of the two curves is not linear and the conditions for a perfect market are not satisfied. The slope of the demand or supply curve depends on the so called **Price Elasticity**. For the demand side, **Price elasticity** indicates how demand reacts on changes in prices relatively. The formula looks as follows:  

$$
 \epsilon = \frac{\Delta Q/Q}{\Delta P/P} =  \frac{\%\,\mathrm{Change\,in\,the\,quantity\,of\,goods\,demanded}}{\%\,\mathrm{Change\,in\,price}}
$$

Hence, price elasticity delivers a value Îµ := [0, â] that can also show whether price increases would be worthwhile. An elasticity of Îµ = 1 tells us that a price increase of one percents implies a demand decrease of one percent and represents the maximum turnover point for suppliers. As a result, elasticities of Îµ < 1 are inelastic and otherwise elastic.   

#< quiz "Elastic_orInelastic"
question: You are bar owner and want to maximize your turnover. In a particular period of time, you found out that the demand on beer is linear and a price increase from 3 Units to 4 Units implies a decreasing sales quantity from 1000 quantity units to 875 quantity units. Which of the following responses is correct? 
sc:
- The price is inelastic - You should not increase the price.
- The price is elastic - You should increase the price by 47%.
- The price is inelastic - You should increase the price by 34%.
- The price is inelastic - You can easily more than double the price.*

success: Great, your answer is correct!
failure: Try again.

#>

Price elasticity can be also shown graphically. The following graph illustrates the difference between elasticities in demand. 

**Task:** Execute the chunk below to see how the demand curve slope changes with different price elasticities

```{r "1.2.2", warning=FALSE}
#< task
#Create a fictive data set 
DatasetPE <- data.frame(
  Price = 1:10,
  Demand_05 = 10 - 0.5*1:10,
  Demand_1 = 10 - 1*1:10,
  Demand_2 = 10 - 2*1:10
)
colnames(DatasetPE) <- c("Price", "Îµ > 1", "Îµ = 1", "Îµ < 1")

DatasetPE <- DatasetPE %>% 
  pivot_longer(cols = starts_with("Îµ"), names_to = "Îµ", values_to = "Quantity_Value") 

# Umwandeln der Daten in "long format"
ggplot(data = DatasetPE, aes(x = Price, y = Quantity_Value, group = Îµ)) +
  geom_line() +
  theme_bw() +
  geom_segment(aes(x = 4, y = 8, xend = 4, yend = 9), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 9, xend = 2, yend = 9), color = "red", linetype = "dashed") +
  geom_text(aes(x=2.75, y=9.25, label = "ÎQU = 2"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=8.95, label = "ÎPU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=7.85, label = "Îµ = 2"), color = "black", angle = 337.5, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 6, xend = 4, yend = 7), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 7, xend = 3, yend = 7), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=7.2, label = "ÎQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=6.9, label = "PU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=5.45, label = "Îµ = 1"), color = "black", angle = 315, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 2, xend = 4, yend = 4), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 4, xend = 3, yend = 4), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=4.2, label = "ÎQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=3.45, label = "ÎPU = 2"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.6, y=1.7, label = "Îµ = 0,5"), color = "black", angle = 292.5, hjust=0, size=4, alpha = 0.1) +
  
  labs(title = "Demand Curves and their Price Elasticities",
       x = "Quantity in QU (Quantity Units)", 
       y = "Price in PU (Price Units)") +
  xlim(0, 10) + 
  ylim(0, 10) +
  scale_x_continuous(breaks = 0:10) +
  theme(panel.grid.minor=element_blank(),plot.background=element_blank()) 
#>
```

Returning to the subject of pre-purchase information, a clear classification of these are needed. Alan T. Sorensen and Kenneth Train claimed that pre-purchase information changes the perception of the books quality and that a distinction between anticipated ex ante utility and experienced ex post utility is needed to measure the effect of these information on welfare. In simpler terms, we assume that a consumer obtains a different use depending on the availability of pre-purchase information. In sum, the consumer could face three various situations depending on the books expected quality. The consumer can expect lower quality than the actual quality $$(\bar R_{j} < R_{j})$$, the same quality $$(\bar R_{j} = R_{j})$$ or a higher expected quality than the actual quality $$(\bar R_{j} > R_{j})$$. 

```{r "1.2.3", warning=FALSE}
#Create a fictive data set 
yPredQualityPrice <- c(1.6,1.0,0.50,0.15,-0.10)
xQuan <- c(1,2,3,4,5)
yRealQualityPrice <- c(2.1,1.5,1,0.65,0.4)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Pred_Price = yPredQualityPrice, Quan = xQuan, Real_Price = yRealQualityPrice, Group = xGroup)

#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quan, y=Pred_Price, group = Group), linetype = 2, size = 0.9) +
  geom_line(aes(x=Quan, y=Real_Price, group = Group), linetype = 1, size = 0.9) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 3, y = 0, xend = 3, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 0, xend = 4, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 5, yend = 1),size=1.1, color = "red") +

#Mark Zone A
  geom_text(aes(x=1.25, y=1.15, label = "A"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1, xend = 1, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone B
  geom_text(aes(x=1.5, y=1.5, label = "B"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1.5, xend = 1, yend = 2.1), color = "black", linetype = "dashed") +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone C
  geom_text(aes(x=2.27, y=1.17, label = "C"), color = "black", hjust=0, size=5, alpha = 0.1) +
#Mark Zone D
  geom_text(aes(x=3.6, y=0.9, label = "D"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 4, y = 0.65, xend = 4, yend = 1), color = "black", linetype = "dashed") +
#Mark intersection points
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
  geom_point(aes(x = 4, y=1), color = "red", size=2.5) +
  geom_text(aes(x=1.8, y=0.93, label = "P1"), color = "black", hjust=0, size=3, alpha = 0.1) +
  geom_text(aes(x=3.8, y=0.93, label = "P2"), color = "black", hjust=0, size=3, alpha = 0.1) +

#Add line description
  geom_text(aes(x=0, y=1.55, label = "Expected Quality without \nPre-Purchase Information\n _  _  _  _  _  _  _  _  _  _  "), color = "Black", hjust=0, size= 2.2, alpha=0.1) + 
  geom_text(aes(x=0, y=2.05, label = "Actual Quality with \nPre-Purchase Information\n ___________________"), color = "Black", hjust=0, size= 2.2, alpha=0.1) +
  
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(breaks = 1:5, labels = c("", "Q1", "Q*", "Q2", "")) +
  theme(
  axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())
```

As visualized in the upper graph, the consumer would choose quantity Q1 for $$(\bar R_{j} < R_{j})$$, quantity Q* for $$(\bar R_{j} = R_{j})$$ and quantity Q2 for $$(\bar R_{j} > R_{j})$$. For choosing Q1, the consumer would expect a surplus of triangle A while getting an actual surplus of A + B (ex post recognition of actual quality). A consumer with access to pre-purchase information would always choose Q* with a surplus of A + B + C. Overestimating the books quality, the consumer would buy quantity Q2 and obtains a surplus of A + B + C - D. It is obvious, that quality expectations affect the overall utility of the consumer and thus the demand curve appears to shift without impacting the price elasticity. In fact, the curve merely adjusts to the actual demand. 

#< quiz "ValueAdded_Triangle"
question: Which triangle represents the value added of pre-purchase information?
sc:
    - A
    - B
    - C*
    - D
success: Great, your answer is correct!
failure: Try again.
#>

**Summary**

After looking at a simple market diagram, we learnt to distinguish between demand and supply, to categorize the economic meaning of welfare and the composition of consumer / supplier surpluses and the formation of equilibrium values. Focusing the demand side, we got an insight into the topic of price elasticity to better understand the following examination of price elasticities on books. Hence, after collecting the necessary knowledge, we returned to pre-purchase information and their affects on welfare. We found out that pre-purchase information impacts the expectation for quality and leads to an adjustment of the demand curve. 

In the following chapter 2, we get an introduction to the Amazon data set followed by descriptive analyses.

## Exercise 2 -- Data and Descriptive Insights

The underlying examination by Reimers and Waldfogel is based on a data set provided by Amazon. 

In the first part, we deepen our understanding and the origin of the underlying data set and define important attributes that we later use for descriptive and empirical measurements.

Second, we focus on professional reviews and crowd ratings to investigate superficial contexts. To illustrate them, we use descriptive tables. 

Finally, we create an entire overview about descriptive analyses to detect potential effects between professional and non-professional reviews on sales ranks and prices to create a transition to the following empirical part of my Thesis.

After working through this chapter, you are surefooted in dealing with the underlying data set and thus well prepared to continue with the empirical part in chapter 3. 

### Structure

2.1 Introduction to the data set

2.2 Analysis of the pre-purchase information

2.3 Recognition of Potential Effects through overall Descriptive Approaches


## Exercise 2.1 -- Introduction to the Data Set

As already mentioned, the entire quantitative calculations are based on a data set provided by Amazon. The data set includes slightly less than 8.8 million observations of non-professional crowd ratings. One observation represents review(s) at one day between 02-01-2018 and 31-12-2018 for very book in every country. Observations were made in the USA, in UK and in Canada. In addition, the data set was merged with newspaper data from every newspaper magazine listed in 1.1 with information on whether the newspaper reviewed the particular book and whether the New York Times recommended the book. In fact, only 3.22 million observations were taken into consideration due to missing values, which reduces the actual market share what we examined. Reimers and Waldfogel claim that Amazon covered about 44.5 percent of the physical book market share in 2017, which we cannot accurately confirm. After removing about 63% of the data set, 44.5 percent cited by Reimers and Waldfogel means that only a little over 16 percent of the book market still covered by the Amazon data set. To improve the problem sets performance, we have already removed observations that are not relevant to our examination. 

To better understand our data set, we categorize some important variables and take a look at an excerpt of it. 

**Task:** Use the function `readRDS` to load the data set called `dataEst.RDS`. Save this data set under the name `data`.

```{r "2.1.1"}
setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket/replication/data")
#< task
#use readRDS("file_name.RDS") to load your data
#>
data <- readRDS("dataEst.RDS")
#< hint
cat("d__a <- re_dR__('da__Es_.RD_'.")
#>
```

You successfully loaded the data set. Now, we want to see how the data set is organized and which values the single attributes can take. 

**Task:** Use the function `head` to show the first rows from `data`. In addition, use the function `colnames` to list every column name from `data`.

```{r "2.1.2"}
#< task
#use head(data set name) to show your data set
#>
head(data)
colnames(data)
#< hint
cat("h_e_(da__)")
cat("col_a__s(__ta)")
#>
```

As one can see, the main data set consists of 69 variables. Not each of these are essential for the following course and will not be considered further. For those to whom this does not apply will find an explanation below. 

**Identification Variables**

The variable `asin` represents the corporate amazon ID to differentiate between different products. `country` shows us whether the underlying review belongs to the US market (US), the Great Britain market (GB) or to the Canadian market (CA). `canum` is our main ID and combines these variables to create a powerful country-dependent identifier. A title-author identifier is stored in the variable `titleno`. Identification variables are indispensable for each data set to clearly distinguish each variable. In this data set, the combination of `ddate` (see below) and `canum` defines each unique observation. 

**Chronological variables**

`ddate` shows the main date on which the crowd ratings were created What all other chronological variables are based on. `NYT_elapse` (New York Times), `BG_elapse` (Boston Globe), `CHI_elapse` (Chicago Tribune), `LAT_elapse` (Los Angeles Times), `WAPO_elapse` (Washington Post), `WSJ_elapse` (Wall Street Journal) and `OTH_elapse` (All magazines except NYT) indicate how many days have passed since/until the publication of the individual professional review. As a reference date serves here `ddate`. `epos` (if > 0) and `eneg` (if < 0) are counting days since publication. 

**Value Variables** 

The variable `rank` stands for the Amazon sales rank, on the basis of which we will perform most of the calculations. `pamzn` shows us the price of the particular book while `R` provides the given star rating on a five-point scale. `review` delivers the number of reviews in total, so this variable delivers the same number for every specification of `canum`. Each of these attributes occurs twice and once each with an "l" in front of it. This means that the values are logarithmized. 

**Dummy Variables** 

Dummy variables are binary variables that can take only two values (here: 1 and 0). Any variable starting with `dnytpost` indicates "1" if the New York Times has published reviews within the period defined by the following numbers after `dnyt`. For instance, `dnytpost1_5` takes the value 1 if [0 <`NYT_elapse` < 6]. For `dnytpost` and `dnytpost10`, [0 <`NYT_elapse` < 10] and [11 <`NYT_elapse` < 20] holds. `dnytpostpre` takes the value 1 if [-10 < `NYT_elapse` < 20]. The same principle applies to all variables starting with `dothpost` where the variable `OTH_elapse` defines the base instead of `NYT_elapse`. 

To transform sales ranks from our main data set into quantities, the authors provided confidential data to determine quantities. This data got provided by Nielsen, a market research company, and Reimers and Waldfogel have published it only in already edited form. These mentioned confidential data sets contain information about weekly top 100 sold books between 2015 and 2018 while the accessible confidential data only delivers intermediate calculations to determine price elasticities. 

**Note:** In the embedded background of this environment in the further course again and again transformations of data are accomplished, without which many computations would not function. If significant changes of known variables or structures take place, these will be explained again in the course of the future tasks.

**Summary**

Apparently, with the underlying data set, we have not only a very large set of observations in a defined time period, but also a large amount of information in the form of variables. Especially in this case, where an already merged and edited dataset has been published, this large amount of information may also have a detrimental effect on the correct replication of this study. Nevertheless, we continue with the data set provided by Reimers and Waldfogel. In this chapter, we have gained insight into the data set by executing first code chunks in R and the key variables that will continue to guide us throughout the study. Finally, we learned how to define dummy variables and the importance of identifying variables. 

In chapter 2.2, we dare a first dive into the data set by creating descriptive statistics with focus on magazine reviews and Amazon crowd ratings. 


## Exercise 2.2 -- Analysis of the Pre-Purchase Information

In the following part, we will analyze data to gain a deeper understanding of the occurrence of professional ratings. In addition, we want to check whether the collected data from journals appropriates for following estimations. 

**Task:** Run the following chunk to visualize the percentage of books reviewed by magazines, and then `check` it.

**Note:** In sum, the underlying data set includes data about **8770 books**. 

```{r "2.2.1", warning=FALSE}

#< task

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$DOTH <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 , 1, 0)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , 1, 0)

#Calculate professional review shares 
Share_of_NYT_Ratings <- paste0(round(mean(data$DNYT, na.rm = TRUE)*100, 2), "%") 
Share_of_BG_Ratings <- paste0(round(mean(data$DBG, na.rm = TRUE)*100, 2), "%") 
Share_of_CHI_Ratings <- paste0(round(mean(data$DCHI, na.rm = TRUE)*100, 2), "%")
Share_of_LAT_Ratings <- paste0(round(mean(data$DLAT, na.rm = TRUE)*100, 2), "%") 
Share_of_WAPO_Ratings <- paste0(round(mean(data$DWAPO, na.rm = TRUE)*100, 2), "%") 
Share_of_DWSJ_Ratings <- paste0(round(mean(data$DWSJ, na.rm = TRUE)*100, 2), "%") 
Share_of_OTH_Ratings <- paste0(round(mean(data$DOTH, na.rm = TRUE)*100, 2), "%") 
Share_of_ALL_Ratings <- paste0(round(mean(data$DALL, na.rm = TRUE)*100, 2), "%")

#Create a table to illustrate the shares
DesRat <- data.frame(Share_of_NYT_Ratings = c(Share_of_NYT_Ratings, n_distinct(data$titleno[data$DNYT == 1])), Share_of_BG_Ratings = c(Share_of_BG_Ratings, n_distinct(data$titleno[data$DBG == 1])), Share_of_CHI_Ratings = c(Share_of_CHI_Ratings, n_distinct(data$titleno[data$DCHI == 1])), Share_of_LAT_Ratings = c(Share_of_LAT_Ratings, n_distinct(data$titleno[data$DLAT == 1])), Share_of_WAPO_Ratings = c(Share_of_WAPO_Ratings, n_distinct(data$titleno[data$DWAPO == 1])), Share_of_DWSJ_Ratings = c(Share_of_DWSJ_Ratings, n_distinct(data$titleno[data$DWSJ == 1])), Share_of_OTH_Ratings = c(Share_of_OTH_Ratings, n_distinct(data$titleno[data$DOTH == 1])), Share_of_ALL_Ratings = c(Share_of_ALL_Ratings, n_distinct(data$titleno[data$DALL == 1], n_distinct(data$titleno))))

#Create Row Names
rownames(DesRat) <- c("Relative share", "Absolut share") 

#Use of kbl() funktion to create a visualable table
DesRat  %>%
kbl(col.names = c("Share_of_NYT_Ratings" = "New York Times", "Share_of_BG_Ratings" = "Boston Globe", "Share_of_CHI_Ratings" = "Chicago Tribune", "Share_of_LAT_Ratings" = "Los Angeles Times", "Share_of_WAPO_Ratings" = "Washington Post", "Share_of_DWSJ_Ratings" = "Wall Street Journal", "Share_of_OTH_Ratings" = "Non New York Times", "Share_of_ALL_Ratings" = "All"), caption = "Share of Professional Reviewed Books ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
#>

```

In total, 12.66 percent of all books (1521) included in the data set have been professionally reviewed. 11 percent (1315) of these books were reviewed by the New York Times, representing the largest percentage of books reviewed by professionals. The second largest percentage provides the Chicago Tribune with only 1.36 percent (139), which is approximately ten times less than the number of reviews published by the New York Times. Overall, non-New York Times magazines account for nearly 3.1 percent of the share. The following calculations distinguish between The New York Times and non-New York Times magazines, so we should note the size of the intersection between these distinctions. 

**Task:** Run the following chunk to determine the intersections between the single magazines. 

```{r "2.2.2"}

#gt_table

#Create a table to illustrate the shares
DesIntersection <- data.frame(Intersection_NYT = c(NA, n_distinct(data$titleno[data$DNYT == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DNYT == 1 & data$DOTH== 0])),
  
                              Intersection_LAT = c(n_distinct(data$titleno[data$DLAT == 1 & data$DNYT == 1]), NA, n_distinct(data$titleno[data$DLAT == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DLAT == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              
                              Intersection_BG = c(n_distinct(data$titleno[data$DBG == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DLAT == 1]),  NA, n_distinct(data$titleno[data$DBG == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DBG == 1 & data$DWSJ == 1]),n_distinct(data$titleno[data$DBG == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DCHI == 0 & data$DLAT == 0 & data$DNYT == 0])),
                              
                              Intersection_CHI = c(n_distinct(data$titleno[data$DCHI == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DBG == 1]), NA, n_distinct(data$titleno[data$DCHI == 1 & data$DWAPO == 1]), n_distinct(data$titleno[data$DCHI == 1 & data$DWSJ == 1]),n_distinct(data$titleno[data$DCHI == 1 & data$DWSJ == 0 & data$DWAPO == 0 & data$DLAT== 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              Intersection_WAPO = c(n_distinct(data$titleno[data$DWAPO == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DCHI == 1]), NA, n_distinct(data$titleno[data$DWAPO == 1 & data$DWSJ == 1]), n_distinct(data$titleno[data$DWAPO == 1 & data$DWSJ == 0 & data$DLAT == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])),
                              
                              Intersection_DWSJ = c(n_distinct(data$titleno[data$DWSJ == 1 & data$DNYT == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DLAT == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DBG == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DCHI == 1]), n_distinct(data$titleno[data$DWSJ == 1 & data$DWAPO == 1]), NA, n_distinct(data$titleno[data$DWSJ == 1 & data$DWAPO == 0 & data$DLAT == 0 & data$DCHI == 0 & data$DBG == 0 & data$DNYT == 0])))
                    
#DesIntersection <- rbind(DesIntersection, colSums(DesIntersection, na.rm = TRUE))

row.names(DesIntersection) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections")

DesIntersection  %>%
kbl(col.names = c("Intersection_NYT" = "â© New York Times", "Intersection_LAT" = "â© Los Angeles Times", "Intersection_BG" = "â© Boston Globe",  "â© Intersection_CHI" = "â© Chicago Tribune", "â© Intersection_WAPO" = "â© Washington Post", "Intersection_DWSJ" = "â© Wall Street Journal"), caption = "Intersections between New York Times and other magazines ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

```

Looking at the New York Times reviews, we noticed that a number of 201 books were reviewed by at least one other magazine. Conversely, a larger proportion of the reviews from other magazines got were also rated by the New York Times. To determine the exact proportion, we calculated that at least 30 percent (Wall Street Journal) and at most 67 percent (Boston Globe) of non-New York Times magazines were also reviewed by the New York Times. As a result, estimates in this regard may be biased by mutual overlays. The row or column sum does not reflect the total number of reviews in the journals, as we only consider individual overlays. In reality, there are also multiple overlays where three or four journals review one book. 

The advantage for crowd ratings is that we also have the amount of ratings available. In fact, the selection of books to be evaluated by professional reviewers is not random (Berger, Sorensen, Rasmussen, 2010). We want to review whether book ratings from professional magazines are randomly distributed on their five-points scale on Amazon. If this is not the case, we cannot reject the assumption that journals only review "better" rated books on Amazon and vice versa. The left Graphs in the following illustrates that the distribution of professional reviews among crowd ratings is evenly distributed. 

On the other hand, whether authors are popular also depends on the number of books they have published so far. If the authors name is more popular through previous published books, professional reviewers could potentially tend to prefer those authors. The chart on the right visualizes the proportion of professional reviews among the number of the books previously published by the authors. 


#< quiz "Distribution_Guess"
question: Focusing the published books, What is your suggestion? 
sc:
- The more books an author publishes, the more likely these books will be professionally reviewed. 
- The more books an author publishes, the less likely these books will be professionally reviewed.
- The overall distribution is even.*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Replace every ____ gap with ....... and check this chunk to create the data set the following graphs are based on. 
```{r}

#Create Dataset "DesCrowd" to show the distribution among crowd ratings
DesCrowd <- data %>%
  group_by(R) %>%
  summarise(Number_Ratings = n(),
            Number_ = round(sum(DNYT)/ Number_Ratings, 2),
            Number_1 = round(sum(DOTH)/ Number_Ratings, 2))

y1 <- DesCrowd[,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2 <- DesCrowd[,c(1, 4)] %>%
  mutate(Group = "Others")     

colnames(y1) <- c("R", "Number", "Group")
colnames(y2) <- c("R", "Number", "Group")

DesCrowd <- rbind(y1, y2)

#Create Dataset "DesAuth" to show the distribution among authors published books 

DesAuth <- data %>%
  group_by(numbooks) %>%
  summarise(Number_Ratings = n(),
            Number_ = round(sum(DNYT)/ Number_Ratings, 2),
            Number_1 = round(sum(DOTH)/ Number_Ratings, 2))


y1_1 <- DesAuth[,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2_1 <- DesAuth[,c(1, 4)] %>%
  mutate(Group = "Others")     

colnames(y1_1) <- c("numbooks", "Number", "Group")
colnames(y2_1) <- c("numbooks", "Number", "Group")

DesAuth <- rbind(y1_1, y2_1)

```

After we have created the dataset, we can proceed with the creation of the chart.

**Task:** Execute the following chunk to create two graphs representing the distribution of professional reviews among two variables. 

```{r}

#Create the related Graphs

#Create the graph based on crowd ratings

f1 <- ggplot(aes(x = R, fill = Group), data = DesCrowd) +
  geom_bar(aes(y = Number), width = 0.5, stat = "identity", color = "black", position = position_dodge()) +
  geom_vline(xintercept = quantile(data$R, probs = 0.25, na.rm = TRUE)) +
  geom_text(aes(x=4.23, y=0.5, label = "25%"),angle = 270, color = "black", hjust=0, size=6, alpha = 0.1) +
  theme_bw() +
  labs(title = "Crowd Ratings",
       x = "Amazon Star Rating", 
       y = "Relative proportion of professional reviews") +
  scale_y_continuous(breaks = seq(0,1,0.1), labels=scales::percent) +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = c(0.85, 0.75),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#Create the graph based on authors number of books 

f2 <- ggplot(aes(x = numbooks, fill = Group), data = DesAuth) +
  geom_bar(aes(y = Number), width = 0.5, stat = "identity", color = "black", position = position_dodge()) +
  geom_vline(xintercept = quantile(data$numbooks, probs = 0.25, na.rm = TRUE)) +
  geom_text(aes(x=4.23, y=0.35, label = "25%"),angle = 270, color = "black", hjust=0, size=6, alpha = 0.1) +
  theme_bw() +
  labs(title = " Published Books",
       x = "Number of Published Books ", 
       y = "Relative proportion of Professional Reviews") +
  scale_y_continuous(breaks = seq(0,1,0.1), labels=scales::percent) +
  scale_x_continuous(breaks = seq(0, 75, 5)) +
  xlim(0, 75) +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = c(0.85, 0.75),
  panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(f1, f2, ncol = 2)
```

If we focus on the distribution of Amazon star ratings, we recognize a slight leftward skew in the weaker rated books on Amazon. However, this high proportion is to the left of the 25 percent quantile line which states that 75 percent of ratings on Amazon are rated 4.1 stars or even better. In the 75 percent zone, the distribution is rather even while outliers in the 25 percent zone are not unusual. This could be because books that have already been rated poorly on Amazon tend to be rated less often as a result. Hence, we can assume that Amazon star ratings do not visibly influence the book selection of professional reviewers and vice versa. Actually, a remaining risk always exists. 

The on right chart, we notice a quite even distribution among the number of published books between 0 and 21 percent. The 25 percent quantile line is located at two published books which states that a huge amount of 25 percent of all books reviewed on Amazon are written by "inexperienced" authors who published two books or less. The distribution remains even until 65 published books. After that, almost no more books were reviewed. Because of that, we limited the x-axis. Similar to star ratings, we can assume that the book selection of professional reviewers is less dependent on the number of books published so far. 

**Summary**

In summary, we have gained deeper insights into the relative and absolut numbers of professional reviews. It was illustrated to us that a relatively high proportion, between 30 and 67 percent, of all non-New York Times magazines were also reviewed by the New York Times, which may bias future estimates. To disprove the assumption that the selection of professionally reviewed books is not random, we use two different variables (Amazon Star Rating and Number of previous published books) to check whether the relative distribution on said variables disproves this assumption. In both cases we found out, that the distributions regarding to these variables are more or less even which is why we may assume a random choice of books for the time being. After all, the professional reviewers at the New York Times have discretion over about which books they actually review. Accordingly, we can never assume a 100 percent random distribution.

## Exercise 2.3 -- Recognition of Potential Effects through Descriptive Approaches

Before we start identifying potential effect, we need a good overview of the relevant descriptive values. Generally, descriptive values are based on simple statistics to describe and to visualize contexts, that have already happened. These values are easily understood and can provide sound summaries of high data volumes within seconds. The most powerful function of descriptive analyses is the ability to identify relationships in order to make predictions using more complex statistics. This procedure is exactly the same as what we have already done in chapter 2.2. 

Starting with a good overview, we want to create a table including simple descriptive values distinguished by `country`. 

**Task:** Try to fill any ____ gap with ....... and check this chunk to create an overview of the main descriptive values.

```{r}
# Descriptive Values differentiated by country
dataDesKript <- data %>%
   group_by(country) %>%
   summarize(Price = round(mean(pamzn),2), 
            Star_Rating = round(mean(R),2), 
            Sales_Rank = round(mean(rank),2), 
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quantile(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quantile(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quantile(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quantile(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = NROW(country), 
            Editions = n_distinct(asin))

# Descriptive Values for all countries
dataDesKript2 <- data %>%
  summarize(country = "All",
            Price = round(mean(pamzn),2), 
            Star_Rating = round(mean(R),2), 
            Sales_Rank = round(mean(rank),2),
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quantile(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quantile(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quantile(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quantile(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = length(asin), 
            Editions = n_distinct(asin))

# Merge Dataframes
DataDes <- rbind(dataDesKript, dataDesKript2)

# Transform Dataframe into a clearer schema
DataDesTest <- t(DataDes)
colnames(DataDesTest) <- rownames(DataDes)
DataDescriptive <- as.data.frame(DataDesTest)
colnames(DataDescriptive) <- unlist(DataDescriptive[1,])
DataDescriptive <- DataDescriptive[-1,]


# Change Row Names 
row.names(DataDescriptive) = c("Price", "Star rating", "Sales rank", "Number of ratings", "10th", "25th", "50th", "75th", "90th", "Titles", "Observations", "Editions")


# Kable-Function to create an attractive overview

DataDescriptive %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)

```


The first four rows return the mean values of the book price, the Amazon Star Rating, the sales rank and the number of ratings per book. The book markets in all three considered countries do not have fixed book prices. Fixed book prices (FBP) means that the publisher has the exclusive right to set the price of his book. The retailer is not permitted to discount more than five percent from this set price (Nakayama, 2015).

#< quiz "Fixed_Prices"
question: Comparing UK (without FBP) and Germany (with FBP), which country accounted a higher price increase between 1996 (end of FBP in UK) and 2018? 
sc:
- United Kingdom (UK).* 
- Germany.
success: Great, your answer is correct! The UK accounted a price increase of 80 percent after this period, while Germany accounted an increase of 29 percent (Fuchs, Sprang, Beurich, GÃ¶tz, 2019)
failure: Try again.
#>

Unfortunately, our data set only includes countries data from countries without FBP. Comparing the prices of the three available countries, we recognize a large price difference between Canada (21.07) and Great Britain (13.12) while the US account an average price of 15.86. Possible reasons for those expensive book prices could be that Canada imports many of these books where fees and other costs are incurred (Kwan, 2013), transportation costs over large land masses and a loss of economies of scale due to a smaller book market. The star ratings and their percentiles are quite even while large differences occur in the sales ranks and the number of ratings. Considering the fact that sales ranks are generated on their individual market place and less transparent, we cannot list any specific reasons for this. The high differences in the number of ratings may be due to the level of awareness of Amazon in the individual countries.    



**Task:** Read the RDS-file `DataDescriptiveJournals` which represents exactly the same facts as above, except for the fact that `DataDescriptiveJournals` is filtered on data where information on the occurrence of journals is available. Save this data set under the file name `DataDescriptiveJournals`. In Addition, create a table with `kbl()` according to the table above.

```{r}
#read out 
DataDescriptiveJournals <- readRDS("DataDescriptiveJournals.RDS")
# Kable-Function to create an attractive overview

DataDescriptiveJournals %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)

```


