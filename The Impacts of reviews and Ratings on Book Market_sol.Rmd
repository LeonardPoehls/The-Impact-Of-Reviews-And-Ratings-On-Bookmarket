
#< ignore

```{r "setup", eval=FALSE}
library(RTutor)
library(modelsummary)
library(yaml)
library(xfun)
library(tidyverse)
library(kableExtra)
library(grid)
library(gridExtra)
 
 #Set Working Directory
 setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket") 
 
 # Set problem set's name
 ps.name = "The Impacts of reviews and Ratings on Book Market"; sol.file = paste0(ps.name,"_sol.Rmd")
 
 # character vector of all required packages in the problem set
 libs = c("ggplot2", "tidyverse", "kableExtra", "modelsummary") 
 
 # Create Problem Set
 create.ps(sol.file=sol.file, user.name = NULL, ps.name=ps.name,libs=libs,stop.when.finished=FALSE, addons = "quiz")
 
 #open problem set in web browser
show.ps(ps.name,launch.browser=TRUE, auto.save.code=FALSE, sample.solution=FALSE, load.sav=FALSE,  is.solved=FALSE)
 
 stop.without.error()

```

#>

# The Impacts of Professional Reviews and Crowd Ratings on the Book Market

Author: Leonard Pöhls 

Hello readers and welcome to my Bachelor Thesis about how professional reviews and crowd ratings impacting the book market. Do not wonder about the manner of this Thesis. The whole Program is based on the statistical programming language R and its package RTutor to generate interactive problem sets with exercises for its readers. My created Problem is based on the Paper **Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings"** by Imke Reimers and Joel Waldfogel, that has been published in 2021 in the American Economic Journal. It investigates two main aspects. First, how pre-purchase information in the form of crowd ratings from other individual purchaser and professional reviews from daily newspapers do affect sales ranks and revenues. Second, to compare the welfare effect with and without the presence of pre-purchase information. The base of those mentioned welfare effects got determined through the transformation from sales prices and sales ranks into quantity elasticities. The Paper can be opened [here](https://www.aeaweb.org/articles?id=10.1257/aer.20200153). 

In 1995, one year after Amazon opened their gates, about more than 61% of all book sales in the USA has been generated by physical bookstores and bookclubs, while just 10 percent have been made from other channels including Amazon (Curcic, 2023). 

For the following 28 years we experienced the growth of digitization and development of crowd rating infrastructure on online pages. 
Hence, users got capable to receive important non-professional pre-information from other users that potentially influences purchasing behavior and economic welfare effects. As a result, online retailers substituted the trade of physical books in wide parts. From an economic perspective, pre-purchase information likewise effects personal expectation of quality and therefore also the demand.  

Today, the distribution of market share in print books have been changed significantly, while Amazon have taken over the leadership as the biggest retailer of print books in all over the world. For instance, the Amazon share for US book market accounts more than 40% percent and around 50% for the Uk market share (McLoughlin, 2022).

To examine these effects, this problem set aims to reorganize and replicate part of the study from Reimers and Waldfogel by retyping and extending their investigations about the effects of professional reviews and crowd ratings on sales ranks and revenues. 

## Exercise Content

1. Motivation

 1.1 Book Market, Professional Reviews and Crowd Ratings
 
 1.2 Introduction to Welfare, Demand and Price Elasticity

2. Data and Descriptive Insights

 2.1 Introduction to the Data Set
 
 2.2 Analysis of the pre-purchase information
 
 2.3 Recognition of Potential Effects through Descriptive Approaches
 
3. Empirical Strategies on Sales Ranks 

 3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
 3.2 Estimation of the Effects on Sales Ranks and Prices
 
 3.3 Introduction and Implementation of Event Studies
 
4. Translating Sales Ranks into Quantities and Price Elasticity
 
5. Conclusion

6. References

Appendix - tbd.

A1. tbd.

A2. tbd.

A3. tbd.

```{r}
#< task_notest
getwd()
#>

```


## An Instruction how to work with Problem sets

As already mentioned above, the manner of this problem set is to create an interactive environment for its readers. Thus, sometimes appear different Types of exercises with a so called chunk (window) below. Basically, there are three different type of exercises:   
* An empty code chunk without any Information. Consequently, you have to find the solution by yourself. 
* Code chunks with gaps like ___ to replace with the correct code. 
* Those where the whole code is already given. This Code is ready to run.  
If you need some advice by solving the exercise, just press `hint` to get some help. By pressing `run` the code gets executed. The `solution` button is a short-cut to deliver the correct code immediately. To verify the task, click on `check`. If your code was not correct, you would get a corresponding report. 

Next to code chunks, you can also work on some multiple choice quizzes to test your prior knowledge or to check your own text comprehension. Guessing the right answer can also lead to a higher understanding and a maintaining attention. 

Press `Go to next exercise...` to continue and to find further instructive competitions.

<br/>


## Exercise 1 -- Motivation (3018 Words)

In this study, the book market was chosen to determine the impacts from crowd ratings and professional reviews. 

First of all, the following chapter intends to declare the meaning of "pre-purchase information", the book market situation and why especially this market is predestined for measuring review effects. To find an answer, we focus on newspaper magazines and on the role of Amazon concerning crowd ratings and so called sales ranks. 

In the second part of this chapter, we illustrate potential welfare effects by having access to pre-purchase information with and without the restriction of fixed book prices. Additionally, we explain the transition from sales ranks and sales prices into quantity elasticities.  

After editing this chapter, you gained an insight of the initial situation on the book market and you received deeper knowledge about basic economic issues. Thus you will be well prepared to continue with chapter 2. 

### Structure 

1.1 Book Market, Professional Reviews and Crowd Ratings

1.2 Introduction to Welfare, Demand and Price Elasticity


## Exercise 1.1 -- Book Market, Professional Reviews and Crowd Ratings

Generally, economists differentiate goods concerning their characteristics, their occurrences or other conditions. Focusing the degree of uncertainty, we distinguish between three different goods, **Search Goods**, **Experience Goods** and **Credence Goods**. Buyer of **Search Goods** already have an accurate perception and a high degree of certainty of what they want to buy. For instance, sugar or computer are typical search goods. **Experience Goods** are associated with a lower degree of certainty. Without any information advantages, buyers are not capable to assess the goods quality until the buyer starts consuming it, such as visits to cinema or whine. By the consumption of **Credence Goods** the buyer never gets into the situation to evaluate the quality of the underlying good as the degree of uncertainty is the highest here. Common credence goods are services like lawyers or surgeons (Wieneke, 2019). 

#< quiz "Books_As_Goods"
question: What would you guess books belong to?
sc:
- Search Goods.
- Experience Goods.*
- Credence Goods.
success: Great, your answer is correct!
failure: Try again.

#>

The underlying paper focuses the book market to examine the impacts from crowd ratings and professional reviews on sales ranks. So why is the book market particularly suitable for this study? In the underlying paper, Reimers and Waldfogel enumerated three main reasons for this claim. First, books belong to experience goods. For the other two goods, pre-purchase information is less or not relevant. Second, the number of professional reviews (in high visible media) is relatively small and distributed across a few big newspapers. The third reason refers to the data set on which the entire examination is based on. The high frequently data on book demand at Amazon should contain about 45% of the US physical book market, what is approximately comparable to Mcloughlins numbers from 2022. 

As explained, professional reviews are defined as periodically appearing reviews in daily newspaper articles. The data set includes information about the appearance of professional reviews from the New York Times, the Chicago Tribune, the Boston Globe, the Wall Street Journal, the Los Angeles Times and the Washington Post. However, quantitative information as well as star ratings are not available. Additionally, The New York Times recommends nine books every week (New York Times, 2023). Professional reviews enable access to pre-purchase information. 

#< quiz "Big_Newspapers"
question: What is your suggestion, which of these newspapers has the most impact on the sales quantity?
sc:
- The New York Times.*
- The Chicago Tribune.
- The Boston Globe.
- The Wall Street Journal.
- The Los Angeles Times.
- The Washington Post.
success: Great, your answer is correct!
failure: Try again.

#>

As well as professional reviews, the Amazon data set also includes information about star ratings from buyers. Identified as buyer on Amazon, everyone is permitted to evaluate the bought product on a five-point scale. Basically, people benefit from other customers reviews. However, in contrast to professional reviews, crowd rating are tending to generate less trust. Crowd ratings are susceptible to fake content for defaming or fraudulent purposes while professional reviews were created by objective and professional reviewers. Therefore, the more ratings exist for the particular product, the more likely the calculated average star rating approximates the "actual" quality. Thus crowd ratings are representing the second pre-purchase information.                                                                 
 
Hence, Reimers and Waldfogel claim that the consumer interacts with both of these types of pre-purchase information in a different way, perhaps resulting in different or superimposed effects. One possible reason could be the difference in accessibility of these information types. While crowd ratings are visible for every Amazon user, professional reviews are accessible but not automatically visible for everyone. Furthermore, it is less likely that people who randomly find a book on Amazon will verify the existence of a professional review afterwards. Vice versa, after finding a professional reviewed book, people automatically get access to crowd ratings during the purchasing process. Incidentally, word-of-mouth can be also understood as pre-purchase information. Nevertheless, the consumer has access to at least one type of pre-purchase information. 


#< quiz "Fakereviews_Amazon"
question: How many reviews on Amazon are fake or unreliable?
sc:
- 61%.
- 23%.
- 9%.
- 42%.*
- 15%.
success: Great, your answer is correct! (Stieb, 2022)
failure: Try again.

#>


Previously, we mentioned to aim an estimation on so called sales ranks. Sales ranks are the numerical representation of how your products sells in contrast to other products in the same categorie (Wisniach, 2022). Actually, we do not have information about sold quantities so that sales ranks are substituting these values. [Amazon](https://www.amazon.com/gp/help/customer/display.html?nodeId=GGGMZK378RQPATDJ) defined their sales rank as hourly updated calculation to "reflect recent and historical sales of every item sold on Amazon". Hence, sales ranks are relative numbers to compare sales activities. To make assumptions regarding to a welfare analysis, the authors of the underlying paper collected more data from New York Times top-100 weekly bestsellers from 2018 to transform ranks into quantities. As a result, we are capable to examine price elasticities. 

**Summary**

To sum up, we obtained an explanation of goods concerning their level of certainty and assigned books to the experience goods. Furthermore, we enumerated three different types of pre-purchase information from which we use two for the estimation. We recognized that crowd ratings and professional reviews exhibit different effects and potentially superimpose each other what possibly leads to different results in estimation. Finally, we got an introduction to Amazons sales ranks and their importance for the further course. 

In the following chapter 1.2, you will pick up the economic basic knowledge you need to understand the analysis and the manner of this examination. 

## Exercise 1.2 -- Introduction to Welfare, Demand and Price Elasticity

Colloquially, the term "welfare" is associated with many contexts, such as unemployment benefit or other social assistance. The actual origin of this term lies in the economy. Mathematically, welfare is sum of producer surplus and consumer surplus. Basically, consumer surplus is the difference between the price for a good that consumers are willing to pay and the actual price of this good. Against this, Producer surplus is the difference between the price that suppliers would be willing to charge for their goods and the actual price of this good. Actually, the economic reality is much more complex. For simplifying purposes, we focus on polypol markets (markets with many suppliers) under perfect conditions, that the model requires in order to work. Without even one of these assumptions, the model is invalid.   

#< quiz "perfectMarket_Conditions"
question: Which of these conditions is **not** relevant for a perfect market? 
sc:
- Perfect information availability (Knowledge about every price for the underlying good).
- No personal preferences (Preferences, that prevent you from acting rationally).
- Homogenous goods (Equal goods).
- Fast reaction velocity (Changing market conditions are quickly recognized from every market participant).
- Every good has the same quality.*
- A large number of demanders and suppliers.
success: Great, your answer is correct! Equality of quality and homogeneity of goods do not from an equivalent. Homogeneity only includes the physical condition and the substitutability. 
failure: Try again.

#>

Under these conditions, we apply a graph to illustrate the situation between demand and supply to better understand the added value of pre-purchase information. 

**Task:** Run the following chunk to create this model. Press `check` to collect your points. 

```{r "1.2.1"}
#< task_notest

#load the package "ggplot2"
library(ggplot2)
#Create a fictive data set 
demand <- c(2, 1.5, 1, 0.5, 0)
xAxis <- c(0, 1,2,3,4)
supply <- c(0, 0.5, 1, 1.5, 2)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Price = demand, Quantity = xAxis, Supply = supply, Group = xGroup)
#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quantity, y=Price, group = Group), linetype = 1, size = 0.8) +
  geom_line(aes(x=Quantity, y=supply, group = Group), linetype = 1, size = 0.8) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 2, yend = 1), color = "red", linetype = "dashed") +
#Mark Zone A
  geom_text(aes(x=0.5, y=1.3, label = "A"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone B
  geom_text(aes(x=0.5, y=0.7, label = "B"), color = "black", hjust=0, size=8, alpha = 0.1) +
#Mark Zone C
  geom_text(aes(x=1.5, y=0.3, label = "C"), color = "black", hjust=0, size=8, alpha = 0.1) +  
#Mark intersection point
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
#Add line description
geom_text(aes(x=0.5, y=1.83, label = "Demand"), color = "Black", angle = 315, size= 4, alpha=0.1) +
geom_text(aes(x=3.35, y=1.78, label = "Supply"), color = "Black", angle = 45, size= 4, alpha=0.1) +
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(labels = c("0", "", "Q*", "", "")) +
  scale_y_continuous(labels = c("0", "", "P*", "", "")) +
  theme(
 # axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#>

```

The linear demand curve shows how the consumers act on a perfect market. Obviously, a maximum price exists at which no more is consumed. This model likewise assumes unrealistically that free goods are consumed infinitely. On the other side, the linear supply curve illustrates the suppliers point of view. The model concludes that every supplier can offer his good for the maximum price. Vice versa, the more the price falls, the fewer suppliers can still offer their good. In natural competition, suppliers are forced to align their prices. This is due to the fact that each supplier wants to maximize its turnover, so it is not worthwhile for the suppliers who can offer the lowest price to actually offer the lowest price. In long term, the actual market price will converge to P* and every supplier that can not offer for P* disappears. Finally, P* defines the equilibrium price and Q* the equilibrium quantity.  

#< quiz "Mark_Surplus"
question: Which of these marked triangles represents the consumer surplus? 
sc:
- A.*
- B.
- C.
success: Great, your answer is correct!
failure: Try again.
#>

**Price Elasticity**

As already mentioned, the previous model was a severe simplification of a complex market. For instance, the fiscus also impacts market activity by implementing price caps, by subsidizing various branches or in ensuring that no so called price cartels are created. Price cartels are an association of organizations closing price agreements for particular goods to bypass the competition. Usually, the course of the two curves is not linear and the conditions for a perfect market are not satisfied. The slope of the demand or supply curve depends on the so called **Price Elasticity**. For the demand side, **Price elasticity** indicates how demand reacts on changes in prices relatively. The formula looks as follows:  

$$
 \epsilon = \frac{\Delta Q/Q}{\Delta P/P} =  \frac{\%\,\mathrm{Change\,in\,the\,quantity\,of\,goods\,demanded}}{\%\,\mathrm{Change\,in\,price}}
$$

Hence, price elasticity delivers a value ε := [0, ∞] that can also show whether price increases would be worthwhile. An elasticity of ε = 1 tells us that a price increase of one percents implies a demand decrease of one percent and represents the maximum turnover point for suppliers. As a result, elasticities of ε < 1 are inelastic and otherwise elastic.   

#< quiz "Elastic_orInelastic"
question: You are bar owner and want to maximize your turnover. In a particular period of time, you found out that the demand on beer is linear and a price increase from 3 Units to 4 Units implies a decreasing sales quantity from 1000 quantity units to 875 quantity units. Which of the following responses is correct? 
sc:
- The price is inelastic - You should not increase the price.
- The price is elastic - You should increase the price by 47%.
- The price is inelastic - You should increase the price by 34%.
- The price is inelastic - You can easily more than double the price.*

success: Great, your answer is correct!
failure: Try again.

#>

Price elasticity can be also shown graphically. The following graph illustrates the difference between elasticities in demand. 

**Task:** Execute the chunk below to see how the demand curve slope changes with different price elasticities

```{r "1.2.2", warning=FALSE}
#< task
#Create a fictive data set 
DatasetPE <- data.frame(
  Price = 1:10,
  Demand_05 = 10 - 0.5*1:10,
  Demand_1 = 10 - 1*1:10,
  Demand_2 = 10 - 2*1:10
)
colnames(DatasetPE) <- c("Price", "ε > 1", "ε = 1", "ε < 1")

DatasetPEN <- DatasetPE %>% 
  pivot_longer(cols = starts_with("ε"), names_to = "ε", values_to = "Quantity_Value") 

# Umwandeln der Daten in "long format"
ggplot(data = DatasetPEN, aes(x = Price, y = Quantity_Value, group = ε)) +
  geom_line() +
  theme_bw() +
  geom_segment(aes(x = 4, y = 8, xend = 4, yend = 9), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 9, xend = 2, yend = 9), color = "red", linetype = "dashed") +
  geom_text(aes(x=2.75, y=9.25, label = "ΔQU = 2"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=8.95, label = "ΔPU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=7.85, label = "ε = 2"), color = "black", angle = 337.5, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 6, xend = 4, yend = 7), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 7, xend = 3, yend = 7), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=7.2, label = "ΔQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=6.9, label = "PU = 1"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=5, y=5.45, label = "ε = 1"), color = "black", angle = 315, hjust=0, size=4, alpha = 0.1) +
  
  geom_segment(aes(x = 4, y = 2, xend = 4, yend = 4), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 4, xend = 3, yend = 4), color = "red", linetype = "dashed") +
  geom_text(aes(x=3.15, y=4.2, label = "ΔQU = 1"), color = "red", hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.25, y=3.45, label = "ΔPU = 2"), color = "red", angle = 270, hjust=0, size=2.5, alpha = 0.1) +
  geom_text(aes(x=4.6, y=1.7, label = "ε = 0,5"), color = "black", angle = 292.5, hjust=0, size=4, alpha = 0.1) +
  
  labs(title = "Demand Curves and their Price Elasticities",
       x = "Quantity in QU (Quantity Units)", 
       y = "Price in PU (Price Units)") +
  xlim(0, 10) + 
  ylim(0, 10) +
  scale_x_continuous(breaks = 0:10) +
  theme(panel.grid.minor=element_blank(),plot.background=element_blank()) 
#>
```

Returning to the subject of pre-purchase information, a clear classification of these are needed. Alan T. Sorensen and Kenneth Train claimed that pre-purchase information changes the perception of the books quality and that a distinction between anticipated ex ante utility and experienced ex post utility is needed to measure the effect of these information on welfare. In simpler terms, we assume that a consumer obtains a different use depending on the availability of pre-purchase information. In sum, the consumer could face three various situations depending on the books expected quality. The consumer can expect lower quality than the actual quality ($\bar R_{j}$ < $R_{j}$), the same quality ($\bar R_{j}$ = $R_{j}$) or a higher expected quality than the actual quality ($\bar R_{j}$ > $R_{j}$). 

```{r "1.2.3", warning=FALSE}
#< task

#Create a fictive data set 
yPredQualityPrice <- c(1.6,1.0,0.50,0.15,-0.10)
xQuan <- c(1,2,3,4,5)
yRealQualityPrice <- c(2.1,1.5,1,0.65,0.4)
xGroup <- c(1, 1, 1, 1, 1)
DatasetTest <- data.frame(Pred_Price = yPredQualityPrice, Quan = xQuan, Real_Price = yRealQualityPrice, Group = xGroup)

#Create a plot
ggplot(data = DatasetTest) +
  geom_line(aes(x=Quan, y=Pred_Price, group = Group), linetype = 2, size = 0.9) +
  geom_line(aes(x=Quan, y=Real_Price, group = Group), linetype = 1, size = 0.9) +
  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 3, y = 0, xend = 3, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 4, y = 0, xend = 4, yend = 1), color = "red", linetype = "dashed") +
  geom_segment(aes(x = 0, y = 1, xend = 5, yend = 1),size=1.1, color = "red") +

#Mark Zone A
  geom_text(aes(x=1.25, y=1.15, label = "A"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1, xend = 1, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone B
  geom_text(aes(x=1.5, y=1.5, label = "B"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 1, y = 1.5, xend = 1, yend = 2.1), color = "black", linetype = "dashed") +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 1.5), color = "black", linetype = "dashed") +
#Mark Zone C
  geom_text(aes(x=2.27, y=1.17, label = "C"), color = "black", hjust=0, size=5, alpha = 0.1) +
#Mark Zone D
  geom_text(aes(x=3.6, y=0.9, label = "D"), color = "black", hjust=0, size=5, alpha = 0.1) +
  geom_segment(aes(x = 4, y = 0.65, xend = 4, yend = 1), color = "black", linetype = "dashed") +
#Mark intersection points
  geom_point(aes(x = 2, y=1), color = "red", size=2.5) +
  geom_point(aes(x = 4, y=1), color = "red", size=2.5) +
  geom_text(aes(x=1.8, y=0.93, label = "P1"), color = "black", hjust=0, size=3, alpha = 0.1) +
  geom_text(aes(x=3.8, y=0.93, label = "P2"), color = "black", hjust=0, size=3, alpha = 0.1) +

#Add line description
  geom_text(aes(x=0, y=1.55, label = "Expected Quality without \nPre-Purchase Information\n _  _  _  _  _  _  _  _  _  _  "), color = "Black", hjust=0, size= 2.2, alpha=0.1) + 
  geom_text(aes(x=0, y=2.05, label = "Actual Quality with \nPre-Purchase Information\n ___________________"), color = "Black", hjust=0, size= 2.2, alpha=0.1) +
  
#Set specific layout
  theme_bw() +
  labs(title = "How is Pre-Purchase Information related to Welfare?",
       x = "Quantity", 
       y = "Price") +
  scale_x_continuous(breaks = 1:5, labels = c("", "Q1", "Q*", "Q2", "")) +
  theme(
  axis.text.y=element_blank(),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#>
  
```

As visualized in the upper graph, the consumer would choose quantity Q1 for $$(\bar R_{j} < R_{j})$$, quantity Q* for $$(\bar R_{j} = R_{j})$$ and quantity Q2 for $$(\bar R_{j} > R_{j})$$. For choosing Q1, the consumer would expect a surplus of triangle A while getting an actual surplus of A + B (ex post recognition of actual quality). A consumer with access to pre-purchase information would always choose Q* with a surplus of A + B + C. Overestimating the books quality, the consumer would buy quantity Q2 and obtains a surplus of A + B + C - D. It is obvious, that quality expectations affect the overall utility of the consumer and thus the demand curve appears to shift without impacting the price elasticity. In fact, the curve merely adjusts to the actual demand. 

#< quiz "ValueAdded_Triangle"
question: Which triangle represents the value added of pre-purchase information?
sc:
    - A
    - B
    - C*
    - D
success: Great, your answer is correct!
failure: Try again.
#>

**Summary**

After looking at a simple market diagram, we learnt to distinguish between demand and supply, to categorize the economic meaning of welfare and the composition of consumer / supplier surpluses and the formation of equilibrium values. Focusing the demand side, we got an insight into the topic of price elasticity to better understand the following examination of price elasticities on books. Hence, after collecting the necessary knowledge, we returned to pre-purchase information and their affects on welfare. We found out that pre-purchase information impacts the expectation for quality and leads to an adjustment of the demand curve. 

In the following chapter 2, we get an introduction to the Amazon data set followed by descriptive analyses.

## Exercise 2 -- Data and Descriptive Insights (3814)

The underlying examination by Reimers and Waldfogel is based on a data set provided by Amazon. 

In the first part, we deepen our understanding and the origin of the underlying data set and define important attributes that we later use for descriptive and empirical measurements.

Second, we focus on professional reviews and crowd ratings to investigate superficial contexts. To illustrate them, we use descriptive tables. 

Finally, we create an entire overview about descriptive analyses to detect potential effects between professional and non-professional reviews on sales ranks and prices to create a transition to the following empirical part of my Thesis.

After working through this chapter, you are surefooted in dealing with the underlying data set and thus well prepared to continue with the empirical part in chapter 3. 

### Structure

2.1 Introduction to the data set

2.2 Analysis of the pre-purchase information

2.3 Recognition of Potential Effects through overall Descriptive Approaches


## Exercise 2.1 -- Introduction to the Data Set

As already mentioned, the entire quantitative calculations are based on a data set provided by Amazon. The data set includes slightly less than 8.8 million observations of non-professional crowd ratings. One observation represents review(s) at one day between 02-01-2018 and 31-12-2018 for very book in every country. Observations were made in the USA, in UK and in Canada. In addition, the data set was merged with newspaper data from every newspaper magazine listed in 1.1 with information on whether the newspaper reviewed the particular book and whether the New York Times recommended the book. In fact, only 3.22 million observations were taken into consideration due to missing values, which reduces the actual market share what we examined. Reimers and Waldfogel claim that Amazon covered about 44.5 percent of the physical book market share in 2017, which we cannot accurately confirm. After removing about 63% of the data set, 44.5 percent cited by Reimers and Waldfogel means that only a little over 16 percent of the book market still covered by the Amazon data set. To improve the problem sets performance, we have already removed observations that are not relevant to our examination. 

To better understand our data set, we categorize some important variables and take a look at an excerpt of it. 

**Task:** Use the function `readRDS` to load the data set called `dataEst.RDS`. Save this data set under the name `data`.

```{r "2.1.1", warning=FALSE}
setwd("D:/Universitaet Ulm Unterlagen/Bachelorarbeit/The-Impact-Of-Reviews-And-Ratings-On-Bookmarket/replication/data")
#< task
#use readRDS("file_name.RDS") to load your data
#>
data <- readRDS("dataEst.RDS")
#< hint
cat("d__a <- re_dR__('da__Es_.RD_'.")
#>
```

You successfully loaded the data set. Now, we want to see how the data set is organized and which values the single attributes can take. 

**Task:** Use the function `head` to show the first rows from `data`. In addition, use the function `colnames` to list every column name from `data`.

```{r "2.1.2"}
#< task
#use head(data set name) to show your data set
#>
head(data)
colnames(data)
#< hint
cat("h_e_(da__)")
cat("col_a__s(__ta)")
#>
```

As one can see, the main data set consists of 69 variables. Not each of these are essential for the following course and will not be considered further. For those to whom this does not apply will find an explanation below. 

**Identification Variables**

The variable `asin` represents the corporate amazon ID to differentiate between different products. `country` shows us whether the underlying review belongs to the US market (US), the Great Britain market (GB) or to the Canadian market (CA). `canum` is our main ID and combines these variables to create a powerful country-dependent identifier. A title-author identifier is stored in the variable `titleno`. Identification variables are indispensable for each data set to clearly distinguish each variable. In this data set, the combination of `ddate` (see below) and `canum` defines each unique observation. 

**Chronological variables**

`ddate` shows the main date on which the crowd ratings were created What all other chronological variables are based on. `NYT_elapse` (New York Times), `BG_elapse` (Boston Globe), `CHI_elapse` (Chicago Tribune), `LAT_elapse` (Los Angeles Times), `WAPO_elapse` (Washington Post), `WSJ_elapse` (Wall Street Journal) and `OTH_elapse` (All magazines except NYT) indicate how many days have passed since/until the publication of the individual professional review. As a reference date serves here `ddate`. `epos` (if > 0) and `eneg` (if < 0) are counting days since publication. 

**Value Variables** 

The variable `rank` stands for the Amazon sales rank, on the basis of which we will perform most of the calculations. `pamzn` shows us the price of the particular book while `R` provides the given star rating on a five-point scale. `review` delivers the number of reviews in total, so this variable delivers the same number for every specification of `canum`. Each of these attributes occurs twice and once each with an "l" in front of it. This means that the values are logarithmized. 

**Dummy Variables** 

Dummy variables are binary variables that can take only two values (here: 1 and 0). Any variable starting with `dnytpost` indicates "1" if the New York Times has published reviews within the period defined by the following numbers after `dnyt`. For instance, `dnytpost1_5` takes the value 1 if [0 <`NYT_elapse` < 6]. For `dnytpost` and `dnytpost10`, [0 <`NYT_elapse` < 10] and [11 <`NYT_elapse` < 20] holds. `dnytpostpre` takes the value 1 if [-10 < `NYT_elapse` < 20]. The same principle applies to all variables starting with `dothpost` where the variable `OTH_elapse` defines the base instead of `NYT_elapse`. 

To transform sales ranks from our main data set into quantities, the authors provided confidential data to determine quantities. This data got provided by Nielsen, a market research company, and Reimers and Waldfogel have published it only in already edited form. These mentioned confidential data sets contain information about weekly top 100 sold books between 2015 and 2018 while the accessible confidential data only delivers intermediate calculations to determine price elasticities. 

**Note:** In the embedded background of this environment in the further course again and again transformations of data are accomplished, without which many computations would not function. If significant changes of known variables or structures take place, these will be explained again in the course of the future tasks.

**Summary**

Apparently, with the underlying data set, we have not only a very large set of observations in a defined time period, but also a large amount of information in the form of variables. Especially in this case, where an already merged and edited dataset has been published, this large amount of information may also have a detrimental effect on the correct replication of this study. Nevertheless, we continue with the data set provided by Reimers and Waldfogel. In this chapter, we have gained insight into the data set by executing first code chunks in R and the key variables that will continue to guide us throughout the study. Finally, we learned how to define dummy variables and the importance of identifying variables. 

In chapter 2.2, we dare a first dive into the data set by creating descriptive statistics with focus on magazine reviews and Amazon crowd ratings. 


## Exercise 2.2 -- Analysis of the Pre-Purchase Information

In the following part, we will analyze data to gain a deeper understanding of the occurrence of professional ratings. In addition, we want to check whether the collected data from journals appropriates for following estimations. 

**Task:** Create two dummy variables that indicate `1` when an observation belongs to a book that has been reviewed. Additionally, read in the data set `DesRat` and save it accordingly, and then `check` the chunk.

**Note:** In sum, the underlying data set includes data about **8770 books**. 

```{r "2.2.1", warning=FALSE}

#< fill_in

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$____ <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 , _, _)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , __ 0)

DesRat <- r____DS(_________DS_)

#>

#Creating two new dummy variables to indicate professional reviews from non-NYT Magazines and from all Magazines
data$DOTH <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 , 1, 0)
data$DALL <- ifelse(data$DBG == 1 | data$DCHI == 1 | data$DLAT == 1 | data$DWAPO == 1 | data$DWSJ == 1 | data$DNYT == 1 , 1, 0)

DesRat <- readRDS("DesRat.RDS")
```

**Task:** Press `check` to visualize the percentage of books reviewed by magazines.

```{r "2.2.2", warning=FALSE}

#Create Row Names
rownames(DesRat) <- c("Relative share", "Absolut share") 

#Use of kbl() funktion to create a visualable table
DesRat  %>%
kbl(col.names = c("Share_of_NYT_Ratings" = "New York Times", "Share_of_BG_Ratings" = "Boston Globe", "Share_of_CHI_Ratings" = "Chicago Tribune", "Share_of_LAT_Ratings" = "Los Angeles Times", "Share_of_WAPO_Ratings" = "Washington Post", "Share_of_DWSJ_Ratings" = "Wall Street Journal", "Share_of_OTH_Ratings" = "Non New York Times", "Share_of_ALL_Ratings" = "All"), caption = "Share of Professional Reviewed Books ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 


```



In total, 12.66 percent of all books (1521) included in the data set have been professionally reviewed. 11 percent (1315) of these books were reviewed by the New York Times, representing the largest percentage of books reviewed by professionals. The second largest percentage provides the Chicago Tribune with only 1.36 percent (139), which is approximately ten times less than the number of reviews published by the New York Times. Overall, non-New York Times magazines account for nearly 3.1 percent of the share. The following calculations distinguish between The New York Times and non-New York Times magazines, so we should note the size of the intersection between these distinctions. 

**Task:** Create according to chunk `2.2.2` a table based on the function `kbl()` to visualize the intersections between the single magazines. 

```{r "2.2.3", warning=FALSE}
#< fill_in

#gt_table

DesIntersection <- re___D_("DesIntersection.RDS")

row.names(_______________) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections", "Sum")

DesIntersection  %>%
kbl(__l_n____ = c("Intersection_NYT" = "∩ New York Times", "Intersection_LAT" = "∩ Los Angeles Times", "Intersection_BG" = "∩ Boston Globe",  "∩ Intersection_CHI" = "∩ Chicago Tribune", "∩ Intersection_WAPO" = "∩ Washington Post", "Intersection_DWSJ" = "∩ Wall Street Journal"), ______ = "Intersections between New York Times and other magazines ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("", 8, 8) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

#>

DesIntersection <- readRDS("DesIntersection.RDS")

row.names(DesIntersection) <- c("New York Times", "Los Angeles Times", "Boston Globe", "Chicago Tribune", "Washington Post", "Wall Street Journal", "Without Intersections", "Sum")

DesIntersection  %>%
kbl(col.names = c("Intersection_NYT" = "∩ New York Times", "Intersection_LAT" = "∩ Los Angeles Times", "Intersection_BG" = "∩ Boston Globe",  "∩ Intersection_CHI" = "∩ Chicago Tribune", "∩ Intersection_WAPO" = "∩ Washington Post", "Intersection_DWSJ" = "∩ Wall Street Journal"), caption = "Intersections between New York Times and other magazines ") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("", 8, 8) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

Looking at the New York Times reviews, we noticed that a number of 201 books were reviewed by at least one other magazine. Conversely, a larger proportion of the reviews from other magazines got were also rated by the New York Times. To determine the exact proportion, we calculated that at least 30 percent (Wall Street Journal) and at most 67 percent (Boston Globe) of non-New York Times magazines were also reviewed by the New York Times. As a result, estimates in this regard may be biased by mutual overlays. The row or column sum does not reflect the total number of reviews in the journals, as we only consider individual overlays. In reality, there are also multiple overlays where three or four journals review one book. 

The advantage for crowd ratings is that we also have the amount of ratings available. In fact, the selection of books to be evaluated by professional reviewers is not random (Berger, Sorensen, Rasmussen, 2010). We want to review whether book ratings from professional magazines are randomly distributed on their five-points scale on Amazon. If this is not the case, we cannot reject the assumption that journals only review "better" rated books on Amazon and vice versa. The left Graphs in the following illustrates that the distribution of professional reviews among crowd ratings is evenly distributed. 

On the other hand, whether authors are popular also depends on the number of books they have published so far. If the authors name is more popular through previous published books, professional reviewers could potentially tend to prefer those authors. The chart on the right visualizes the proportion of professional reviews among the number of the books previously published by the authors. 


#< quiz "Distribution_Guess"
question: Focusing the published books, What is your suggestion? 
sc:
- The more books an author publishes, the more likely these books will be professionally reviewed. 
- The more books an author publishes, the less likely these books will be professionally reviewed.
- The overall distribution is even.*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Replace every ____ gap with ....... and check this chunk to create the data set the following graphs are based on. 
```{r "2.2.4", warning=FALSE}

#Create Dataset "DesCrowd" to show the distribution among crowd ratings
DesCrowd <- data %>%
  group_by(R) %>%
  summarise(Number_Ratings = n(),
            Number_ = round(sum(DNYT)/ Number_Ratings, 2),
            Number_1 = round(sum(DOTH)/ Number_Ratings, 2))

y1 <- DesCrowd[,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2 <- DesCrowd[,c(1, 4)] %>%
  mutate(Group = "Others")     

colnames(y1) <- c("R", "Number", "Group")
colnames(y2) <- c("R", "Number", "Group")

DesCrowd <- rbind(y1, y2)

#Create Dataset "DesAuth" to show the distribution among authors published books 

DesAuth <- data %>%
  group_by(numbooks) %>%
  summarise(Number_Ratings = n(),
            Number_ = round(sum(DNYT)/ Number_Ratings, 2),
            Number_1 = round(sum(DOTH)/ Number_Ratings, 2))


y1_1 <- DesAuth[,c(1, 3)] %>%
  mutate(Group = "New York Times")
y2_1 <- DesAuth[,c(1, 4)] %>%
  mutate(Group = "Others")     

colnames(y1_1) <- c("numbooks", "Number", "Group")
colnames(y2_1) <- c("numbooks", "Number", "Group")

DesAuth <- rbind(y1_1, y2_1)

```

After we have created the dataset, we can proceed with the creation of the chart.

**Task:** Execute the following chunk to create two graphs representing the distribution of professional reviews among two variables. 

```{r "2.2.5", warning=FALSE}

#Create the related Graphs

#Create the graph based on crowd ratings

f1 <- ggplot(aes(x = R, fill = Group), data = DesCrowd) +
  geom_bar(aes(y = Number), width = 0.5, stat = "identity", color = "black", position = position_dodge()) +
  geom_vline(xintercept = quantile(data$R, probs = 0.25, na.rm = TRUE)) +
  geom_text(aes(x=4.23, y=0.5, label = "25%"),angle = 270, color = "black", hjust=0, size=6, alpha = 0.1) +
  theme_bw() +
  labs(title = "Crowd Ratings",
       x = "Amazon Star Rating", 
       y = "Relative proportion of professional reviews") +
  scale_y_continuous(breaks = seq(0,1,0.1), labels=scales::percent) +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = c(0.85, 0.75),
  panel.grid.minor=element_blank(),plot.background=element_blank())

#Create the graph based on authors number of books 

f2 <- ggplot(aes(x = numbooks, fill = Group), data = DesAuth) +
  geom_bar(aes(y = Number), width = 0.5, stat = "identity", color = "black", position = position_dodge()) +
  geom_vline(xintercept = quantile(data$numbooks, probs = 0.25, na.rm = TRUE)) +
  geom_text(aes(x=4.23, y=0.35, label = "25%"),angle = 270, color = "black", hjust=0, size=6, alpha = 0.1) +
  theme_bw() +
  labs(title = " Published Books",
       x = "Number of Published Books ", 
       y = "Relative proportion of Professional Reviews") +
  scale_y_continuous(breaks = seq(0,1,0.1), labels=scales::percent) +
  scale_x_continuous(breaks = seq(0, 75, 5)) +
  xlim(0, 75) +
  guides(fill=guide_legend(title="")) +
  theme(legend.position = c(0.85, 0.75),
  panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(f1, f2, ncol = 2)
```

If we focus on the distribution of Amazon star ratings, we recognize a slight leftward skew in the weaker rated books on Amazon. However, this high proportion is to the left of the 25 percent quantile line which states that 75 percent of ratings on Amazon are rated 4.1 stars or even better. In the 75 percent zone, the distribution is rather even while outliers in the 25 percent zone are not unusual. This could be because books that have already been rated poorly on Amazon tend to be rated less often as a result. Hence, we can assume that Amazon star ratings do not visibly influence the book selection of professional reviewers and vice versa. Actually, a remaining risk always exists. 

The on right chart, we notice a quite even distribution among the number of published books between 0 and 21 percent. The 25 percent quantile line is located at two published books which states that a huge amount of 25 percent of all books reviewed on Amazon are written by "inexperienced" authors who published two books or less. The distribution remains even until 65 published books. After that, almost no more books were reviewed. Because of that, we limited the x-axis. Similar to star ratings, we can assume that the book selection of professional reviewers is less dependent on the number of books published so far. 

**Summary**

In summary, we have gained deeper insights into the relative and absolut numbers of professional reviews. It was illustrated to us that a relatively high proportion, between 30 and 67 percent, of all non-New York Times magazines were also reviewed by the New York Times, which may bias future estimates. To disprove the assumption that the selection of professionally reviewed books is not random, we use two different variables (Amazon Star Rating and Number of previous published books) to check whether the relative distribution on said variables disproves this assumption. In both cases we found out, that the distributions regarding to these variables are more or less even which is why we may assume a random choice of books for the time being. After all, the professional reviewers at the New York Times have discretion over about which books they actually review. Accordingly, we can never assume a 100 percent random distribution.

## Exercise 2.3 -- Recognition of Potential Effects through Descriptive Approaches

Before we start identifying potential effect, we need a good overview of the relevant descriptive values. Generally, descriptive values are based on simple statistics to describe and to visualize contexts, that have already happened. These values are easily understood and can provide sound summaries of high data volumes within seconds. The most powerful function of descriptive analyses is the ability to identify relationships in order to make predictions using more complex statistics. This procedure is exactly the same as what we have already done in chapter 2.2. 

Starting with a good overview, we want to create a table including simple descriptive values distinguished by `country`. 

**Task:** Try to fill any ____ gap with ....... and check this chunk to create an overview of the main descriptive values.

```{r "2.3.1", warning=FALSE}

#fill_in

# Descriptive Values differentiated by country
dataDesKript <- ____ %>%
   group_by(_______) %>%
   summarize(Price = round(mean(_____),2), 
            Star_Rating = round(____(R),2), 
            Sales_Rank = round(mean(rank),2), 
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quan____(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quan____(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quan____(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quan____(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = NROW(country), 
            Editions = n_distinct(asin))
#Read in dataDesKript2.RDS
dataDesKript2 <- ________("dataDesKript2.RDS")
# Merge Dataframes
DataDes <- rbind(dataDesKript, dataDesKript2)

# Transform Dataframe into a clearer schema
DataDesTest <- t(DataDes)
colnames(DataDesTest) <- rownames(DataDes)
DataDescriptive <- as.data.frame(DataDesTest)
colnames(DataDescriptive) <- unlist(DataDescriptive[1,])
DataDescriptive <- DataDescriptive[-1,]

# Change Row Names 
row.names(Data__________) = c("Price", "Star rating", "Sales rank", "Number of ratings", "10th", "25th", "50th", "75th", "90th", "Titles", "Observations", "Editions")

# Kable-Function to create an attractive overview
DataDescriptive %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)
#< 

dataDesKript <- data %>%
   group_by(country) %>%
   summarize(Price = round(mean(pamzn),2), 
            Star_Rating = round(mean(R),2), 
            Sales_Rank = round(mean(rank),2), 
            Number_of_Ratings = round(mean(review), 2),
            Teenth = quantile(R, probs = 0.1, na.rm = TRUE), 
            Tweentyfifth = quantile(R, probs = 0.25, na.rm = TRUE),
            Fiftith = quantile(R, probs = 0.5, na.rm = TRUE),
            Seventyfifth = quantile(R, probs = 0.75, na.rm = TRUE), 
            Ninetith = quantile(R, probs = 0.9, na.rm = TRUE),
            Titles = n_distinct(titleno),
            Observations = NROW(country), 
            Editions = n_distinct(asin))
#Read in dataDesKript2.RDS

# Merge Dataframes
DataDes <- rbind(dataDesKript, dataDesKript2)

# Transform Dataframe into a clearer schema
DataDesTest <- t(DataDes)
colnames(DataDesTest) <- rownames(DataDes)
DataDescriptive <- as.data.frame(DataDesTest)
colnames(DataDescriptive) <- unlist(DataDescriptive[1,])
DataDescriptive <- DataDescriptive[-1,]

# Change Row Names 
row.names(DataDescriptive) = c("Price", "Star rating", "Sales rank", "Number of ratings", "10th", "25th", "50th", "75th", "90th", "Titles", "Observations", "Editions")

# Kable-Function to create an attractive overview
DataDescriptive %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)
```


The first four rows return the mean values of the book price, the Amazon Star Rating, the sales rank and the number of ratings per book. The book markets in all three considered countries do not have fixed book prices. Fixed book prices (FBP) means that the publisher has the exclusive right to set the price of his book. The retailer is not permitted to discount more than five percent from this set price (Nakayama, 2015).

#< quiz "Fixed_Prices"
question: Comparing UK (without FBP) and Germany (with FBP), which country accounted a higher price increase between 1996 (end of FBP in UK) and 2018? 
sc:
- United Kingdom (UK).* 
- Germany.
success: Great, your answer is correct! The UK accounted a price increase of 80 percent after this period, while Germany accounted an increase of 29 percent (Fuchs, Sprang, Beurich, Götz, 2019)
failure: Try again.
#>

Unfortunately, our data set only includes countries data from countries without FBP. Comparing the prices of the three available countries, we recognize a large price difference between Canada (21.07) and Great Britain (13.12) while the US account an average price of 15.86. Possible reasons for those expensive book prices could be that Canada imports many of these books where fees and other costs are incurred (Kwan, 2013), transportation costs over large land masses and a loss of economies of scale due to a smaller book market. The star ratings and their percentiles are quite even while large differences occur in the sales ranks and the number of ratings. Considering the fact that sales ranks are generated on their individual market place and less transparent, we cannot list any specific reasons for this. The high differences in the number of ratings may be due to the level of awareness of Amazon in the individual countries. Obviously, the US market accounts for more than twice as many observations as Canada or Great Britain.      

Hence, we gained insight into general descriptive values over the entire data set. To elaborate this, let us review the same values for observations where the availability of professional reviews is guaranteed. 

**Task:** Use the fuction `readRDS` to read the RDS-file `DataDescriptiveJournals`. Save this data set under the file name `DataDescriptiveJournals`. In Addition, create a table with `kbl()` according to the table above.

```{r "2.3.2", warning=FALSE}
#read out 
DataDescriptiveJournals <- readRDS("DataDescriptiveJournals.RDS")
# Kable-Function to create an attractive overview

DataDescriptiveJournals %>%
kbl(col.names = c("CA" = "Canada", "GB" = "Great Britain", "US" = "United States", "All" = "All"), caption = "Group Rows") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("Star rating percentiles", 5, 9) %>%
  pack_rows("", 10, 12)

```

Without differentiating between magazines or knowing whether the review turned out "positive" or "negative", we find an increase of price of about 15 percent, a decrease in average sales rank of about 39 percent, and the average number of ratings of about 86 percent, as well as significantly higher variance in percentiles of star ratings. The Amazon star rating also drops by a smaller percentage of about 2.3 percent. This leads to the assumption that the occurrence of professional reviews extensively affects the price, the sales rank and the number of ratings. 

The variance in percentiles of star ratings could be due to the fact that we have no information about how it has been reviewed, with which a polarization of the reviews may have taken place. For the New York Times reviews, the main data set contains information about whether the New York Times recommended a book using the variable `drecommended`. In the following, we generate two chronological graphs to show the descriptive affects from professional reviews on sales ranks and prices. 

**Note:** For further examinations we distinguish between the US data and the entire data, as the US market is the largest and has the most average crowd ratings (cno == 3 -> US). 

**Task:** Execute the following chunk to create three different filtered data sets. Press `check` to collect your points.

```{r "2.3.3", warning=FALSE}

#< fill_in

#U.S. data recommended by New York Times 
dataReviewRec <- data %>%
  filter(drecommended == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = ____(rank, na.rm = TRUE), 
            AVGCR = ____(R, na.rm = TRUE))

#U.S. data reviewed by the New York Times but not recommended
dataReviewNRec <- data %>%
  filter(drecommended == _ & DNYT == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE))

#U.S. data not recommended or reviewed by The New York Times.
dataReviewNNYT <- data %>%
  filter(drecommended == _ & DNYT == _ & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE), 
            AVGCR = mean(R, na.rm = TRUE)) 
#>

```
 
After creating these three datasets, proceed to transform the data by creating the final dataset to visualize the results. 
 
**Task:** Replace every ____ gap with the correct code and check this chunk to create the data set the following graphs are based on. 
 
```{r "2.3.4", warning = FALSE}
dataNYTPrice <- data.frame(Price = c(dataReviewRec$AVGPrice, dataReviewNRec$AVGPrice, dataReviewNNYT$AVGPrice), SalesRank = c(dataReviewRec$AVGSalesRank, dataReviewNRec$AVGSalesRank, dataReviewNNYT$AVGSalesRank), Category = c("Recommended", "Not Recommended", "Not NYT"))

#Plot 1: Mean Prices
f3 <- dataNYTPrice %>%
   ggplot() +
   geom_histogram(aes(y = Price, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = Price - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ylim(0, 21) +
  # geom_bar(aes(y = SR, x = Category,  fill = Category), position = "dodge", stat = "identity")
 scale_fill_manual("", values = c("Not NYT" = "deepskyblue", "Not Recommended" = "darkorchid2", "Recommended" = "blue3")) +
  labs(title = "Differences in mean prices (US Data only)",
       x = "", 
       y = "Price in $US") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f4 <- dataNYTPrice %>%
   ggplot() +
   geom_histogram(aes(y = SalesRank, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = SalesRank - 75000, label = round(SalesRank, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
#   ylim(0, 21) +
  # geom_bar(aes(y = SR, x = Category,  fill = Category), position = "dodge", stat = "identity")
 scale_fill_manual("", values = c("Not NYT" = "deepskyblue", "Not Recommended" = "darkorchid2", "Recommended" = "blue3")) +
  labs(title = "Differences in mean Sales Ranks (US Data only)",
       x = "", 
       y = "Sales Rank") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom",
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(f3, f4, ncol = 2)
```

The Price differences between New York Times recommended and non-recommended books is not significant while there is a big price gap between New York Times data and non new York Times data. The fact that the price difference between recommended and non-recommended books is quite small could be attributable to the fact that a non-recommendation does not always equate to a poor rating. On the other hand, books not mentioned in the New York Times are on average about $2.8 US cheaper than books mentioned in the New York Times.                    Observing the average sales ranks, we determine clearer differences between these three categories. Since the sales ranks are not direct quantity data, they can still serve as a quite useful comparative value. We see a decline of approximately 35% in sales rank from category to category. 

#< quiz "Prices_and_SalesRanks"
question: Which of the following statements may be made? 
sc:
- With the information of these graphs we can assume a price elasticity ε < 1 for the US book market. 
- None of those listed.
- With the information of these graphs we can assume a price elasticity ε > 1 for the US book market.
- The effect of professional reviews on sales ranks (quantities) could be higher than the effect on prices.*
success: Great, your answer is correct! 
failure: Try again.
#>

We want to conduct a similar examination for crowd ratings, with the difference that we have information not only on the occurrence of these ratings, but also on their level on the five-point scale. Above, we already noted a negative association between the Amazon star rating and the occurrence of professional reviews. 

Before we start, let us divide Amazon star ratings into three categories: less than three stars, three and four stars, and more than four stars. 

**Task:** Create three dummy variables that show `1` when the star rating is between one and three, three and four and more than five. Orientate on exercise `2.2.1` to create a dummy variable. Then, try to replace the ___ gaps to create three filtered data sets according to `2.3.3`. Name these data sets `dataCR1_3`, `dataCR3_4` and `dataCR4_5`.


```{r "2.3.5", warning = FALSE}

data$starRating1_3 <- ifelse(data$R >= 1 & data$R < 3, 1, 0)
data$starRating3_4 <- ifelse(data$R >= 3 & data$R < 4, 1, 0)
data$starRating4_5 <- ifelse(data$R >= 4 & data$R < 5, 1, 0)

dataCR1_3 <- data %>%
  filter(data$starRating1_3 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))

dataCR3_4 <- data %>%
  filter(data$starRating3_4 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))

dataCR4_5 <- data %>%
  filter(data$starRating4_5 == 1 & DALL == 0 & cno == 3) %>%
  summarise(AVGPrice = mean(pamzn), 
            AVGSalesRank = mean(rank, na.rm = TRUE))


```

**Task:** Now run the following chunk to see how crowd ratings affect the price and sales ranks. Press `check` to confirm. 

```{r "2.3.6", warning= FALSE}

dataCRPrice <- data.frame(Price = c(dataCR1_3$AVGPrice, dataCR3_4$AVGPrice, dataCR4_5$AVGPrice), SalesRank = c(dataCR1_3$AVGSalesRank, dataCR3_4$AVGSalesRank, dataCR4_5$AVGSalesRank), Category = c("1-3 stars", "3-4 Stars", "more than 4 stars"))

#Plot 1: Mean Prices
f5 <- dataCRPrice %>%
   ggplot() +
   geom_histogram(aes(y = Price, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = Price - 1.5, label = round(Price, 2)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
   ylim(0, 21) +
 scale_fill_manual("", values = c("1-3 stars" = "deepskyblue", "3-4 Stars" = "darkorchid2", "more than 4 stars" = "blue3")) +
  labs(title = "Differences in mean prices (US Data only)",
       x = "", 
       y = "Price in $US") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom", 
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

#Plot 2: Mean Sales Ranks
f6 <- dataCRPrice %>%
   ggplot() +
   geom_histogram(aes(y = SalesRank, x = Category,  fill = Category), stat = "identity", width = 0.3) +
  geom_text(aes(x = Category, y = SalesRank - 220000, label = round(SalesRank)), color = "white", size = 5) +
   coord_flip() +
   scale_x_discrete(expand = c(0, 1)) +
   theme_bw() +
 scale_fill_manual("", values = c("1-3 stars" = "deepskyblue", "3-4 Stars" = "darkorchid2", "more than 4 stars" = "blue3")) +
  labs(title = "Differences in mean Sales Ranks (US Data only)",
       x = "", 
       y = "Sales Rank") +
  guides(fill = guide_legend(reverse = TRUE)) +
   theme(legend.position = "bottom",
   axis.text.y=element_blank(),
   panel.grid.minor=element_blank(),plot.background=element_blank())

grid.arrange(f5, f6, ncol = 2)

```

In contrast to the affects of professional reviews, the mean price exhibits an inverse behavior. The Price seems to decrease the more stars a rating has while sales ranks generally behave according to the occurrence of professional reviews. Obviously, crowd ratings affect prices differently than professional reviews in magazines. This could be due to professional reviews triggering a (short-term) increase in demand that may not be served in the short term, leading to a price increase. On the other hand, higher crowd ratings on Amazon could rather resulting in long-term increase in demand, which leads to more contested competition, where everyone has to reduce their prices.                                                                 In fact, book pricing depends on more factors than are included in our data set. 

The mean sales rank decreases the more stars were given. As we noticed, more than 75 percent of all observations were rated with four stars or higher, so we can assume a similar percentage of observations in the upper bar (565063). Nevertheless, the mean sales rank for books recommended by the New York Times is significantly lower than the mean sales rank of books rated more than four stars by the crowd. Nevertheless, the mean sales rank for New York Times-recommended books is significantly lower than the mean sales rank for crowd ratings rated higher than four stars. 

To verify whether absolut book prices remain constant over time, we create a timeline for the average book price. We only observe for top 10000 and top 1000 sales ranks because we claim higher time based dependencies the 

**task:** Replace every ___ gap with the correct code. Use `left_join` to merge the data sets `dataTop10000` and `dataTop1000` by `ddate`

```{r "2.3.7", warning=FALSE}

dataTop10000 <- data %>%
  filter(cno == 3 & rank <= 10000) %>%
  group_by(ddate) %>%
  summarise(price = mean(pamzn), 
            Group = "Top 10.000")

dataTop1000 <- data %>%
  filter(cno == 3 & rank <= 1000) %>%
  group_by(ddate) %>%
  summarise(price = mean(pamzn), 
            Group = "Top 1.000")

dataTop <- rbind(dataTop10000, dataTop1000)

dataTop %>%
   ggplot() +
   theme_bw() +
   geom_line(aes(x = ddate, color = Group,  y = price), stat = "identity", width = 0.3,  position = position_dodge()) +
   scale_color_manual("Category", values = c("Top 10.000" = "deepskyblue", "Top 1.000" = "red"))  +
   labs(title = "Differences in mean Sales Ranks (US Data only)",
        x = "Date", 
        y = "Mean Price in $US")  
  
```

Overall, prices rise within one year by approximately $1.5 US. Price collapses can be observed in the spring, and there is a massive drop in prices in the summer months from June to September, with average prices (Top 1.000) falling by around 14 percent. The slump at the end of November is more like an outlier value. Obviously, better sold books tend to vary in price due to specific seasons. Lower prices in summer could be due to lower demand, as people prefer to read during the winter months. For the following event study in chapter 3.3 we have to take into account that there are fluctuations in book prices within a year and that these can be attributed to various factors. 

**Summary**

Using descriptive analysis, we gained deeper insight into the data and were able to establish initial associations between variables related to pre-purchase information and price or quantity (sales ranks). First, we created an overview over general values of the underlying data set. By selectively filtering out certain observations that were effected by pre-purchase information from professional reviewers, we found differences primarily in prices and sales ranks. We have deepened our research and found out that both professional reviews and Amazon crowd ratings negatively affect the sales rank (a lower rank implies higher volumes sold). Conversely, crowd ratings lower average prices while professional reviews raise average prices. Finally, we reviewed whether book prices fluctuating within one year and found out that the more "successful" books are, the more the price fluctuates within one year. For long-term studies, we need data that go beyond a one-year horizon. 

In the following chapter, we start with the empirical part of this problem set. 

## Exercise 3 -- Empirical Strategies on Sales Ranks and Prices (ca. 4650)

In following chapter *Empirical Strategies on Sales Ranks and Prices* we focus on empirical methods in general and their application to real data from Amazon. 

First, we give an introduction to empirical basics about the subject of regressions and the specific methods that come along with it. In general, it deals with control variables, fixed effects, robust standard errors, and the use of logarithmic estimates. 

In chapter 3.2, we focus on the replication of the main regression originally created by Reimers and Waldfogel. We discuss different of those effects and deepen the examination with further regressions. Based on these regressions on real data, we also focus on the validity of regressions and the explanation of values that inform them. 

Chapter 3.3 explains the subject of event studies. We also conduct two event studies based on the underlying data set to identify long- and short-term reactions of sales ranks and prices to professional reviews. 

Working through this chapter will provide fundamentals to advanced empirical strategies that are used to make predictive statements in economic science. 

### Structure

3.1 Regressions, Robust Standard Errors and Fixed Effects 
 
3.2 Estimation of the Effects on Sales Ranks and Prices
 
3.3 Introduction and Implementation of Event Studies

## Exercise 3.1 -- Regressions, Robust Standard Errors and Fixed Effects

Before we get into the subject matter of regressions, we learn how to classify this topic. In general, regressions describe a quantitative statistical approach to explaining associations between a dependent variable and one (linear) or more (multiple) independent (explanatory) variables in order to gain predictive information. The formula for a linear regression is as follows:

$$ y = x_0 + β_1x_1 + \varepsilon $$

where y indicates the dependent variable and x1 represents the explanation variable. x0 denotes the so called intercept on the y-axis, id est the average value of y if the explanatory variable x1 has no influence. ε is the error term and shows the sum of the residuals from the regression. Finally, β1 indicates the average increase or decrease of y for every unit increase of x1. Here, the value of β1 is determined by the Ordinary least squares method (OLS). Further information about this method you find [here](https://www.mathsisfun.com/data/least-squares-regression.html).                                      

#< quiz "Simple_Regression"
question: We want to estimate the effect from years of education (x1) on the income (y). We determined the following formula for 1.000 people with ŷ = 500 + 300*x1. What is the correct answer? 
sc:
- With ten years of education you exactly earn 3500 monetary units. 
- Within the 1.000 individuals, each additional year in education results in 300 more monetary units.
- We only have 1.000 observations - we cannot make a statement.
- Within the 1.000 individuals, each additional year in education results in an average of 300 more monetary units -> to make statements beyond our sample, we rather need more data and more variables.*  
success: Great, your answer is correct!
failure: Try again.
#>

Obviously, the amount of income depends not only on the years of education (yoe). In reality, many more measurable variables affect the amount of income. For instance, the older people get the more they tend to earn. Hence, the age also effects the amount of income. In following we add the variable age as a so called *control variable* to the regression from the quiz:  

$$ y = x_0 + β_1yoe + β_2age + \varepsilon $$

Now we directly control for the effect of age on income, so that age no longer affects the coefficient on price. However, adding to many control variables could lead to over fitting, which means that the model is overfitted to the specific sample, so that the model reacts to random variation instead of actual contexts. 

#< quiz "Multiple_Regression"
question: What is the correct answer? 
sc:
- Adding the age to the regression increases the value of β1. 
- Adding the age to the regression decreases the value of β1.*
- Adding the age to the regression does not influence the coefficient of yoe.  
success: Great, your answer is correct!
failure: Try again.
#>

The ideal number of control variables generally depends on the research question and the number of observations. But even with a high frequently data set like the Amazon data set it would be recommended not to control for too many variables (Kranz, 2022). 

We then perform a multiple regression in R. Regression results are usually presented in the form of tables, listing the values of the coefficients. For illustration, we regress the sales rank on the price from the main Amazon data set:  

$$ y = x_0 + β_1Price + β_2StarRating + \varepsilon $$

**Task:** Replace the ___ gaps with the correct code. Do not forget to press `check`.

```{r "3.1.1", warning = FALSE}
#< fill_in
# We draw a sample of 100.000 observations for performance purposes from `data`
dataLite <- data[sample(nrow(____), size = _____, replace = TRUE),]
# We use the function lm() to regress `pamzn` and `R` on `rank`
regFirstStep <- __(rank ~ _____ + _, data = dataLite)
# The function summary() creates a regression table
summary(regFirstStep)
#>

dataLite <- data[sample(nrow(data), size = 100000, replace = TRUE),]
regFirstStep <- lm(rank ~ pamzn + R, data = dataLite)
summary(regFirstStep)

```

The first column, starting from row two, contains the estimated coefficients for `pamzn` and `R`. The value of 14052.8 indicates that an increase of `pamzn` by one unit on average results in an increase of the sales rank by 14052.8 positions. In the second column, the standard error is given. This is a calculation to determine the accuracy of the individual estimator. Basically, the lower the standard error is, the less the estimator varies. The formula is as follows and can be found [here](https://bookdown.org/mike/data_analysis/linear-regression.html).

$$\widehat{SE}(\hat\beta_k)=s\sqrt{[(\mathbf{X}^T\mathbf{X})^{-1}]_{kk}}$$

Where $s$ is the standard error of the entire regression, $X$ the the covariance matrix, and $\hat\beta_k$ the regression coefficient. The t-value is the estimate divided by its individual standard error. The last column shows the significance level, which indicates the probability of the estimator being exactly as high as it is merely by chance.

#< quiz "t_value"
question: What is the correct answer? 
sc:
- A high level on significance indicates an expressive model. 
- The significance level gives no information about the expressiveness of the model.
- To determine the expressiveness, we need a combination of the significance level and other values.*  
success: Great, your answer is correct! Standard errors, the number of observations and the R-squared value also affect the expressiveness of a model. 
failure: Try again.
#>

The R-squared value in the regression table below indicates a coefficient of determination within $[0;1]$, which provides information on how the explanatory variables fit the respective model. In this context, a perfect R-squared value of 1 means a perfect coefficient of determination. In practice, the R-squared value increases with a higher number of observations and more or better chosen explanatory variables. Adding too many variables to the regression might give the illusion of a higher R-squared value, but it also leads to overfitting. 

#### **Robust Standard Errors**

To apply regressions correctly, certain assumptions must be made. One of these assumptions is **homoskedasticity**. To fulfill this assumption, the residuals of the regression must be uniformly distributed. **Robust Statistics** addresses making estimates that are insensitive to small changes in the basic assumptions like outlier values falsifying the regression residuals (Prof.Dr. Rachev, 2007). In R, there are different methods to implement Robust Standard Errors, all of which follow a similar approach. Basically, this algorithm determines the coefficients by disregarding outlier values and other disruptive factors. 

**Task:** Run the following code to run a regression with robust standard errors. Do not forget to press `check`.

```{r "3.1.2", warning=FALSE}
#< fill_in
# Load the package `fixest` to use the following functions feols()
________(fixest)
# We use the function feols() to add Fixed Effects and Robust Standard Errors. Use `dataLite` for your estimate.
regRSE <- _____(____ ~ pamzn + R, vcov = "hetero", data = ________)
# The function summary() creates a regression table
_______(regRSE)
#>

library(fixest)
regRSE <- feols(rank ~ pamzn + R, vcov = "hetero", data = dataLite)
summary(regRSE)
```

There are almost no differences in the coefficients for `pamzn` and `R`, while there are large differences in the standard errors. If the assumption of homoskedasticity is fulfilled, estimators without robust standard errors tend to have lower standard errors. This could be due to the fact, that Robust Standard Errors take uncertainties into consideration which makes the estimate somewhat less imprecise. However, Robust Standard Errors should still be used to guarantee homoscedasticity. 

#< quiz "Robust_standard_errors"
question: We assume that (for the same regression) the standard errors for the regular regression are lower than for the regression where robust standard errors were used. Whats is the correct answer? 
sc:
- You can omit the robust standard errors, as they reduce the accuracy of the coefficients. 
- Robust standard errors should generally be added to any regression, as regressions without them are generally inaccurate.
- It is advisable to add robust standard errors but not necessary. Even with higher standard errors, it may be important to avoid heteroscedasticity.*  
success: Great, your answer is correct!  
failure: Try again.
#>

**Note:** Heteroscedasticity is the opposite of homoskedasticity.

#### **Fixed Effects**

To make sense of so-called fixed effects, we start with an explanation of fixed effects followed by their implementation in R. Before we start explaining fixed effects, we focus on the **endogeneity problem** of explanatory variables. The endogeneity problem occurs when the explanatory variable $x_k$ depends on the error term $\varepsilon $. For instance, we examine the effect of price (as the only explanatory variable) on the sales rank. Since the book price is determined by many other factors, the price appears to be an endogenous variable. As a result, biases arise and the respective coefficient could become inaccurate. When using fixed effects, we select certain variables to control for. These fixed effects are kept constant and do not affect the estimation. Using this method, we attempt to reduce the variation within explanatory variables by minimizing the potential for bias from omitted variables (Hill, Davis, Roos and French, 2020).

#< quiz "Fixed_Effects"
question: What is your suggestion, does the implementation of fixed effects significantly affect the value of the coefficients of the explanatory variable(s)? 
sc:
- Yes.* 
- No.
success: Great, your answer is correct!  
failure: Try again.
#>

Let us look at an example for this. We want to create a regression where the sales rank indicates the dependent variables $y$ and $x_1$  as explanatory variable for the Amazon book price. The formula looks as follows:  

$$ y = β_0 + β_1 x_1 + \varepsilon $$

In the following, the variable `canum` (chapter 2.1) serves us as fixed effects. The regression considers the variable canum as fixed, controlling for each expression of the variable `canum`. Thus, book- and country-specific differences should eliminate effects on the price.  

**Task:** Replace the ___ gaps to create a regressions with fixed effects. Compare this regression `regFirstStep` with a regression without differences occur. Press `check` after solving this exercise. 

```{r "3.1.3", warning =FALSE}

#< fill_in

# Load the package `modelsummary` to gain a more handsome regression table
library(____________) 
# Create a regression using `canum` as fixed effects. Add robust standard errors. 
regFE <- feols(____ ~ pamzn | _____, ____ = "he____", data = dataLite)
# Create a regular regression without robust standard errors and fixed effects.
regWFE <- lm(rank ~ pamzn, data = dataLite)
modelsummary(list(regWFE, regFE), statistic = "({std.error})", coef_rename = c("pamzn" = "Amazon Price"))
#>

library(modelsummary) 
regFE <- feols(rank ~ pamzn | canum, vcov = "hetero", data = dataLite)
regWFE <- lm(rank ~ pamzn, data = dataLite)
modelsummary(list(regWFE, regFE), statistic = "({std.error})", coef_rename = c("pamzn" = "Amazon Price"))

```

Column two shows the fixed-effects estimate with robust standard errors, while column one shows the regular regression. It is noticeable that the regressions estimate completely different values for the coefficients. Chapter 2.3 showed price differences that can be attributed to the respective country in which the book was rated. The regular regression seems to show exactly this endogeneity problem, that the Amazon price also depends on the country where the book was rated, as well as many other factors. Using fixed effects, we eliminated the country-specific effect and obtained the information that the Amazon price and the sales rank are negatively related. 

The fixed-effects model requires a so-called panel data structure that has data available for each individual (here: Books) at different points in time. However, the addition of fixed effects can also lead to inaccuracies and biases. The disadvantage is that the variables used as fixed effects can no longer be included in the regression as explanatory variables. This model also assumes that the explanatory variable is not collinear (perfectly correlated) to the fixed effects, otherwise this explanatory variable would be determined purely by the fixed effects. Finally, the endogeneity problem can not get completely eliminated by a fixed-effects model. Better suited for this purpose is the Difference-in-Difference approach or Instrumental Variable Estimation, which are not further discussed in the course of this problem set.                                                      

**Summary**

To sum up, this chapter has given us a handsome overview about some basics and advanced methods and tools for working with regressions. First, we learned how regressions basically work, how to interpret them using particular values, and how they are implemented in R. We distinguished between linear and multiple regressions and how control variables potentially affect multiple regressions. We found multiple differences in the regression coefficients and the coefficients of determination when we added control variables. Second, the effect of Robust Standard Errors was discussed and implemented in R. We extracted the information that inserting Robust Standard Errors can be quite useful to ensure homoskedasticity. Finally, we learned how to explain and to implement fixed effects. Application on the Amazon data set underscored the utility and potential of fixed effects for subsequent examinations. 

In the following chapter, we apply the methods and tools from this chapter to examine the overall effects on sales ranks and prices.

## Exercise 3.2 -- Estimation of the Effects on Sales Ranks and Prices

After the introduction for regressions, we focus on the research question to estimate the effect from professional reviews and crowd ratings on the book market. Replicating the underlying regressions by Reimers and Waldfogel, the data set must first be transformed.  

**Task:** Create the data set `dataUS` that only includes observations from the US market. To enable this, use the condition `cno == 3`. Finally, use `arrange()` to sort the data by `canum` and then by `ddate`. Press `check` to confirm your solution. 

```{r "3.2.1", warning=FALSE}
#< task
#>
dataUS <- data %>%
    filter(cno == 3) %>%
    arrange(canum, ddate)
```

In their paper, Reimers and Waldfogel used logarithmic values for their estimation. The logarithmization of dependent or explanatory variables can be associated with many advantages. First, logarithmization can be used to stabilize the variance of the residuals to avoid heteroskedasticity. Further, outlier values are mitigated in their effect so that they have less influence on the magnitude of the regression coefficient. The third advantage is due to interpretation of the regression coefficients. Logarithmized coefficients allow us to interpret these coefficients as percentages for simplicity. The possible interpretation are as follows (Kranz, 2022):

* **log - log**: log $\hat/y$ = $\hat\beta_0$ + log $β_1 x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately $β_1$ percent. 

* **log - level**: log $\hat/y$ = $\hat\beta_0$ + $β_1 x_1$, for a one unit increase in $x_1$, the predicted value of $y$ increases by approximately 100 * $β_1$ percent.

* **level - log**: $\hat/y$ = $\hat\beta_0$ + log $β_1 x_1$, for a one percent increase in $x_1$, the predicted value of $y$ increases by approximately 0.01 * $β_1$ units. 

The regressions created by Reimers and Waldfogel are based on the **log - log** method. In their main regression table, they created five different regressions using `lrank` as dependent variable. The variable `L1.lrank` specified the lagged sales rank from the last date documented in the data set. The addition of `L1.lrank` has the advantage that seasonal fluctuations are included in the regression. Similarly, this variable is predestined for estimates such as this one because the variable can capture short-term (triggered by professional reviews) increases in sales rank.    

**Task:** Run the following chunk to create `reg2` with `ano` as fixed effects and robust standard errors. 

```{r "3.2.2", warning=FALSE}

#< task

#read the package `fixest`
library(fixest)
#Create the regression `reg2`
reg2 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR  + dnytpost1_3 + dnytpost6_3 +  dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | ano, vcov = "hetero", data = dataUS)
#>
```

Computing regressions in this high-frequency data set requires high computational power. For this reason, we compute only two of these regressions within the problem set. Now, we want to generate the second regression using more explanatory variables. 

**Task:** Create `reg3` according to `reg2` above. Add the explanatory variables `lrR`, `dnytpost10_1`, `dnytpost10_2`, `dothpost_1`, `dothpost_2`, `dothpost_3`, `dothpost10_1` and `dothpost10_2`. Remove the explanatory variables `dothpostpre_1`,`dothpostpre_2` and `dothpostpre_3`.

```{r "3.2.3", warning=FALSE}
#< task
# Create reg3 here
#>
reg3 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR + lrR + dnytpost1_3 + dnytpost6_3 + dnytpost10_1 + dnytpost10_2 +  dnytpost10_3 + dothpost_1 + dothpost_2 + dothpost_3 + dothpost10_1 + dothpost10_2 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | ano, vcov = "hetero", data = dataUS)

```

The variable `ano` gives an Amazon identifier (asin) and the numbers `1`, `2` and `3` after the dummy variables `dnytpost` and `dothpost` provide information about which country the observation belongs to. U.S. Data were used for both regressions.  

Subsequently, the three remaining regressions are then read in. To the first regression `reg1` some dummy variables were added as control variables. These dummy variables  For performance reasons we have omitted these variables in this task, but they are stored in `reg1` which we will read in. In the interval $[-20;40]$ for `NYT_elapse` and `OTH_elapse` all expressions were stored individually in different dummy variables, in sum 120 control variables.

**Task:** Read in the files `reg1.RDS`, `reg4.RDS`, and `reg5.RDS` and save them also like this. Do not forget to press `check`. 

```{r "3.2.4"}
#< task
# Enter your code here
#>

reg1 <- readRDS("reg1.RDS")

reg4 <- readRDS("reg4.RDS")

reg5 <- readRDS("reg5.RDS")

```

After all regressions are available, proceed with the generation of a regression table for comparison purposes. For this purpose, we use the function `modelsummary`, which can display multiple regressions more clearly and with more possibilities than `summary`.

**Task:** Use the function `modelsummary` from the R package `modelsummary` to display the five regressions `reg1`, `reg2`, `reg3`, `reg4` and `reg5`. Do not forget to `check` this chunk. 

```{r "3.2.5"}

#< fill_in

RegTable <- ____________(list(____, reg2, ____, ____, ____), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("L1.lrank" = "Lagged log sales rank", "lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable 

#>

RegTable <- modelsummary(list(reg1, reg2, reg3, reg4, reg5), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("L1.lrank" = "Lagged log sales rank", "lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable 

```

The results of the upper estimation deviate visibly  from the underlying results of Reimers and Waldfogel. This deviation is likely due to the variable `L1.lrank`, which is originally calculated with an specific Stata (the programming language used by the authors for the original data processing) function called `L1`. For my calculation of this variable I created a manual function, which can be found [here](t.b.d.). Visibly, the coefficient for `L1.lrank` turns out to be lower than calculated by the authors. Consequently, the actual effect of seasonal effects emanating from the variable `L1.lrank` could potentially be smaller than calculated by Reimers and Waldfogel. 

#< quiz "Reg_Assumptions"
question: Which of these assumptions can be made? 
sc:
- The impact of the New York Times is only short-term, as the coefficient becomes smaller over time.
- The R-squared error of over 0.95 is due to overfitting after adding these amount of explanatory variables. 
- Log Amazon price (the coefficient) > 0, so an increasing price appears to increase sales rank and implies a decrease in sales volume.*
success: Great, your answer is correct!  
failure: Try again.
#>

The coefficient for `NYT: 0-5 days` represents the occurrence of a New York Times review in the US within the first five days of its publication.

#< quiz "Coeff_Interpretation"
question: How should the coefficient for column 2 `NYT 0-5 days` be interpreted? 
sc:
- A professional review in the New York Times lowers the sales rank by an average of 0.28 percent within the first five days of publication.
- A professional review from the New York Times lowers the sales rank by an average of 0.280*100 units within the first five days of publication.
- A professional review from the New York Times lowers the sales rank by an average of 28 percent within the first five days of publication.*
success: Great, your answer is correct!  
failure: Try again.
#>

The coefficient for `Log Amazon Price` is 0.083. This value indicates that a price increase on average increases the sales rank.

#< quiz "Coeff_Interpretation2"
question: How should the coefficient for column 2 `Log Amazon Price` be interpreted? 
sc:
- By increasing the price for one unit, the sales rank increases by an average of 0.084*100 units.
- By increasing the price for one percent, the sales rank increases by an average of 0.084 percent. As a result, the price is definitely elastic.
- By increasing the price for one percent, the sales rank increases by an average of 0.084 percent. There is no information about price elasticity.
success: Great, your answer is correct! Concrete quantities are needed to measure price elasticity.   
failure: Try again.
#>

Expectedly, the New York Times recommendations are associated with the greatest effect on sales ranks. The coefficient accounts approximately -0.34 for U.S. data and about -0.36 for all data. In column three, an *interaction term* between `lR` and `lreview` was used. Generally, interaction terms are used to control for bias when an explanatory variable also depends on another independent variable. In this case, the addition of these interaction term implies a shrinking effect of the star ratings. This suggests that star ratings have more influence depending on multiple underlying ratings (Reimers, Waldfogel, 2021).  

In the following, price is examined as independent variable. In chapter 2.3, the assumption was made that New York Times reviews could increases the price in the short-term. To investigate this further, we regress price on similar explanatory variables as in the regression above.        

**Task:** Create an according regression by filling in the gaps. Use the variable `canum` as fixed effects.

```{r "3.2.6"}
#< fill_in
reg6 <- fe___(______ ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 +  dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | _____, vcov = ________, data = dataUS)

reg7 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

#>

reg6 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + lrR + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

reg7 <- feols(lpamzn ~ lrank + lreview + lR  + dnytpost1_3 + dnytpost6_3 + dnytpost10_3 + dothpost_3 + dothpost10_3 + dnytpostpre_1 + dnytpostpre_2 + dnytpostpre_3 + dothpostpre_1 + dothpostpre_2 + dothpostpre_3 + dnytpost1r_3 + dnytpost6r_3 + dnytpost10r_3 + dnytpostprer_1 + dnytpostprer_2 + dnytpostprer_3 + epos + epos2 + epos3 + eneg + eneg2 + eneg3 | canum, vcov = "hetero", data = dataUS)

```

**Task:** Run the following chunk to visualize the regression with `modelsummary`.

```{r "3.2.7"}

#< task_notest
RegTable2 <- modelsummary(list(reg6, reg7), statistic = "({std.error})", coef_omit = "DOTH|DNYT|epos|eneg|postpre|_1|_2", coef_rename = c("lpamzn" = "Log Amazon price", "lreview" = "Number of ratings", "lR" = "log star rating","lrR" = "log number of ratings x log stars", "dnytpost1_3" = "NYT: 0-5 days", "dnytpost6_3" = "NYT: 6-10 days", "dnytpost10_3" = "NYT: 11-20 days", "dnytpost1r_3" = "NYT Rec: 0-5 days", "dnytpost6r_3" = "NYT Rec: 6-10 days", "dnytpost10r_3" = "NYT Rec: 11-20 days", "dothpost_3" = "OTH: 1-10 days", "dothpost10_3" = "OTH: 11-20 days"))
 RegTable2 
#>

```

The occurrence of New York Times reviews within the first five days of publication is associated with a 0.01 percent price increase. This price increase remains relatively even over time looking at review occurrence within five days to 20 days of publication. Chapter 2.3 also assumed that Amazon star ratings tend to increase demand in long-term, which may lead to lower prices due to more competition. As above, I added an interaction term for "reg7" to illustrate the dependence of Amazon star ratings on the number of reviews. When estimating the effect on the Amazon price, the addition of this interaction term results in a positive coefficient on the `log star rating` of 0.045 instead of a negative coefficient of -0.007. 

#< quiz "Des_vs_Pred"
question: What does this mean for the assumption of a long-term price increase? 
sc:
- The assumption is incorrect because descriptive analyzes cannot account for dependencies between different effects.*
- The assumption is correct and the interaction term is inappropriate for this situation.
success: Great, your answer is correct! As a rule, descriptive analyses do not take into account the dependencies of the variables. In this case, the descriptive analysis leads to a false assumption.    
failure: Try again.
#>

**Summary**

After expanding our methodical knowledge in empirical economics in Section 3.1, we replicated the main regression table (with log sales rank as the dependent variable) from the underlying paper, and found that the coefficients differed from the original estimation results. These differences are due to the lagged sales rank, which was not replicatable. However, the regression from this problem shew similar coefficients but with higher standard errors. We found large short-term effects of New York Times reviews, while the overall effect of other professional reviews was less present. In addition, we learned how interaction terms work and how they are implemented in R. Second, we examined the effects on the Amazon price. We found overall small but positive effects from New York Times reviews and negative effects from other journals. By using an interaction term, we discarded the assumption that an increase in Amazon star ratings is associated with a decrease in Amazon prices.

In the following chapter event studies are explained and implemented in R.

## Exercise 3.3 -- Introduction and Implementation of Event Studies

In their paper, Reimers and Waldfogel used an so-called **Event Study** to illustrate and estimate the impact of events (here: publication of professional reviews) that took place. Originally, Event Studies come from the financial sector from James Dolly in 1933. He examined the price effects of stock splits, studying the occurrence of price changes at the time of the split (MacKinley, 1997, ppt. 13). MacKinley listed several methods for applying Event Studies, including *Cross-Sectional Models* that use regressions (MacKinley, 1997, ppt. 33). Reimers and Waldfogel also used such an approach to visualize short-term effects on the Amazon sales rank.

The first step of the authors procedure is to place all books chronologically on top of each other. For this purpose, the variables `NYT_elapse` and `OTH_elapse` are suitable. For both scenarios, the variable is filtered 20 days before and 40 days after the review is published. Before implementing this Event Study, the data structure need to get reviewed. 

**Task:** Create the data set `dataESNYT` and `dataESOTH` to count the number of books with information about 20 days before and 40 days after the review is published.

```{r "3.3.1", warning=FALSE}
#< task
dataESNYT <- data %>%
  filter(cno == 3 & NYT_elapse >= -20 & NYT_elapse <= 40) %>%
  group_by(titleno) %>%
  summarize(Sum_observations = n())

nrow(dataESNYT)
sum(dataESNYT$Sum_observations)

dataESOTH <- data %>%
  filter(cno == 3 & OTH_elapse >= -20 & OTH_elapse <= 40) %>%
  group_by(titleno) %>%
  summarize(Sum_observations = n())

nrow(dataESOTH)
sum(dataESOTH$Sum_observations)
#>

```

A total of 46641 observations are available for the New York Times and 11917 observations are available for other magazines in the U.S. during this period. This corresponds to a number of 1193 and 322 books. The next step is to create a regression as follows:    

$$ y = β_0 + β_1LaggedSalesRank + β_2Price + β_3NumberOfReviews + β_4StarRating + \varepsilon $$


**Task:** Implement the regression above in R and save this regression under `reg7`. Use Robust Standard Errors and `canum` as fixed effects.

```{r "3.3.2", warning = FALSE}
#< task
# Create `reg7` here
#>
reg7 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR | canum, vcov = "hetero", data = dataUS)

```

Now, this approach consists in aggregation the regression residuals on the filtered variable `NYT_elapse`. Before that, however, the residuals must be adjusted so that they have a value of zero for `NYT_elapse = -1`. This ensures that we can ideally estimate the effects after event start. 

**Task:** `Check` the following Code to adjust the residuals from `reg7`. 

```{r "3.3.3", warning=FALSE}

#< task
reg7$residuals <- reg7$residuals - mean(reg7$residuals[which(dataUS$NYT_elapse == -1)])
#>

```

Further, we aggregate the residuals on `NYT_elapse`. As a result, for each expression of `NYT_elapse`, an average residual is generated.

**Task:** `Check` the following chunk to aggregate the residuals on `NYT_elapse`.

```{r "3.3.4", warning=FALSE}

#< task
reg7_agg <- aggregate(reg7$residuals, by = list(dataUS$NYT_elapse), FUN = mean)
  colnames(reg7_agg) <- c("NYT_elapse", "Avg.Sales.rank")
 reg7_agg <- reg7_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg7))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg7))))
#>
#mean(reg7$residuals[which(dataUS$NYT_elapse == -1)])
```

The variables `Max95` and `Min95` are indicating the 95 percent *confidence interval*. A confidence interval indicates to x percent (here: 95 percent) how probable it is that the actual value is in this interval or graphical range. The formula is as follows: 

$$\bar{X} \pm Z \cdot \frac{s}{\sqrt{n}}$$

where s indicates the standard deviation, Z the z-value for the confidence level and n the sample size. A simplified z-table can be found [here](http://www.ltcconline.net/greenl/courses/201/estimation/smallConfLevelTable.htm).

**Task:** Create a `ggplot`- graph to visualize the average residuals on the y-axis and `NYT_elapse` on the x-axis. Add a vertical line to illustrate the day before the review was published. Information can be found [here](https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_vline).

```{r "3.3.5", warning=FALSE}

#< fill_in
ESNYT <- ggplot(reg7_agg, aes(x = __________, y = ______________)) + 
   geom_line() + 
   ggtitle("Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
   geom_line(aes(y = _____), linetype = "dashed") + 
   geom_line(aes(y = _____), linetype = "dotted") +
   # Add the vertical line here (Use: size = 0.5, color = "red")
 
   scale_y_reverse() + 
   ylim(0.3, -0.6) +
   theme_minimal()
 ESNYT 
#>
ESNYT <- ggplot(reg7_agg, aes(x = NYT_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
  geom_line(aes(y = Max95), linetype = "dashed") + 
  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.3, -0.6) +
  theme_minimal()
ESNYT 
```

As the graph shows, a published review in the New York Times is associated with a huge increase in log sales rank of about 0.40 units. After almost two weeks, the sales ranks returns to its base. The proximity of the confidence interval line to the main line indicates a high level of significance. Analogous, we implement the same event study for all other journals. 

**Task:** According to above, adjust the residuals on `OTH_elapse = 1` and call it `reg7.1$residuals`. Furthermore, create `reg7.1_agg` with `aggregate()` on the variable `OTH_elapse`. Finally, add the minimum and maximum value of the 95 percent confidence interval. 

```{r "3.3.6", warning=FALSE}

#< task
# Add your code here
#>

reg7$residuals <- reg7$residuals - mean(reg7$residuals[which(dataUS$OTH_elapse == -1)])

reg7.1_agg <- aggregate(reg7$residuals, by = list(dataUS$OTH_elapse), FUN = mean)
  colnames(reg7.1_agg) <- c("OTH_elapse", "Avg.Sales.rank")
 reg7.1_agg <- reg7.1_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg7))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg7))))
```

**Task:** Press `check` to illustrate the differences between effects of New York Times and other magazines. 

```{r "3.3.7", warning=FALSE}

ESOTH <- ggplot(reg7.1_agg, aes(x = OTH_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Event Study - Other Magazines") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
#  geom_line(aes(y = Max95), linetype = "dashed") + 
#  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.5, -0.5) +
  theme_minimal()
ESOTH 

```

While `ESNYT` delivers a similar course to the New York Times Event Study from Reimers and Waldfogel, there are major differences in the graph produced here. This could be due to `L1.lrank`, which is not replicatable and differs from the variable created by Reimers and Waldfogel. Since no added value is generated from the graph, the effects (based on Event Studies) of other magazines are not discussed further in the course of this problem set. 

In chapter 2.3, it was claimed that occurring intersections between magazines could contaminate our estimations. We found that there are a total of 201 books that have been reviewed by other magazines besides the New York Times. Contamination can potentially lead to higher standard errors and inconsistencies in our estimate and result in misinterpretations. Contamination was permitted in the main regression because a number of observations at this level were not available for the event study. Now, we implement another Event Study with adjusted data.  

**Task:** Create a data set `dataAdj` from which you filter out contaminated data where more than the New York Times has reviewed a book. 

```{r "3.3.8", warning=FALSE}

#< fill_in
dataAdj <- data %>%
  filter(((data$DBG == 0 & data$DCHI == _ & data$DLAT == _ & data$DWAPO == _ & data$DWSJ == _ & data$DNYT == _) | data$DNYT == _) & cno == _)
#>

dataAdj <- data %>%
  filter(((data$DBG == 0 & data$DCHI == 0 & data$DLAT == 0 & data$DWAPO == 0 & data$DWSJ == 0 & data$DNYT == 1) | data$DNYT == 0) & cno == 3)

```

We now count about 50.000 observations less and filtered out the contaminated data. Let us repeat creating an Event Study of New York Times reviews. 

**Task:** `Check` the following code to create an additional Event Study. 

```{r "3.3.9", warning = FALSE}

#< task
# 1. Create a regression based on the adjusted data
reg8 <- feols(lrank ~ L1.lrank + lpamzn + lreview + lR | canum, vcov = "hetero", data = dataAdj)
# 2. Mean-adjustment on NYT_elapse = -1.
reg8$residuals <- reg8$residuals - mean(reg8$residuals[which(dataAdj$NYT_elapse == -1)])
# 3. Aggregation 
reg8_agg <- aggregate(reg8$residuals, by = list(dataAdj$NYT_elapse), FUN = mean)
  colnames(reg8_agg) <- c("NYT_elapse", "Avg.Sales.rank")
 reg8_agg <- reg8_agg %>%
   mutate(Max95 = Avg.Sales.rank + (1.96*sd(Avg.Sales.rank)/ sqrt(length(reg8))),
          Min95 = Avg.Sales.rank - (1.96*sd(Avg.Sales.rank) / sqrt(length(reg8))))
# 4. Visualizing
ESNYTNew <- ggplot(reg8_agg, aes(x = NYT_elapse, y = Avg.Sales.rank)) + 
   geom_line() +
   ggtitle("Adjusted Event Study - New York Times") +
   scale_x_continuous(breaks = seq(-20, 40, by = 10), limits = c(-20, 40)) + 
  geom_line(aes(y = Max95), linetype = "dashed") + 
  geom_line(aes(y = Min95), linetype = "dotted") +
  geom_vline(xintercept = -1, color = "red", size = 0.5) +
  scale_y_reverse() + 
  ylim(0.3, -0.6) +
  theme_minimal()
ESNYTNew
#>

```

As the graph shows, there are almost no deviations from previous diagram. However, Event Studies also entail some disadvantages. First, the mean-adjusted method does not work that well. This is due to the fact that if many of these events occur simultaneously, possible seasonal fluctuation or other endogenous effects cannot be ruled out (V. Henderson Jr, 1990, pp. 288) As a result, the time period must be well chosen. Furthermore, Event Studies cannot provide causal evidence. Finally, other effects also influence the estimation, especially when the regression consists of only four explanatory variables.

**Summary** 

Event Studies are useful methods to illustrate the effect of events occurring at different times. We received information about the origin of Event Studies and how they work in general. We then focused on the implementation of event studies in R using so-called cross functional models, which can estimate the effect on a chronological sequence using aggregated residuals. We replicated these Event Studies from Reimers and Waldfogel and differentiated between New York Times reviews and reviews from other journals. While New York Times reviews show large effects on log sales rank, the estimate of the effect for other journals differs from the authors' estimate, making it unusable. Further, we adjusted the data to rule out contamination potential that was already detected in chapter 2.2. Finally, some disadvantages have been listed. 

In chapter 4., we link sales ranks to sales quantities to determine and examine price elasticities.

## Exercise 4 -- Translating Sales Ranks in Quantities and Price Elasticities

The research results of the sales ranks on Amazon in regressions are limited to relative statements about the sales level of a book as opposed to other books. In order to make economic statements regarding to price elasticity and welfare, absolut sales volumes are needed. Reimers and Waldfogel collected data from the Top 100 Weekly Bestsellers, where Reimers and Waldfogel claim they matched 876 books (hits in U.S. asin). The collected data was originally produced by [the Nielsen Company](https://www.nielsen.com/), which is a market research company, and therefore the data cannot be replicated. Due to the fact that the Amazon data set is based on daily observations of sales ranks while the Nielsen data lists only weekly observations, the authors created a formula based on the general assumption that sales and ranks are exponentially related (Chevalier and Goolsbee, 2003) to reconcile daily sales ranks with weekly sales data: 

$$q_{jw} = \sum_{t \in w} A_{rjt} - B + \upsilon_{jw}$$

where $t$ and $j$ stand for the day and the book index, $w$ denotes the week, $ \upsilon_{jw}$ represents an error term and A and B are to be estimated by using least squares (Reimers and Waldfogel, 2021). By applying a 500-fold bootstrapping procedure (re-sampling method in which the coefficients are estimated using 500 different samples), estimators A and B were associated with average values of 10167 and 0.45. 

#< quiz "Sampling_vs_Methods"
question: Which of these two statements could potentially have an adverse effect at an bootstrapping procedure? 
sc:
- Re-sampling methods often require intensive computer power, especially if the data set is large.*
- To rely on the solution of this method, the underlying data must be normally distributed.
success: Great, your answer is correct!   
failure: Try again.
#>

In the following, not only price elasticities are calculated, but also elasticities from professional valuations and Amazon star ratings. Calculating these elasticities, Reimers and Waldfogel used the following formula:  

$$
 \epsilon = \frac{B*Coefficient \ of \ Elasticity}{1 - Coefficient \ of \ Log Sales Rank} 
$$

To implement this formula in R, we use the coefficients estimated in `reg5`. 

**Task:** `Check` the following Code to see how the 25th, the 50th and the 75th quantile of Amazon star rating elasticities get implemented in R.

```{r, "4.1"}

#< task_notest
star_elas_25 <- dataUS$B * (reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.25, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

star_elas_50 <- dataUS$B * ( reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.5, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

star_elas_75 <- dataUS$B * ( reg5$coefficients["lR"] +  reg5$coefficients["lrR"] * quantile(dataUS$lreview, probs = 0.75, na.rm = TRUE)) / (1 -  reg5$coefficients["L1.lrank"])

dataUS$star_elas_25 <- star_elas_25
dataUS$star_elas_50 <- star_elas_50
dataUS$star_elas_75 <- star_elas_75
#>

```

After this instruction the further elasticities are calculated by the reader. 

**Task:** Determine the elasticities for `lpamzn`, `lreview`, `dothpost_3` and `dothpost10_3`. According to the chunk above, save your results as column in `dataUS` under the names `price_elas_mean`, `star_elas_mean`, `oth_1_10` and `oth_11_20`. Press `Check` to confirm.

**Note:** The coefficient for `lrR` was added to account for interaction effects. For the following chunks `lrR` is not needed.  

```{r "4.2"}
#< task

#>

price_elas_mean <- dataUS$B * reg5$coefficients["lpamzn"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$price_elas_mean <- price_elas_mean

star_elas_mean <- dataUS$B * (reg5$coefficients["lR"] + reg5$coefficients["lrR"] * mean(dataUS$lreview)) / (1 - reg5$coefficients["L1.lrank"])
dataUS$star_elas_mean <- star_elas_mean
	
oth_1_10 <- dataUS$B *  reg5$coefficients["dothpost_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$oth_1_10 <- oth_1_10

oth_11_20 <- dataUS$B *  reg5$coefficients["dothpost10_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$oth_11_20 <- oth_11_20


```

Elasticities for the New York Times elasticity effects are calculated below. 

**Task:** Press `check` to calculate the remaining elasticities. 

```{r "4.3"}

#< task_notest
nyt_1_5_not_rec <- dataUS$B * reg5$coefficients["dnytpost1_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_1_5_not_rec <- nyt_1_5_not_rec
nyt_6_10_not_rec <- dataUS$B * reg5$coefficients["dnytpost6_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_6_10_not_rec <- nyt_6_10_not_rec
nyt_11_20_not_rec <- dataUS$B * reg5$coefficients["dnytpost10_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_11_20_not_rec <- nyt_11_20_not_rec

nyt_1_5_rec <- dataUS$B * reg5$coefficients["dnytpost1r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_1_5_rec <- nyt_1_5_rec
nyt_6_10_rec <- dataUS$B * reg5$coefficients["dnytpost6r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_6_10_rec <- nyt_6_10_rec
nyt_11_20_rec <- dataUS$B * reg5$coefficients["dnytpost10r_3"] / (1 - reg5$coefficients["L1.lrank"])
dataUS$nyt_11_20_rec <- nyt_11_20_rec

#>

```

Finally, we read in the remaining elasticity values. 

**Task:** Read in the following RDS files: `other_only.RDS`, `nyt_not_rec_only.RDS`, `nyt_rec_only.RDS`, `both_not_rec.RDS`, `both_rec.RDS`, `nytoverall.RDS` and `overall.RDS`. 

```{r "4.4"}

#< task

#>
readRDS("other_only.RDS")
readRDS("nyt_not_rec_only.RDS")
readRDS("nyt_rec_only.RDS")
readRDS("both_not_rec.RDS")
readRDS("both_rec.RDS")
readRDS("nytoverall.RDS")
readRDS("overall.RDS")
```

After collecting all elasticity values, we create a table to visualize them. The authors also generated within a bootstrapping procedure standard errors for the estimated elasticity values. Basically, in addition to a variance-covariance matrix, they created normally distributed random variables to calculate within a bootstrapping procedure standard errors the elasticities that are present for us. For reasons of performance, these calculations will have no relevance in the further course. 

Let us illustrate these elasticities in a table. 

**Task:** First, create a data set with `data.frame` to list all elasticity values. Then, use `kbl()` to create a table containing the elasticity values. Fill in the ___ gaps and press `check` to confirm your input. 

```{r "4.5"}

#< fill_in
summary_data <- d_______me(Effects = c(mean(______elas_mean), mean(star_elas_25), mean(star_elas_50), mean(star_elas_75), mean(star_elas_mean), mean(nyt_1_5_not_rec), mean(nyt_6_10_not_rec), mean(nyt_11_20_not_rec), mean(nyt_1_5_rec), mean(nyt_6_10_rec), mean(nyt_11_20_rec), mean(oth_1_10), mean(oth_11_20), other_only, nyt_not_rec_only, nyt_rec_only, both_not_rec, both_rec, overall, nytoverall))

row.names(__m_________) = c("Price Elasticity", "Star Elasticity 25%", "Star Elasticity 50%", "Star Elasticity 75%", "Star Elasticity Overall", "NYT 1-5 Days", "NYT 6-11 Days", "NYT 11-20 Days", "NYT 1-5 Days rec", "NYT 6-11 Days rec", "NYT 11-20 Days rec", "OTH 1-10 Days", "OTH 11-20 Days", "Only other Magazines", "Only NYT not recommended", "Only NYT recommended", "Both not recommended", "Both recommended", "overall", "NYT reviewed")

______y_da__ %>%
kbl(caption = "Elasticities") %>%
  kable_paper("striped", full_width = TRUE) %>%
  p______ws("*Percent effect of review on annual q*", 14, 20) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
#>

summary_data <- data.frame(Effects = c(mean(price_elas_mean), mean(star_elas_25), mean(star_elas_50), mean(star_elas_75), mean(star_elas_mean), mean(nyt_1_5_not_rec), mean(nyt_6_10_not_rec), mean(nyt_11_20_not_rec), mean(nyt_1_5_rec), mean(nyt_6_10_rec), mean(nyt_11_20_rec), mean(oth_1_10), mean(oth_11_20), other_only, nyt_not_rec_only, nyt_rec_only, both_not_rec, both_rec, overall, nytoverall))

row.names(summary_data) = c("Price Elasticity", "Star Elasticity 25%", "Star Elasticity 50%", "Star Elasticity 75%", "Star Elasticity Overall", "NYT 1-5 Days", "NYT 6-11 Days", "NYT 11-20 Days", "NYT 1-5 Days rec", "NYT 6-11 Days rec", "NYT 11-20 Days rec", "OTH 1-10 Days", "OTH 11-20 Days", "Only other Magazines", "Only NYT not recommended", "Only NYT recommended", "Both not recommended", "Both recommended", "overall", "NYT reviewed")

summary_data %>%
kbl(caption = "Elasticities") %>%
  kable_paper("striped", full_width = TRUE) %>%
  pack_rows("*Percent effect of review on annual q*", 14, 20) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) 

```








